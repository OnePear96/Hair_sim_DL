{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from models import DPINet\n",
    "#from data import PhysicsFleXDataset, collate_fn\n",
    "\n",
    "from utils import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--relation_dim'], dest='relation_dim', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--pstep', type=int, default=2)\n",
    "parser.add_argument('--n_rollout', type=int, default=0)\n",
    "parser.add_argument('--time_step', type=int, default=0)\n",
    "parser.add_argument('--time_step_clip', type=int, default=0)\n",
    "parser.add_argument('--dt', type=float, default=1./60.)\n",
    "parser.add_argument('--nf_relation', type=int, default=300)\n",
    "parser.add_argument('--nf_particle', type=int, default=200)\n",
    "parser.add_argument('--nf_effect', type=int, default=200)\n",
    "parser.add_argument('--env', default='')\n",
    "parser.add_argument('--train_valid_ratio', type=float, default=0.9)\n",
    "parser.add_argument('--outf', default='files')\n",
    "parser.add_argument('--dataf', default='data')\n",
    "parser.add_argument('--num_workers', type=int, default=10)\n",
    "parser.add_argument('--gen_data', type=int, default=0)\n",
    "parser.add_argument('--gen_stat', type=int, default=0)\n",
    "parser.add_argument('--log_per_iter', type=int, default=1000)\n",
    "parser.add_argument('--ckp_per_iter', type=int, default=10000)\n",
    "parser.add_argument('--eval', type=int, default=0)\n",
    "parser.add_argument('--verbose_data', type=int, default=1)\n",
    "parser.add_argument('--verbose_model', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--n_instance', type=int, default=0)\n",
    "parser.add_argument('--n_stages', type=int, default=0)\n",
    "parser.add_argument('--n_his', type=int, default=0)\n",
    "\n",
    "parser.add_argument('--n_epoch', type=int, default=1000)\n",
    "parser.add_argument('--beta1', type=float, default=0.9)\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--batch_size', type=int, default=1)\n",
    "parser.add_argument('--forward_times', type=int, default=2)\n",
    "\n",
    "parser.add_argument('--resume_epoch', type=int, default=0)\n",
    "parser.add_argument('--resume_iter', type=int, default=0)\n",
    "\n",
    "# shape state:\n",
    "# [x, y, z, x_last, y_last, z_last, quat(4), quat_last(4)]\n",
    "parser.add_argument('--shape_state_dim', type=int, default=14)\n",
    "\n",
    "# object attributes:\n",
    "parser.add_argument('--attr_dim', type=int, default=0)\n",
    "\n",
    "# object state:\n",
    "parser.add_argument('--state_dim', type=int, default=0)\n",
    "parser.add_argument('--position_dim', type=int, default=0)\n",
    "\n",
    "# relation attr:\n",
    "parser.add_argument('--relation_dim', type=int, default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"--env SingleHair --gen_data 1\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases_dict = dict()\n",
    "\n",
    "args.n_rollout = 50\n",
    "args.num_workers = 3\n",
    "args.gen_stat = 1\n",
    "\n",
    "# object states:\n",
    "# [x, y, z, xdot, ydot, zdot]\n",
    "args.state_dim = 6\n",
    "args.position_dim = 3\n",
    "\n",
    "# object attr:\n",
    "# [rigid]\n",
    "args.attr_dim = 1\n",
    "\n",
    "# relation attr:\n",
    "# [none]\n",
    "args.relation_dim = 1\n",
    "\n",
    "args.time_step = 500\n",
    "args.time_step_clip = 20\n",
    "args.n_instance = 1\n",
    "args.n_stages = 1\n",
    "\n",
    "args.neighbor_radius = 0.08\n",
    "\n",
    "phases_dict[\"instance_idx\"] = [0, 30]\n",
    "phases_dict[\"root_num\"] = [[]]\n",
    "phases_dict[\"instance\"] = ['solid']\n",
    "phases_dict[\"material\"] = ['solid']\n",
    "\n",
    "args.outf = 'dump_SingleHair/' + args.outf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/data_SingleHair\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.outf = args.outf + '_' + args.env\n",
    "args.dataf = 'data/' + args.dataf + '_' + args.env\n",
    "print (args.dataf)\n",
    "os.system('mkdir -p ' + args.outf)\n",
    "os.system('mkdir -p ' + args.dataf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_PyFleX(info):\n",
    "\n",
    "    env, root_num = info['env'], info['root_num']\n",
    "    thread_idx, data_dir, data_names = info['thread_idx'], info['data_dir'], info['data_names']\n",
    "    n_rollout, n_instance = info['n_rollout'], info['n_instance']\n",
    "    time_step, time_step_clip = info['time_step'], info['time_step_clip']\n",
    "    shape_state_dim, dt = info['shape_state_dim'], info['dt']\n",
    "\n",
    "    env_idx = info['env_idx'] # =11\n",
    "\n",
    "    np.random.seed(round(time.time() * 1000 + thread_idx) % 2**32)\n",
    "    \n",
    "    stats = [init_stat(3), init_stat(3)]\n",
    "\n",
    "    import pyflex\n",
    "    pyflex.init()\n",
    "\n",
    "    for i in range(n_rollout):\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"%d / %d\" % (i, n_rollout))\n",
    "\n",
    "        rollout_idx = thread_idx * n_rollout + i\n",
    "        rollout_dir = os.path.join(data_dir, str(rollout_idx))\n",
    "        os.system('mkdir -p ' + rollout_dir)\n",
    "        \n",
    "        # scene_params: [len(box) at dim x,len(box) at dim y,len(box) at dim z, num_hair per circle, num_circle]\n",
    "        cap_size = [0.1,1.5]\n",
    "        N_hairs = 1\n",
    "\n",
    "\n",
    "        scene_params = np.array(cap_size)\n",
    "\n",
    "        pyflex.set_scene(env_idx, scene_params, thread_idx)\n",
    "        n_particles = pyflex.get_n_particles()\n",
    "        n_shapes = 1\n",
    "        N_particles_per_hair = int(n_particles/N_hairs)\n",
    "        idx_begins = np.arange(N_hairs)*N_particles_per_hair\n",
    "        idx_hairs = [[i,i+N_particles_per_hair-1] for i in idx_begins]\n",
    "\n",
    "        positions = np.zeros((time_step, n_particles, 3), dtype=np.float32)\n",
    "        velocities = np.zeros((time_step, n_particles, 3), dtype=np.float32)\n",
    "        shape_position = np.zeros((time_step, n_shapes, 3), dtype=np.float32)\n",
    "        shape_velocities = np.zeros((time_step, n_shapes, 3), dtype=np.float32)\n",
    "\n",
    "        for j in range(time_step_clip):\n",
    "            p_clip = pyflex.get_positions().reshape(-1, 4)[:, :3]\n",
    "            shape_p_clip = pyflex.get_shape_states()[:3].reshape(-1,3)\n",
    "            pyflex.step()\n",
    "\n",
    "        for j in range(time_step):\n",
    "            positions[j] = pyflex.get_positions().reshape(-1, 4)[:, :3]\n",
    "            shape_position[j] = pyflex.get_shape_states()[:3].reshape(-1,3)\n",
    "        #    shape_prevposition = pyflex.get_shape_states()[3:6].reshape(-1,3)\n",
    "            if j == 0:\n",
    "                velocities[j] = (positions[j] - p_clip) / dt\n",
    "                shape_velocities[j] = (shape_position[j] - shape_p_clip)/dt\n",
    "            else:\n",
    "                velocities[j] = (positions[j] - positions[j - 1]) / dt\n",
    "                shape_velocities[j] = (shape_position[j] - shape_position[j-1])/dt\n",
    "\n",
    "            pyflex.step()\n",
    "            data = [positions[j], velocities[j], idx_hairs, shape_position[j], shape_velocities[j]]\n",
    "            store_data(data_names, data, os.path.join(rollout_dir, str(j) + '.h5'))\n",
    "        \n",
    "        # change dtype for more accurate stat calculation\n",
    "        # only normalize positions and velocities\n",
    "        datas = [positions.astype(np.float64), velocities.astype(np.float64)]\n",
    "\n",
    "        for j in range(len(stats)): \n",
    "            # here j = 2, refers to positions and velocities\n",
    "            stat = init_stat(stats[j].shape[0]) \n",
    "            # stat= np.zeros((3,3))\n",
    "            stat[:, 0] = np.mean(datas[j], axis=(0, 1))[:]\n",
    "            stat[:, 1] = np.std(datas[j], axis=(0, 1))[:]\n",
    "            stat[:, 2] = datas[j].shape[0] * datas[j].shape[1] # time_step*n_particles\n",
    "            stats[j] = combine_stat(stats[j], stat)\n",
    "\n",
    "    pyflex.clean()\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsFleXDataset(Dataset):\n",
    "\n",
    "    def __init__(self, args, phase, phases_dict, verbose):\n",
    "        self.args = args\n",
    "        self.phase = phase\n",
    "        self.phases_dict = phases_dict\n",
    "        self.verbose = verbose\n",
    "        self.data_dir = os.path.join(self.args.dataf, phase)\n",
    "        self.stat_path = os.path.join(self.args.dataf, 'stat.h5')\n",
    "\n",
    "        os.system('mkdir -p ' + self.data_dir)\n",
    "\n",
    "        #    self.data_names = ['positions', 'velocities', 'shape_quats', 'clusters', 'scene_params']\n",
    "        self.data_names = ['positions', 'velocities','hair_idx','shape_position']\n",
    "\n",
    "        ratio = self.args.train_valid_ratio\n",
    "        if phase == 'train':\n",
    "            self.n_rollout = int(self.args.n_rollout * ratio)\n",
    "        elif phase == 'valid':\n",
    "            self.n_rollout = self.args.n_rollout - int(self.args.n_rollout * ratio)\n",
    "        else:\n",
    "            raise AssertionError(\"Unknown phase\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rollout * (self.args.time_step - 1)\n",
    "\n",
    "    def load_data(self, name):\n",
    "        self.stat = load_data(self.data_names[:2], self.stat_path)\n",
    "        for i in range(len(self.stat)):\n",
    "            self.stat[i] = self.stat[i][-self.args.position_dim:, :]\n",
    "            # print(self.data_names[i], self.stat[i].shape)\n",
    "\n",
    "    def gen_data(self, name):\n",
    "        # if the data hasn't been generated, generate the data\n",
    "        print(\"Generating data ... n_rollout=%d, time_step=%d\" % (self.n_rollout, self.args.time_step))\n",
    "\n",
    "        infos = []\n",
    "        for i in range(self.args.num_workers):\n",
    "            info = {\n",
    "                'env': self.args.env,\n",
    "                'root_num': self.phases_dict['root_num'],\n",
    "                'thread_idx': i,\n",
    "                'data_dir': self.data_dir,\n",
    "                'data_names': self.data_names,\n",
    "                'n_rollout': self.n_rollout // self.args.num_workers,\n",
    "                'n_instance': self.args.n_instance,\n",
    "                'time_step': self.args.time_step,\n",
    "                'time_step_clip': self.args.time_step_clip,\n",
    "                'dt': self.args.dt,\n",
    "                'shape_state_dim': self.args.shape_state_dim}\n",
    "\n",
    "            info['env_idx'] = 11\n",
    "            infos.append(info)\n",
    "\n",
    "        cores = self.args.num_workers\n",
    "        pool = mp.Pool(processes=cores)\n",
    "        data = pool.map(gen_PyFleX, infos)\n",
    "\n",
    "        print(\"Training data generated, warpping up stats ...\")\n",
    "\n",
    "        if self.phase == 'train' and self.args.gen_stat:\n",
    "            # positions [x, y, z], velocities[xdot, ydot, zdot]\n",
    "            self.stat = [init_stat(3), init_stat(3)]\n",
    "            for i in range(len(data)):\n",
    "                for j in range(len(self.stat)):\n",
    "                    self.stat[j] = combine_stat(self.stat[j], data[i][j])\n",
    "            store_data(self.data_names[:2], self.stat, self.stat_path)\n",
    "        else:\n",
    "            print(\"Loading stat from %s ...\" % self.stat_path)\n",
    "            self.stat = load_data(self.data_names[:2], self.stat_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_rollout = idx // (self.args.time_step - 1)\n",
    "        idx_timestep = idx % (self.args.time_step - 1)\n",
    "\n",
    "        # ignore the first frame for env RiceGrip\n",
    "        if self.args.env == 'RiceGrip' and idx_timestep == 0:\n",
    "            idx_timestep = np.random.randint(1, self.args.time_step - 1)\n",
    "\n",
    "        data_path = os.path.join(self.data_dir, str(idx_rollout), str(idx_timestep) + '.h5')\n",
    "        data_nxt_path = os.path.join(self.data_dir, str(idx_rollout), str(idx_timestep + 1) + '.h5')\n",
    "\n",
    "        data = load_data(self.data_names, data_path)\n",
    "\n",
    "        '''\n",
    "        vel_his = []\n",
    "        for i in range(self.args.n_his):\n",
    "            path = os.path.join(self.data_dir, str(idx_rollout), str(max(1, idx_timestep - i - 1)) + '.h5')\n",
    "            data_his = load_data(self.data_names, path)\n",
    "            vel_his.append(data_his[1])\n",
    "\n",
    "        data[1] = np.concatenate([data[1]] + vel_his, 1)\n",
    "        '''\n",
    "        \n",
    "        attr, state, relations, n_particles, n_shapes = \\\n",
    "                prepare_input(data, self.stat, self.args, self.phases_dict, self.verbose)\n",
    "\n",
    "        ### label\n",
    "        data_nxt = normalize(load_data(self.data_names, data_nxt_path), self.stat)\n",
    "\n",
    "        label = torch.FloatTensor(data_nxt[1][:n_particles])\n",
    "\n",
    "        return attr, state, relations, n_particles, n_shapes, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(data, stat, args, phases_dict, verbose=0, var=False):\n",
    "    '''\n",
    "    for a single hair\n",
    "    '''\n",
    "    positions, velocities, hairs_idx, shape_position = data\n",
    "    n_shapes = 1\n",
    "    hairs_idx_begin = [idx[0] for idx in hairs_idx]\n",
    "    n_particles = positions.shape[0]\n",
    "    R = 0.1\n",
    "    \n",
    "    ### object attributes\n",
    "    #   dim 10: [rigid, fluid, root_0, root_1, gripper_0, gripper_1, mass_inv,\n",
    "    #            clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep]\n",
    "    #   here we only consider the hairs but not the gripper, attr_dim = 1, attr = 0 for hair, attr = 1 for shapes\n",
    "    attr = np.zeros((n_particles+n_shapes, args.attr_dim))\n",
    "    \n",
    "    ### construct relations\n",
    "    Rr_idxs = []        # relation receiver idx list\n",
    "    Rs_idxs = []        # relation sender idx list\n",
    "    Ras = []            # relation attributes list\n",
    "    values = []         # relation value list (should be 1)\n",
    "    node_r_idxs = []    # list of corresponding receiver node idx\n",
    "    node_s_idxs = []    # list of corresponding sender node idx\n",
    "    psteps = []         # propagation steps\n",
    "    \n",
    "    ##### add env specific graph components\n",
    "    ### specify for shapes\n",
    "    rels = []\n",
    "    vals = []\n",
    "    \n",
    "    for i in range(n_shapes):\n",
    "        attr[n_particles+i, 0] = 1\n",
    "        dis = np.linalg.norm(positions[:,:2]- shape_position[0,:2],axis = 1)\n",
    "        nodes_rel = np.nonzero(dis <= R)[0]\n",
    "        # for relation between hair nodes and a gripper, we note it as 1\n",
    "        gripper = np.ones(nodes_rel.shape[0], dtype=np.int) * (n_particles+i)\n",
    "        rels += [np.stack([nodes_rel, gripper, np.ones(nodes_rel.shape[0])], axis=1)]\n",
    "        vals += [np.ones(nodes_rel.shape[0], dtype=np.int)]\n",
    "        \n",
    "    \n",
    "    ##### add relations between leaf particles\n",
    "    ## here we only consider the relations in a hair: the relation between a node and the nodes nearby\n",
    "    ## simple case for one hair, TEMPORARY 2 rels for one link\n",
    "    nodes_p = np.arange(n_particles-1)\n",
    "    val = np.linalg.norm(positions[1:]-positions[:-1],axis = 1)\n",
    "    R1 = np.stack([nodes_p,nodes_p+1, np.zeros(n_particles-1)],axis = 1)\n",
    "    R2 = np.stack([nodes_p+1,nodes_p, np.zeros(n_particles-1)],axis = 1)\n",
    "    rels += [np.concatenate([R1,R2],axis = 0)]\n",
    "    vals += [val,val]\n",
    "    \n",
    "    rels = np.concatenate(rels, 0)\n",
    "    vals = np.concatenate(vals, 0)\n",
    "    \n",
    "  #  print (vals.shape)\n",
    " #   print (rels.shape)\n",
    "    \n",
    "    \n",
    "    if rels.shape[0] > 0:\n",
    "        if verbose:\n",
    "            print(\"Relations neighbor\", rels.shape)\n",
    "        Rr_idxs.append(torch.LongTensor([rels[:, 0], np.arange(rels.shape[0])]))\n",
    "        Rs_idxs.append(torch.LongTensor([rels[:, 1], np.arange(rels.shape[0])]))\n",
    "        # Ra: relation attributes\n",
    "    #    Ra = np.zeros((rels.shape[0], args.relation_dim))  \n",
    "        Ra = rels[:,2]\n",
    "        Ras.append(torch.FloatTensor(Ra))\n",
    "        # values could be changed\n",
    "     #   values.append(torch.FloatTensor([1] * rels.shape[0]))\n",
    "      #  values.append(rels[:,2])\n",
    "        #### for hairs: values equals to the length of this segment\n",
    "        values.append(torch.FloatTensor(vals))\n",
    "        node_r_idxs.append(np.arange(n_particles))\n",
    "        node_s_idxs.append(np.arange(n_particles + n_shapes))\n",
    "        psteps.append(args.pstep)\n",
    "        \n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Attr shape (after hierarchy building):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "        print(\"Particle attr:\", np.sum(attr[:n_particles], axis=0))\n",
    "        print(\"Shape attr:\", np.sum(attr[n_particles:n_particles+n_shapes], axis=0))\n",
    "        print(\"Roots attr:\", np.sum(attr[n_particles+n_shapes:], axis=0))\n",
    "        \n",
    "        \n",
    "    ### normalize data\n",
    "    data = [positions, velocities]\n",
    "    data = normalize(data, stat, var)\n",
    "    positions, velocities = data[0], data[1]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Particle positions stats\")\n",
    "        print(positions.shape)\n",
    "        print(np.min(positions[:n_particles], 0))\n",
    "        print(np.max(positions[:n_particles], 0))\n",
    "        print(np.mean(positions[:n_particles], 0))\n",
    "        print(np.std(positions[:n_particles], 0))\n",
    "\n",
    "        show_vel_dim = 6 if args.env == 'RiceGrip' else 3\n",
    "        print(\"Velocities stats\")\n",
    "        print(velocities.shape)\n",
    "        print(np.mean(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        print(np.std(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        \n",
    "    state = torch.FloatTensor(np.concatenate([positions, velocities], axis=1))\n",
    "    attr = torch.FloatTensor(attr)\n",
    "    relations = [Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps]\n",
    "\n",
    "    return attr, state, relations, n_particles, n_shapes#, instance_idx\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {phase: PhysicsFleXDataset(\n",
    "    args, phase, phases_dict, args.verbose_data) for phase in ['train', 'valid']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['train'].load_data(args.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations neighbor (60, 3)\n",
      "Attr shape (after hierarchy building): (32, 1)\n",
      "Object attr: [1.]\n",
      "Particle attr: [0.]\n",
      "Shape attr: [1.]\n",
      "Roots attr: [0.]\n",
      "Particle positions stats\n",
      "(31, 3)\n",
      "[ 0.46258657 -1.82854189 -0.21068137]\n",
      "[ 0.46258657  1.6929597  -0.21068137]\n",
      "[ 0.46258657 -0.09084632 -0.21068137]\n",
      "[0.00000000e+00 1.05113194e+00 1.38777878e-16]\n",
      "Velocities stats\n",
      "(31, 3)\n",
      "[-0.08565128  2.97585322  0.01638161]\n",
      "[6.93889390e-17 1.51725969e+00 1.04083409e-17]\n"
     ]
    }
   ],
   "source": [
    "data = datasets['train'].__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr, state, rels, n_particles, n_shapes, label = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "           18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,  1,  2,  3,  4,  5,  6,\n",
       "            7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "           25, 26, 27, 28, 29, 30],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "           18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "           36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "           54, 55, 56, 57, 58, 59]])],\n",
       " [tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,  0,  1,  2,  3,  4,  5,\n",
       "            6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
       "           24, 25, 26, 27, 28, 29],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "           18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "           36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "           54, 55, 56, 57, 58, 59]])],\n",
       " [tensor([0.1146, 0.1135, 0.1172, 0.1160, 0.1160, 0.1156, 0.1153, 0.1150, 0.1147,\n",
       "          0.1143, 0.1140, 0.1136, 0.1132, 0.1129, 0.1125, 0.1121, 0.1118, 0.1114,\n",
       "          0.1111, 0.1107, 0.1104, 0.1101, 0.1097, 0.1094, 0.1091, 0.1088, 0.1085,\n",
       "          0.1082, 0.1079, 0.1077, 0.1146, 0.1135, 0.1172, 0.1160, 0.1160, 0.1156,\n",
       "          0.1153, 0.1150, 0.1147, 0.1143, 0.1140, 0.1136, 0.1132, 0.1129, 0.1125,\n",
       "          0.1121, 0.1118, 0.1114, 0.1111, 0.1107, 0.1104, 0.1101, 0.1097, 0.1094,\n",
       "          0.1091, 0.1088, 0.1085, 0.1082, 0.1079, 0.1077])],\n",
       " [tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])],\n",
       " [array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "         17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30])],\n",
       " [array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "         17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])],\n",
       " [2]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {x: torch.utils.data.DataLoader(\n",
    "    datasets[x], batch_size=args.batch_size,\n",
    "    shuffle=True if x == 'train' else False,\n",
    "    num_workers=args.num_workers,\n",
    "    collate_fn=collate_fn)\n",
    "    for x in ['train', 'valid']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations neighborRelations neighborRelations neighbor  "
     ]
    }
   ],
   "source": [
    "phase = 'train'\n",
    "for i, data in enumerate(dataloaders[phase]):\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr, state, rels, n_particles, n_shapes, label = data\n",
    "Ra, node_r_idx, node_s_idx, pstep = rels[3], rels[4], rels[5], rels[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rr, Rs = [], []\n",
    "for j in range(len(rels[0])):\n",
    "    Rr_idx, Rs_idx, values = rels[0][j], rels[1][j], rels[2][j]\n",
    "    Rr.append(torch.sparse.FloatTensor(Rr_idx, values, torch.Size([node_r_idx[j].shape[0], Ra[j].size(0)])))\n",
    "    Rs.append(torch.sparse.FloatTensor(Rs_idx, values, torch.Size([node_s_idx[j].shape[0], Ra[j].size(0)])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [attr, state, Rr, Rs, Ra, label]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predicted = model(\n",
    "                    attr, state, Rr, Rs, Ra, n_particles,\n",
    "                    node_r_idx, node_s_idx, pstep,\n",
    "                    instance_idx, phases_dict, args.verbose_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 31 is out of bounds for dimension 0 with size 31",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-d2c4dcaa354b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# receiver_state, sender_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mstate_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_r_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mstate_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_s_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mstate_r_rel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mstate_s_rel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRsp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 31 is out of bounds for dimension 0 with size 31"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "Rrp = Rr[s].t()\n",
    "Rsp = Rs[s].t()\n",
    "\n",
    "# receiver_attr, sender_attr\n",
    "attr_r = attr[node_r_idx[s]]\n",
    "attr_s = attr[node_s_idx[s]]\n",
    "attr_r_rel = Rrp.mm(attr_r)\n",
    "attr_s_rel = Rsp.mm(attr_s)\n",
    "\n",
    "# receiver_state, sender_state\n",
    "state_r = state[node_r_idx[s]]\n",
    "state_s = state[node_s_idx[s]]\n",
    "state_r_rel = Rrp.mm(state_r)\n",
    "state_s_rel = Rsp.mm(state_s)\n",
    "state_diff = state_r_rel - state_s_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4626,  1.6930, -0.2107, -0.0857, -0.0317,  0.0164],\n",
       "        [ 0.4626,  1.5715, -0.2107, -0.0857, -0.0620,  0.0164],\n",
       "        [ 0.4626,  1.4512, -0.2107, -0.0857, -0.1093,  0.0164],\n",
       "        [ 0.4626,  1.3262, -0.2107, -0.0857, -0.1674,  0.0164],\n",
       "        [ 0.4626,  1.2027, -0.2107, -0.0857, -0.2191,  0.0164],\n",
       "        [ 0.4626,  1.0792, -0.2107, -0.0857, -0.2719,  0.0164],\n",
       "        [ 0.4626,  0.9560, -0.2107, -0.0857, -0.3236,  0.0164],\n",
       "        [ 0.4626,  0.8331, -0.2107, -0.0857, -0.3747,  0.0164],\n",
       "        [ 0.4626,  0.7104, -0.2107, -0.0857, -0.4247,  0.0164],\n",
       "        [ 0.4626,  0.5881, -0.2107, -0.0857, -0.4736,  0.0164],\n",
       "        [ 0.4626,  0.4662, -0.2107, -0.0857, -0.5213,  0.0164],\n",
       "        [ 0.4626,  0.3446, -0.2107, -0.0857, -0.5677,  0.0164],\n",
       "        [ 0.4626,  0.2233, -0.2107, -0.0857, -0.6124,  0.0164],\n",
       "        [ 0.4626,  0.1024, -0.2107, -0.0857, -0.6556,  0.0164],\n",
       "        [ 0.4626, -0.0180, -0.2107, -0.0857, -0.6970,  0.0164],\n",
       "        [ 0.4626, -0.1381, -0.2107, -0.0857, -0.7364,  0.0164],\n",
       "        [ 0.4626, -0.2577, -0.2107, -0.0857, -0.7739,  0.0164],\n",
       "        [ 0.4626, -0.3769, -0.2107, -0.0857, -0.8092,  0.0164],\n",
       "        [ 0.4626, -0.4956, -0.2107, -0.0857, -0.8423,  0.0164],\n",
       "        [ 0.4626, -0.6139, -0.2107, -0.0857, -0.8731,  0.0164],\n",
       "        [ 0.4626, -0.7317, -0.2107, -0.0857, -0.9015,  0.0164],\n",
       "        [ 0.4626, -0.8490, -0.2107, -0.0857, -0.9274,  0.0164],\n",
       "        [ 0.4626, -0.9658, -0.2107, -0.0857, -0.9507,  0.0164],\n",
       "        [ 0.4626, -1.0821, -0.2107, -0.0857, -0.9714,  0.0164],\n",
       "        [ 0.4626, -1.1979, -0.2107, -0.0857, -0.9893,  0.0164],\n",
       "        [ 0.4626, -1.3132, -0.2107, -0.0857, -1.0045,  0.0164],\n",
       "        [ 0.4626, -1.4279, -0.2107, -0.0857, -1.0169,  0.0164],\n",
       "        [ 0.4626, -1.5422, -0.2107, -0.0857, -1.0265,  0.0164],\n",
       "        [ 0.4626, -1.6559, -0.2107, -0.0857, -1.0333,  0.0164],\n",
       "        [ 0.4626, -1.7691, -0.2107, -0.0857, -1.0368,  0.0164],\n",
       "        [ 0.4626, -1.8819, -0.2107, -0.0857, -1.0389,  0.0164]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = gen_PyFleX(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, datas = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions, velocities, hair_idx, shape_position = datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(data, stat, args, phases_dict, verbose=0, var=False):\n",
    "    '''\n",
    "    for a single hair\n",
    "    '''\n",
    "    positions, velocities, hairs_idx, shape_position = data\n",
    "    n_shapes = 1\n",
    "    hairs_idx_begin = [idx[0] for idx in hairs_idx]\n",
    "    n_particles = positions.shape[0]\n",
    "    R = 0.1\n",
    "    R = 2\n",
    "    \n",
    "    ### object attributes\n",
    "    #   dim 10: [rigid, fluid, root_0, root_1, gripper_0, gripper_1, mass_inv,\n",
    "    #            clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep]\n",
    "    #   here we only consider the hairs but not the gripper, attr_dim = 1, attr = 0 for hair, attr = 1 for shapes\n",
    "    attr = np.zeros((n_particles+n_shapes, args.attr_dim))\n",
    "    \n",
    "    ### construct relations\n",
    "    Rr_idxs = []        # relation receiver idx list\n",
    "    Rs_idxs = []        # relation sender idx list\n",
    "    Ras = []            # relation attributes list\n",
    "    values = []         # relation value list (should be 1)\n",
    "    node_r_idxs = []    # list of corresponding receiver node idx\n",
    "    node_s_idxs = []    # list of corresponding sender node idx\n",
    "    psteps = []         # propagation steps\n",
    "    \n",
    "    ##### add env specific graph components\n",
    "    ### specify for shapes\n",
    "    rels = []\n",
    "    vals = []\n",
    "    \n",
    "    for i in range(n_shapes):\n",
    "        attr[n_particles+i, 0] = 1\n",
    "        dis = np.linalg.norm(positions[:,:2]- shape_position[0,:2],axis = 1)\n",
    "        nodes_rel = np.nonzero(dis <= R)[0]\n",
    "        # for relation between hair nodes and a gripper, we note it as 1\n",
    "        gripper = np.ones(nodes_rel.shape[0], dtype=np.int) * (n_particles+i)\n",
    "        rels += [np.stack([nodes_rel, gripper, np.ones(nodes_rel.shape[0])], axis=1)]\n",
    "        vals += [np.ones(nodes_rel.shape[0], dtype=np.int)]\n",
    "        \n",
    "    \n",
    "    ##### add relations between leaf particles\n",
    "    ## here we only consider the relations in a hair: the relation between a node and the nodes nearby\n",
    "    ## simple case for one hair, TEMPORARY 2 rels for one link\n",
    "    nodes_p = np.arange(n_particles-1)\n",
    "    val = np.linalg.norm(positions[1:]-positions[:-1],axis = 1)\n",
    "    R1 = np.stack([nodes_p,nodes_p+1, np.zeros(n_particles-1)],axis = 1)\n",
    "    R2 = np.stack([nodes_p+1,nodes_p, np.zeros(n_particles-1)],axis = 1)\n",
    "    rels += [np.concatenate([R1,R2],axis = 0)]\n",
    "    vals += [val,val]\n",
    "    \n",
    "    rels = np.concatenate(rels, 0)\n",
    "    vals = np.concatenate(vals, 0)\n",
    "    \n",
    "  #  print (vals.shape)\n",
    " #   print (rels.shape)\n",
    "    \n",
    "    \n",
    "    if rels.shape[0] > 0:\n",
    "        if verbose:\n",
    "            print(\"Relations neighbor\", rels.shape)\n",
    "        Rr_idxs.append(torch.LongTensor([rels[:, 0], np.arange(rels.shape[0])]))\n",
    "        Rs_idxs.append(torch.LongTensor([rels[:, 1], np.arange(rels.shape[0])]))\n",
    "        # Ra: relation attributes\n",
    "    #    Ra = np.zeros((rels.shape[0], args.relation_dim))  \n",
    "        Ra = rels[:,2]\n",
    "        Ras.append(torch.FloatTensor(Ra))\n",
    "        # values could be changed\n",
    "     #   values.append(torch.FloatTensor([1] * rels.shape[0]))\n",
    "      #  values.append(rels[:,2])\n",
    "        #### for hairs: values equals to the length of this segment\n",
    "        values.append(vals)\n",
    "        node_r_idxs.append(np.arange(n_particles))\n",
    "        node_s_idxs.append(np.arange(n_particles + n_shapes))\n",
    "        psteps.append(args.pstep)\n",
    "        \n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Attr shape (after hierarchy building):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "        print(\"Particle attr:\", np.sum(attr[:n_particles], axis=0))\n",
    "        print(\"Shape attr:\", np.sum(attr[n_particles:n_particles+n_shapes], axis=0))\n",
    "        print(\"Roots attr:\", np.sum(attr[n_particles+n_shapes:], axis=0))\n",
    "        \n",
    "        \n",
    "    ### normalize data\n",
    "    data = [positions, velocities]\n",
    "    data = normalize(data, stat, var)\n",
    "    positions, velocities = data[0], data[1]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Particle positions stats\")\n",
    "        print(positions.shape)\n",
    "        print(np.min(positions[:n_particles], 0))\n",
    "        print(np.max(positions[:n_particles], 0))\n",
    "        print(np.mean(positions[:n_particles], 0))\n",
    "        print(np.std(positions[:n_particles], 0))\n",
    "\n",
    "        show_vel_dim = 6 if args.env == 'RiceGrip' else 3\n",
    "        print(\"Velocities stats\")\n",
    "        print(velocities.shape)\n",
    "        print(np.mean(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        print(np.std(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        \n",
    "    state = torch.FloatTensor(np.concatenate([positions, velocities], axis=1))\n",
    "    attr = torch.FloatTensor(attr)\n",
    "    relations = [Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps]\n",
    "\n",
    "    return attr, state, relations, n_particles, n_shapes#, instance_idx\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr, state, relations, n_particles, n_shapes = prepare_input(data, stat, args, phases_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps = relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "          31, 31, 31, 31, 31, 31, 31, 31, 31, 31,  1,  2,  3,  4,  5,  6,  7,  8,\n",
       "           9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "          27, 28, 29, 30,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "          14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "         [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "          36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "          54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
       "          72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]])]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rs_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.11461496, 0.11352849,\n",
       "        0.11723447, 0.11598921, 0.11598206, 0.11562109, 0.1153357 ,\n",
       "        0.11500478, 0.11466742, 0.11431694, 0.1139586 , 0.11359477,\n",
       "        0.11322808, 0.11286092, 0.11249518, 0.11213303, 0.11177492,\n",
       "        0.11142218, 0.11107552, 0.11073506, 0.11040092, 0.11007237,\n",
       "        0.10974967, 0.10943198, 0.10911858, 0.10881019, 0.1085012 ,\n",
       "        0.10820907, 0.10786438, 0.10772401, 0.11461496, 0.11352849,\n",
       "        0.11723447, 0.11598921, 0.11598206, 0.11562109, 0.1153357 ,\n",
       "        0.11500478, 0.11466742, 0.11431694, 0.1139586 , 0.11359477,\n",
       "        0.11322808, 0.11286092, 0.11249518, 0.11213303, 0.11177492,\n",
       "        0.11142218, 0.11107552, 0.11073506, 0.11040092, 0.11007237,\n",
       "        0.10974967, 0.10943198, 0.10911858, 0.10881019, 0.1085012 ,\n",
       "        0.10820907, 0.10786438, 0.10772401])]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(data, stat, args, phases_dict, verbose=0, var=False):\n",
    "\n",
    "    # Arrangement:\n",
    "    # particles, shapes, roots\n",
    "\n",
    "    if args.env == 'RiceGrip':\n",
    "        positions, velocities, shape_quats, clusters, scene_params = data\n",
    "        n_shapes = shape_quats.size(0) if var else shape_quats.shape[0]\n",
    "    elif args.env == 'FluidShake':\n",
    "        positions, velocities, shape_quats, scene_params = data\n",
    "        n_shapes = shape_quats.size(0) if var else shape_quats.shape[0]\n",
    "        clusters = None\n",
    "    elif args.env == 'BoxBath':\n",
    "        positions, velocities, clusters = data\n",
    "        n_shapes = 0\n",
    "    elif args.env == 'FluidFall':\n",
    "        positions, velocities = data\n",
    "        n_shapes = 0\n",
    "        clusters = None\n",
    "    elif args.env == 'Hairs':\n",
    "        positions, velocities, hairs_idx = data\n",
    "        n_shapes = 1\n",
    "        hairs_idx_begin = [idx[0] for idx in hairs_idx]\n",
    "        clusters = None\n",
    "\n",
    "    count_nodes = positions.size(0) if var else positions.shape[0]\n",
    "    n_particles = count_nodes - n_shapes\n",
    "\n",
    "    if verbose:\n",
    "        print(\"positions\", positions.shape)\n",
    "        print(\"velocities\", velocities.shape)\n",
    "\n",
    "        print(\"n_particles\", n_particles)\n",
    "        print(\"n_shapes\", n_shapes)\n",
    "        if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            print(\"shape_quats\", shape_quats.shape)\n",
    "\n",
    "    ### instance idx\n",
    "    #   instance_idx (n_instance + 1): start idx of instance\n",
    "    if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "        instance_idx = [0, n_particles]\n",
    "    elif args.env == 'Hairs':\n",
    "        instance_idx = [0, count_nodes]\n",
    "    else:\n",
    "        instance_idx = phases_dict[\"instance_idx\"]\n",
    "    if verbose:\n",
    "        print(\"Instance_idx:\", instance_idx)\n",
    "\n",
    "\n",
    "    ### object attributes\n",
    "    #   dim 10: [rigid, fluid, root_0, root_1, gripper_0, gripper_1, mass_inv,\n",
    "    #            clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep]\n",
    "    attr = np.zeros((count_nodes, args.attr_dim))\n",
    "    # no need to include mass for now\n",
    "    # attr[:, 6] = positions[:, -1].data.cpu().numpy() if var else positions[:, -1] # mass_inv\n",
    "    if args.env == 'RiceGrip':\n",
    "        # clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep\n",
    "        attr[:, -3:] = scene_params[-3:]\n",
    "\n",
    "\n",
    "    ### construct relations\n",
    "    Rr_idxs = []        # relation receiver idx list\n",
    "    Rs_idxs = []        # relation sender idx list\n",
    "    Ras = []            # relation attributes list\n",
    "    values = []         # relation value list (should be 1)\n",
    "    node_r_idxs = []    # list of corresponding receiver node idx\n",
    "    node_s_idxs = []    # list of corresponding sender node idx\n",
    "    psteps = []         # propagation steps\n",
    "\n",
    "    ##### add env specific graph components\n",
    "    rels = []\n",
    "    if args.env == 'RiceGrip':\n",
    "        # nodes = np.arange(n_particles)\n",
    "        for i in range(n_shapes):\n",
    "            attr[n_particles + i, 2 + i] = 1\n",
    "\n",
    "            pos = positions.data.cpu().numpy() if var else positions\n",
    "            dis = np.linalg.norm(\n",
    "                pos[:n_particles, 3:6:2] - pos[n_particles + i, 3:6:2], axis=1)\n",
    "            nodes = np.nonzero(dis < 0.3)[0]\n",
    "\n",
    "            if verbose:\n",
    "                visualize_neighbors(positions, positions, 0, nodes)\n",
    "                print(np.sort(dis)[:10])\n",
    "\n",
    "            gripper = np.ones(nodes.shape[0], dtype=np.int) * (n_particles + i)\n",
    "            rels += [np.stack([nodes, gripper, np.ones(nodes.shape[0])], axis=1)]\n",
    "            \n",
    "    elif args.env == 'Hairs':\n",
    "        # TODO: add relations between the hairs and the stick\n",
    "        attr[:,0] = 1\n",
    "        pass\n",
    "\n",
    "    elif args.env == 'FluidShake':\n",
    "        for i in range(n_shapes):\n",
    "            attr[n_particles + i, 1 + i] = 1\n",
    "\n",
    "            pos = positions.data.cpu().numpy() if var else positions\n",
    "            if i == 0:\n",
    "                # floor\n",
    "                dis = pos[:n_particles, 1] - pos[n_particles + i, 1]\n",
    "            elif i == 1:\n",
    "                # left\n",
    "                dis = pos[:n_particles, 0] - pos[n_particles + i, 0]\n",
    "            elif i == 2:\n",
    "                # right\n",
    "                dis = pos[n_particles + i, 0] - pos[:n_particles, 0]\n",
    "            elif i == 3:\n",
    "                # back\n",
    "                dis = pos[:n_particles, 2] - pos[n_particles + i, 2]\n",
    "            elif i == 4:\n",
    "                # front\n",
    "                dis = pos[n_particles + i, 2] - pos[:n_particles, 2]\n",
    "            else:\n",
    "                raise AssertionError(\"more shapes than expected\")\n",
    "            nodes = np.nonzero(dis < 0.1)[0]\n",
    "\n",
    "            if verbose:\n",
    "                visualize_neighbors(positions, positions, 0, nodes)\n",
    "                print(np.sort(dis)[:10])\n",
    "\n",
    "            wall = np.ones(nodes.shape[0], dtype=np.int) * (n_particles + i)\n",
    "            rels += [np.stack([nodes, wall, np.ones(nodes.shape[0])], axis=1)]\n",
    "\n",
    "    if verbose and len(rels) > 0:\n",
    "        print(np.concatenate(rels, 0).shape)\n",
    "\n",
    "    ##### add relations between leaf particles\n",
    "    for i in range(len(instance_idx) - 1):\n",
    "        st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "\n",
    "        if verbose:\n",
    "            print('instance #%d' % i, st, ed)\n",
    "\n",
    "        if args.env == 'BoxBath':\n",
    "            if phases_dict['material'][i] == 'rigid':\n",
    "                attr[st:ed, 0] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.concatenate((np.arange(st), np.arange(ed, n_particles)))\n",
    "            elif phases_dict['material'][i] == 'fluid':\n",
    "                attr[st:ed, 1] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.arange(n_particles)\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported materials\")\n",
    "                \n",
    "        elif args.env == 'Hairs':\n",
    "            # TODO: add relations between the hairs and the stick\n",
    "            pass\n",
    "            \n",
    "          #  if ed not in hairs_idx_begin:\n",
    "             #   queries = np.arange(st, ed)\n",
    "              #  anchors = np.arange(n_particles)\n",
    "\n",
    "        elif args.env == 'FluidFall' or args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            if phases_dict['material'][i] == 'fluid':\n",
    "                attr[st:ed, 0] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.arange(n_particles)\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported materials\")\n",
    "\n",
    "        else:\n",
    "            raise AssertionError(\"Unsupported materials\")\n",
    "\n",
    "        # st_time = time.time()\n",
    "        pos = positions\n",
    "        pos = pos[:, -3:]\n",
    "        if args.env == 'Hairs':\n",
    "            #TODO\n",
    "            pass\n",
    "        else:\n",
    "            rels += find_relations_neighbor(pos, queries, anchors, args.neighbor_radius, 2, var)\n",
    "            # return list of [receiver, sender, relation_type]\n",
    "        # print(\"Time on neighbor search\", time.time() - st_time)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Attr shape (after add env specific graph components):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "\n",
    "    rels = np.concatenate(rels, 0)\n",
    "    if rels.shape[0] > 0:\n",
    "        if verbose:\n",
    "            print(\"Relations neighbor\", rels.shape)\n",
    "        Rr_idxs.append(torch.LongTensor([rels[:, 0], np.arange(rels.shape[0])]))\n",
    "        Rs_idxs.append(torch.LongTensor([rels[:, 1], np.arange(rels.shape[0])]))\n",
    "        Ra = np.zeros((rels.shape[0], args.relation_dim))\n",
    "        Ras.append(torch.FloatTensor(Ra))\n",
    "        values.append(torch.FloatTensor([1] * rels.shape[0]))\n",
    "        node_r_idxs.append(np.arange(n_particles))\n",
    "        node_s_idxs.append(np.arange(n_particles + n_shapes))\n",
    "        psteps.append(args.pstep)\n",
    "\n",
    "    if verbose:\n",
    "        print('clusters', clusters)\n",
    "\n",
    "    # add heirarchical relations per instance\n",
    "    cnt_clusters = 0\n",
    "    for i in range(len(instance_idx) - 1):\n",
    "        st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "        n_root_level = len(phases_dict[\"root_num\"][i])\n",
    "\n",
    "        if n_root_level > 0:\n",
    "            attr, positions, velocities, count_nodes, \\\n",
    "            rels, node_r_idx, node_s_idx, pstep = \\\n",
    "                    make_hierarchy(args.env, attr, positions, velocities, i, st, ed,\n",
    "                                   phases_dict, count_nodes, clusters[cnt_clusters], verbose, var)\n",
    "\n",
    "            for j in range(len(rels)):\n",
    "                if verbose:\n",
    "                    print(\"Relation instance\", j, rels[j].shape)\n",
    "                Rr_idxs.append(torch.LongTensor([rels[j][:, 0], np.arange(rels[j].shape[0])]))\n",
    "                Rs_idxs.append(torch.LongTensor([rels[j][:, 1], np.arange(rels[j].shape[0])]))\n",
    "                Ra = np.zeros((rels[j].shape[0], args.relation_dim)); Ra[:, 0] = 1\n",
    "                Ras.append(torch.FloatTensor(Ra))\n",
    "                values.append(torch.FloatTensor([1] * rels[j].shape[0]))\n",
    "                node_r_idxs.append(node_r_idx[j])\n",
    "                node_s_idxs.append(node_s_idx[j])\n",
    "                psteps.append(pstep[j])\n",
    "\n",
    "            cnt_clusters += 1\n",
    "\n",
    "    if verbose:\n",
    "        if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            print(\"Scene_params:\", scene_params)\n",
    "\n",
    "        print(\"Attr shape (after hierarchy building):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "        print(\"Particle attr:\", np.sum(attr[:n_particles], axis=0))\n",
    "        print(\"Shape attr:\", np.sum(attr[n_particles:n_particles+n_shapes], axis=0))\n",
    "        print(\"Roots attr:\", np.sum(attr[n_particles+n_shapes:], axis=0))\n",
    "\n",
    "    ### normalize data\n",
    "    data = [positions, velocities]\n",
    "    data = normalize(data, stat, var)\n",
    "    positions, velocities = data[0], data[1]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Particle positions stats\")\n",
    "        print(positions.shape)\n",
    "        print(np.min(positions[:n_particles], 0))\n",
    "        print(np.max(positions[:n_particles], 0))\n",
    "        print(np.mean(positions[:n_particles], 0))\n",
    "        print(np.std(positions[:n_particles], 0))\n",
    "\n",
    "        show_vel_dim = 6 if args.env == 'RiceGrip' else 3\n",
    "        print(\"Velocities stats\")\n",
    "        print(velocities.shape)\n",
    "        print(np.mean(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        print(np.std(velocities[:n_particles, :show_vel_dim], 0))\n",
    "\n",
    "    if args.env == 'RiceGrip':\n",
    "        if var:\n",
    "            quats = torch.cat(\n",
    "                [Variable(torch.zeros(n_particles, 4).cuda()), shape_quats,\n",
    "                 Variable(torch.zeros(count_nodes - n_particles - n_shapes, 4).cuda())], 0)\n",
    "            state = torch.cat([positions, velocities, quats], 1)\n",
    "        else:\n",
    "            quat_null = np.array([[0., 0., 0., 0.]])\n",
    "            quats = np.repeat(quat_null, [count_nodes], axis=0)\n",
    "            quats[n_particles:n_particles + n_shapes] = shape_quats\n",
    "            # if args.eval == 0:\n",
    "            # quats += np.random.randn(quats.shape[0], 4) * 0.05\n",
    "            state = torch.FloatTensor(np.concatenate([positions, velocities, quats], axis=1))\n",
    "    else:\n",
    "        if var:\n",
    "            state = torch.cat([positions, velocities], 1)\n",
    "        else:\n",
    "            state = torch.FloatTensor(np.concatenate([positions, velocities], axis=1))\n",
    "\n",
    "    if verbose:\n",
    "        for i in range(count_nodes - 1):\n",
    "            if np.sum(np.abs(attr[i] - attr[i + 1])) > 1e-6:\n",
    "                print(i, attr[i], attr[i + 1])\n",
    "\n",
    "        for i in range(len(Ras)):\n",
    "            print(i, np.min(node_r_idxs[i]), np.max(node_r_idxs[i]), np.min(node_s_idxs[i]), np.max(node_s_idxs[i]))\n",
    "\n",
    "    attr = torch.FloatTensor(attr)\n",
    "    relations = [Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps]\n",
    "\n",
    "    return attr, state, relations, n_particles, n_shapes, instance_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_PyFleX(info):\n",
    "\n",
    "    env, root_num = info['env'], info['root_num']\n",
    "    thread_idx, data_dir, data_names = info['thread_idx'], info['data_dir'], info['data_names']\n",
    "    n_rollout, n_instance = info['n_rollout'], info['n_instance']\n",
    "    time_step, time_step_clip = info['time_step'], info['time_step_clip']\n",
    "    shape_state_dim, dt = info['shape_state_dim'], info['dt']\n",
    "\n",
    "    env_idx = info['env_idx']\n",
    "\n",
    "    np.random.seed(round(time.time() * 1000 + thread_idx) % 2**32)\n",
    "    \n",
    "    stats = [init_stat(3), init_stat(3)]\n",
    "\n",
    "    import pyflex\n",
    "    pyflex.init()\n",
    "\n",
    "    for i in range(n_rollout):\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"%d / %d\" % (i, n_rollout))\n",
    "\n",
    "        rollout_idx = thread_idx * n_rollout + i\n",
    "        rollout_dir = os.path.join(data_dir, str(rollout_idx))\n",
    "        os.system('mkdir -p ' + rollout_dir)\n",
    "        \n",
    "        env_idx = 10\n",
    "        # scene_params: [len(box) at dim x,len(box) at dim y,len(box) at dim z, num_hair per circle, num_circle]\n",
    "        box_size = [0.1,0.1,1.5]\n",
    "        N_p_cirlcle = 50\n",
    "        N_circle = 20\n",
    "        N_hairs = N_circle*N_p_cirlcle\n",
    "\n",
    "\n",
    "        scene_params = np.array(box_size + [N_p_cirlcle] + [N_circle])\n",
    "\n",
    "        pyflex.set_scene(env_idx, scene_params, thread_idx)\n",
    "        n_particles = pyflex.get_n_particles()\n",
    "        N_particles_per_hair = int(n_particles/N_hairs)\n",
    "        idx_begins = np.arange(N_hairs)*N_particles_per_hair\n",
    "        idx_hairs = [[i,i+N_particles_per_hair-1] for i in idx_begins]\n",
    "\n",
    "        positions = np.zeros((time_step, n_particles, 3), dtype=np.float32)\n",
    "        velocities = np.zeros((time_step, n_particles, 3), dtype=np.float32)\n",
    "\n",
    "        for j in range(time_step_clip):\n",
    "            p_clip = pyflex.get_positions().reshape(-1, 4)[:, :3]\n",
    "            pyflex.step()\n",
    "\n",
    "        for j in range(time_step):\n",
    "            positions[j] = pyflex.get_positions().reshape(-1, 4)[:, :3]\n",
    "            if j == 0:\n",
    "                velocities[j] = (positions[j] - p_clip) / dt\n",
    "            else:\n",
    "                velocities[j] = (positions[j] - positions[j - 1]) / dt\n",
    "\n",
    "            pyflex.step()\n",
    "            data = [positions[j], velocities[j], idx_hairs]\n",
    "            store_data(data_names, data, os.path.join(rollout_dir, str(j) + '.h5'))\n",
    "        \n",
    "        # change dtype for more accurate stat calculation\n",
    "        # only normalize positions and velocities\n",
    "        datas = [positions.astype(np.float64), velocities.astype(np.float64)]\n",
    "\n",
    "        for j in range(len(stats)): \n",
    "            # here j = 2, refers to positions and velocities\n",
    "            stat = init_stat(stats[j].shape[0]) \n",
    "            # stat= np.zeros((3,3))\n",
    "            stat[:, 0] = np.mean(datas[j], axis=(0, 1))[:]\n",
    "            stat[:, 1] = np.std(datas[j], axis=(0, 1))[:]\n",
    "            stat[:, 2] = datas[j].shape[0] * datas[j].shape[1] # time_step*n_particles\n",
    "            stats[j] = combine_stat(stats[j], stat)\n",
    "\n",
    "    pyflex.clean()\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsFleXDataset(Dataset):\n",
    "\n",
    "    def __init__(self, args, phase, phases_dict, verbose):\n",
    "        self.args = args\n",
    "        self.phase = phase\n",
    "        self.phases_dict = phases_dict\n",
    "        self.verbose = verbose\n",
    "        self.data_dir = os.path.join(self.args.dataf, phase)\n",
    "        self.stat_path = os.path.join(self.args.dataf, 'stat.h5')\n",
    "\n",
    "        os.system('mkdir -p ' + self.data_dir)\n",
    "\n",
    "        #    self.data_names = ['positions', 'velocities', 'shape_quats', 'clusters', 'scene_params']\n",
    "        self.data_names = ['positions', 'velocities','hair_idx']\n",
    "\n",
    "        ratio = self.args.train_valid_ratio\n",
    "        if phase == 'train':\n",
    "            self.n_rollout = int(self.args.n_rollout * ratio)\n",
    "        elif phase == 'valid':\n",
    "            self.n_rollout = self.args.n_rollout - int(self.args.n_rollout * ratio)\n",
    "        else:\n",
    "            raise AssertionError(\"Unknown phase\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rollout * (self.args.time_step - 1)\n",
    "\n",
    "    def load_data(self, name):\n",
    "        self.stat = load_data(self.data_names[:2], self.stat_path)\n",
    "        for i in range(len(self.stat)):\n",
    "            self.stat[i] = self.stat[i][-self.args.position_dim:, :]\n",
    "            # print(self.data_names[i], self.stat[i].shape)\n",
    "\n",
    "    def gen_data(self, name):\n",
    "        # if the data hasn't been generated, generate the data\n",
    "        print(\"Generating data ... n_rollout=%d, time_step=%d\" % (self.n_rollout, self.args.time_step))\n",
    "\n",
    "        infos = []\n",
    "        for i in range(self.args.num_workers):\n",
    "            info = {\n",
    "                'env': self.args.env,\n",
    "                'root_num': self.phases_dict['root_num'],\n",
    "                'thread_idx': i,\n",
    "                'data_dir': self.data_dir,\n",
    "                'data_names': self.data_names,\n",
    "                'n_rollout': self.n_rollout // self.args.num_workers,\n",
    "                'n_instance': self.args.n_instance,\n",
    "                'time_step': self.args.time_step,\n",
    "                'time_step_clip': self.args.time_step_clip,\n",
    "                'dt': self.args.dt,\n",
    "                'shape_state_dim': self.args.shape_state_dim}\n",
    "\n",
    "            info['env_idx'] = 10\n",
    "            infos.append(info)\n",
    "\n",
    "        cores = self.args.num_workers\n",
    "        pool = mp.Pool(processes=cores)\n",
    "        data = pool.map(gen_PyFleX, infos)\n",
    "\n",
    "        print(\"Training data generated, warpping up stats ...\")\n",
    "\n",
    "        if self.phase == 'train' and self.args.gen_stat:\n",
    "            # positions [x, y, z], velocities[xdot, ydot, zdot]\n",
    "            if self.args.env == 'RiceGrip':\n",
    "                self.stat = [init_stat(6), init_stat(6)]\n",
    "            else:\n",
    "                self.stat = [init_stat(3), init_stat(3)]\n",
    "            for i in range(len(data)):\n",
    "                for j in range(len(self.stat)):\n",
    "                    self.stat[j] = combine_stat(self.stat[j], data[i][j])\n",
    "            store_data(self.data_names[:2], self.stat, self.stat_path)\n",
    "        else:\n",
    "            print(\"Loading stat from %s ...\" % self.stat_path)\n",
    "            self.stat = load_data(self.data_names[:2], self.stat_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_rollout = idx // (self.args.time_step - 1)\n",
    "        idx_timestep = idx % (self.args.time_step - 1)\n",
    "\n",
    "        # ignore the first frame for env RiceGrip\n",
    "        if self.args.env == 'RiceGrip' and idx_timestep == 0:\n",
    "            idx_timestep = np.random.randint(1, self.args.time_step - 1)\n",
    "\n",
    "        data_path = os.path.join(self.data_dir, str(idx_rollout), str(idx_timestep) + '.h5')\n",
    "        data_nxt_path = os.path.join(self.data_dir, str(idx_rollout), str(idx_timestep + 1) + '.h5')\n",
    "\n",
    "        data = load_data(self.data_names, data_path)\n",
    "\n",
    "        vel_his = []\n",
    "        for i in range(self.args.n_his):\n",
    "            path = os.path.join(self.data_dir, str(idx_rollout), str(max(1, idx_timestep - i - 1)) + '.h5')\n",
    "            data_his = load_data(self.data_names, path)\n",
    "            vel_his.append(data_his[1])\n",
    "\n",
    "        data[1] = np.concatenate([data[1]] + vel_his, 1)\n",
    "\n",
    "        attr, state, relations, n_particles, n_shapes, instance_idx = \\\n",
    "                prepare_input(data, self.stat, self.args, self.phases_dict, self.verbose)\n",
    "\n",
    "        ### label\n",
    "        data_nxt = normalize(load_data(self.data_names, data_nxt_path), self.stat)\n",
    "\n",
    "        label = torch.FloatTensor(data_nxt[1][:n_particles])\n",
    "\n",
    "        return attr, state, relations, n_particles, n_shapes, instance_idx, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(data, stat, args, phases_dict, verbose=0, var=False):\n",
    "\n",
    "    # Arrangement:\n",
    "    # particles, shapes, roots\n",
    "\n",
    "    if args.env == 'RiceGrip':\n",
    "        positions, velocities, shape_quats, clusters, scene_params = data\n",
    "        n_shapes = shape_quats.size(0) if var else shape_quats.shape[0]\n",
    "    elif args.env == 'FluidShake':\n",
    "        positions, velocities, shape_quats, scene_params = data\n",
    "        n_shapes = shape_quats.size(0) if var else shape_quats.shape[0]\n",
    "        clusters = None\n",
    "    elif args.env == 'BoxBath':\n",
    "        positions, velocities, clusters = data\n",
    "        n_shapes = 0\n",
    "    elif args.env == 'FluidFall':\n",
    "        positions, velocities = data\n",
    "        n_shapes = 0\n",
    "        clusters = None\n",
    "    elif args.env == 'Hairs':\n",
    "        positions, velocities, hairs_idx = data\n",
    "        n_shapes = 1\n",
    "        hairs_idx_begin = [idx[0] for idx in hairs_idx]\n",
    "        clusters = None\n",
    "\n",
    "    count_nodes = positions.size(0) if var else positions.shape[0]\n",
    "    n_particles = count_nodes - n_shapes\n",
    "\n",
    "    if verbose:\n",
    "        print(\"positions\", positions.shape)\n",
    "        print(\"velocities\", velocities.shape)\n",
    "\n",
    "        print(\"n_particles\", n_particles)\n",
    "        print(\"n_shapes\", n_shapes)\n",
    "        if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            print(\"shape_quats\", shape_quats.shape)\n",
    "\n",
    "    ### instance idx\n",
    "    #   instance_idx (n_instance + 1): start idx of instance\n",
    "    if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "        instance_idx = [0, n_particles]\n",
    "    elif args.env == 'Hairs':\n",
    "        instance_idx = [0, count_nodes]\n",
    "    else:\n",
    "        instance_idx = phases_dict[\"instance_idx\"]\n",
    "    if verbose:\n",
    "        print(\"Instance_idx:\", instance_idx)\n",
    "\n",
    "\n",
    "    ### object attributes\n",
    "    #   dim 10: [rigid, fluid, root_0, root_1, gripper_0, gripper_1, mass_inv,\n",
    "    #            clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep]\n",
    "    attr = np.zeros((count_nodes, args.attr_dim))\n",
    "    # no need to include mass for now\n",
    "    # attr[:, 6] = positions[:, -1].data.cpu().numpy() if var else positions[:, -1] # mass_inv\n",
    "    if args.env == 'RiceGrip':\n",
    "        # clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep\n",
    "        attr[:, -3:] = scene_params[-3:]\n",
    "\n",
    "\n",
    "    ### construct relations\n",
    "    Rr_idxs = []        # relation receiver idx list\n",
    "    Rs_idxs = []        # relation sender idx list\n",
    "    Ras = []            # relation attributes list\n",
    "    values = []         # relation value list (should be 1)\n",
    "    node_r_idxs = []    # list of corresponding receiver node idx\n",
    "    node_s_idxs = []    # list of corresponding sender node idx\n",
    "    psteps = []         # propagation steps\n",
    "\n",
    "    ##### add env specific graph components\n",
    "    rels = []\n",
    "    if args.env == 'RiceGrip':\n",
    "        # nodes = np.arange(n_particles)\n",
    "        for i in range(n_shapes):\n",
    "            attr[n_particles + i, 2 + i] = 1\n",
    "\n",
    "            pos = positions.data.cpu().numpy() if var else positions\n",
    "            dis = np.linalg.norm(\n",
    "                pos[:n_particles, 3:6:2] - pos[n_particles + i, 3:6:2], axis=1)\n",
    "            nodes = np.nonzero(dis < 0.3)[0]\n",
    "\n",
    "            if verbose:\n",
    "                visualize_neighbors(positions, positions, 0, nodes)\n",
    "                print(np.sort(dis)[:10])\n",
    "\n",
    "            gripper = np.ones(nodes.shape[0], dtype=np.int) * (n_particles + i)\n",
    "            rels += [np.stack([nodes, gripper, np.ones(nodes.shape[0])], axis=1)]\n",
    "            \n",
    "    elif args.env == 'Hairs':\n",
    "        # TODO: add relations between the hairs and the stick\n",
    "        attr[:,0] = 1\n",
    "        pass\n",
    "\n",
    "    elif args.env == 'FluidShake':\n",
    "        for i in range(n_shapes):\n",
    "            attr[n_particles + i, 1 + i] = 1\n",
    "\n",
    "            pos = positions.data.cpu().numpy() if var else positions\n",
    "            if i == 0:\n",
    "                # floor\n",
    "                dis = pos[:n_particles, 1] - pos[n_particles + i, 1]\n",
    "            elif i == 1:\n",
    "                # left\n",
    "                dis = pos[:n_particles, 0] - pos[n_particles + i, 0]\n",
    "            elif i == 2:\n",
    "                # right\n",
    "                dis = pos[n_particles + i, 0] - pos[:n_particles, 0]\n",
    "            elif i == 3:\n",
    "                # back\n",
    "                dis = pos[:n_particles, 2] - pos[n_particles + i, 2]\n",
    "            elif i == 4:\n",
    "                # front\n",
    "                dis = pos[n_particles + i, 2] - pos[:n_particles, 2]\n",
    "            else:\n",
    "                raise AssertionError(\"more shapes than expected\")\n",
    "            nodes = np.nonzero(dis < 0.1)[0]\n",
    "\n",
    "            if verbose:\n",
    "                visualize_neighbors(positions, positions, 0, nodes)\n",
    "                print(np.sort(dis)[:10])\n",
    "\n",
    "            wall = np.ones(nodes.shape[0], dtype=np.int) * (n_particles + i)\n",
    "            rels += [np.stack([nodes, wall, np.ones(nodes.shape[0])], axis=1)]\n",
    "\n",
    "    if verbose and len(rels) > 0:\n",
    "        print(np.concatenate(rels, 0).shape)\n",
    "\n",
    "    ##### add relations between leaf particles\n",
    "    for i in range(len(instance_idx) - 1):\n",
    "        st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "\n",
    "        if verbose:\n",
    "            print('instance #%d' % i, st, ed)\n",
    "\n",
    "        if args.env == 'BoxBath':\n",
    "            if phases_dict['material'][i] == 'rigid':\n",
    "                attr[st:ed, 0] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.concatenate((np.arange(st), np.arange(ed, n_particles)))\n",
    "            elif phases_dict['material'][i] == 'fluid':\n",
    "                attr[st:ed, 1] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.arange(n_particles)\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported materials\")\n",
    "                \n",
    "        elif args.env == 'Hairs':\n",
    "            # TODO: add relations between the hairs and the stick\n",
    "            pass\n",
    "            \n",
    "          #  if ed not in hairs_idx_begin:\n",
    "             #   queries = np.arange(st, ed)\n",
    "              #  anchors = np.arange(n_particles)\n",
    "\n",
    "        elif args.env == 'FluidFall' or args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            if phases_dict['material'][i] == 'fluid':\n",
    "                attr[st:ed, 0] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.arange(n_particles)\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported materials\")\n",
    "\n",
    "        else:\n",
    "            raise AssertionError(\"Unsupported materials\")\n",
    "\n",
    "        # st_time = time.time()\n",
    "        pos = positions\n",
    "        pos = pos[:, -3:]\n",
    "        if args.env == 'Hairs':\n",
    "            #TODO\n",
    "            pass\n",
    "        else:\n",
    "            rels += find_relations_neighbor(pos, queries, anchors, args.neighbor_radius, 2, var)\n",
    "            # return list of [receiver, sender, relation_type]\n",
    "        # print(\"Time on neighbor search\", time.time() - st_time)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Attr shape (after add env specific graph components):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "\n",
    "    rels = np.concatenate(rels, 0)\n",
    "    if rels.shape[0] > 0:\n",
    "        if verbose:\n",
    "            print(\"Relations neighbor\", rels.shape)\n",
    "        Rr_idxs.append(torch.LongTensor([rels[:, 0], np.arange(rels.shape[0])]))\n",
    "        Rs_idxs.append(torch.LongTensor([rels[:, 1], np.arange(rels.shape[0])]))\n",
    "        Ra = np.zeros((rels.shape[0], args.relation_dim))\n",
    "        Ras.append(torch.FloatTensor(Ra))\n",
    "        values.append(torch.FloatTensor([1] * rels.shape[0]))\n",
    "        node_r_idxs.append(np.arange(n_particles))\n",
    "        node_s_idxs.append(np.arange(n_particles + n_shapes))\n",
    "        psteps.append(args.pstep)\n",
    "\n",
    "    if verbose:\n",
    "        print('clusters', clusters)\n",
    "\n",
    "    # add heirarchical relations per instance\n",
    "    cnt_clusters = 0\n",
    "    for i in range(len(instance_idx) - 1):\n",
    "        st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "        n_root_level = len(phases_dict[\"root_num\"][i])\n",
    "\n",
    "        if n_root_level > 0:\n",
    "            attr, positions, velocities, count_nodes, \\\n",
    "            rels, node_r_idx, node_s_idx, pstep = \\\n",
    "                    make_hierarchy(args.env, attr, positions, velocities, i, st, ed,\n",
    "                                   phases_dict, count_nodes, clusters[cnt_clusters], verbose, var)\n",
    "\n",
    "            for j in range(len(rels)):\n",
    "                if verbose:\n",
    "                    print(\"Relation instance\", j, rels[j].shape)\n",
    "                Rr_idxs.append(torch.LongTensor([rels[j][:, 0], np.arange(rels[j].shape[0])]))\n",
    "                Rs_idxs.append(torch.LongTensor([rels[j][:, 1], np.arange(rels[j].shape[0])]))\n",
    "                Ra = np.zeros((rels[j].shape[0], args.relation_dim)); Ra[:, 0] = 1\n",
    "                Ras.append(torch.FloatTensor(Ra))\n",
    "                values.append(torch.FloatTensor([1] * rels[j].shape[0]))\n",
    "                node_r_idxs.append(node_r_idx[j])\n",
    "                node_s_idxs.append(node_s_idx[j])\n",
    "                psteps.append(pstep[j])\n",
    "\n",
    "            cnt_clusters += 1\n",
    "\n",
    "    if verbose:\n",
    "        if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            print(\"Scene_params:\", scene_params)\n",
    "\n",
    "        print(\"Attr shape (after hierarchy building):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "        print(\"Particle attr:\", np.sum(attr[:n_particles], axis=0))\n",
    "        print(\"Shape attr:\", np.sum(attr[n_particles:n_particles+n_shapes], axis=0))\n",
    "        print(\"Roots attr:\", np.sum(attr[n_particles+n_shapes:], axis=0))\n",
    "\n",
    "    ### normalize data\n",
    "    data = [positions, velocities]\n",
    "    data = normalize(data, stat, var)\n",
    "    positions, velocities = data[0], data[1]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Particle positions stats\")\n",
    "        print(positions.shape)\n",
    "        print(np.min(positions[:n_particles], 0))\n",
    "        print(np.max(positions[:n_particles], 0))\n",
    "        print(np.mean(positions[:n_particles], 0))\n",
    "        print(np.std(positions[:n_particles], 0))\n",
    "\n",
    "        show_vel_dim = 6 if args.env == 'RiceGrip' else 3\n",
    "        print(\"Velocities stats\")\n",
    "        print(velocities.shape)\n",
    "        print(np.mean(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        print(np.std(velocities[:n_particles, :show_vel_dim], 0))\n",
    "\n",
    "    if args.env == 'RiceGrip':\n",
    "        if var:\n",
    "            quats = torch.cat(\n",
    "                [Variable(torch.zeros(n_particles, 4).cuda()), shape_quats,\n",
    "                 Variable(torch.zeros(count_nodes - n_particles - n_shapes, 4).cuda())], 0)\n",
    "            state = torch.cat([positions, velocities, quats], 1)\n",
    "        else:\n",
    "            quat_null = np.array([[0., 0., 0., 0.]])\n",
    "            quats = np.repeat(quat_null, [count_nodes], axis=0)\n",
    "            quats[n_particles:n_particles + n_shapes] = shape_quats\n",
    "            # if args.eval == 0:\n",
    "            # quats += np.random.randn(quats.shape[0], 4) * 0.05\n",
    "            state = torch.FloatTensor(np.concatenate([positions, velocities, quats], axis=1))\n",
    "    else:\n",
    "        if var:\n",
    "            state = torch.cat([positions, velocities], 1)\n",
    "        else:\n",
    "            state = torch.FloatTensor(np.concatenate([positions, velocities], axis=1))\n",
    "\n",
    "    if verbose:\n",
    "        for i in range(count_nodes - 1):\n",
    "            if np.sum(np.abs(attr[i] - attr[i + 1])) > 1e-6:\n",
    "                print(i, attr[i], attr[i + 1])\n",
    "\n",
    "        for i in range(len(Ras)):\n",
    "            print(i, np.min(node_r_idxs[i]), np.max(node_r_idxs[i]), np.min(node_s_idxs[i]), np.max(node_s_idxs[i]))\n",
    "\n",
    "    attr = torch.FloatTensor(attr)\n",
    "    relations = [Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps]\n",
    "\n",
    "    return attr, state, relations, n_particles, n_shapes, instance_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "datasets = {phase: PhysicsFleXDataset(\n",
    "    args, phase, phases_dict, args.verbose_data) for phase in ['train', 'valid']}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "datasets['train'].gen_data(args.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['train'].load_data(args.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "idx_rollout = idx // (datasets['train'].args.time_step - 1)\n",
    "idx_timestep = idx % (datasets['train'].args.time_step - 1)\n",
    "\n",
    "# ignore the first frame for env RiceGrip\n",
    "if datasets['train'].args.env == 'RiceGrip' and idx_timestep == 0:\n",
    "    idx_timestep = np.random.randint(1, datasets['train'].args.time_step - 1)\n",
    "\n",
    "data_path = os.path.join(datasets['train'].data_dir, str(idx_rollout), str(idx_timestep) + '.h5')\n",
    "data_nxt_path = os.path.join(datasets['train'].data_dir, str(idx_rollout), str(idx_timestep + 1) + '.h5')\n",
    "\n",
    "data = load_data(datasets['train'].data_names, data_path)\n",
    "\n",
    "vel_his = []\n",
    "for i in range(datasets['train'].args.n_his):\n",
    "    path = os.path.join(datasets['train'].data_dir, str(idx_rollout), str(max(1, idx_timestep - i - 1)) + '.h5')\n",
    "    data_his = load_data(datasets['train'].data_names, path)\n",
    "    vel_his.append(data_his[1])\n",
    "\n",
    "data[1] = np.concatenate([data[1]] + vel_his, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['train'].stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions, velocities, hairs_idx = data\n",
    "n_shapes = 1\n",
    "hairs_idx_begin = [idx[0] for idx in hairs_idx]\n",
    "hairs_idx_end = [idx[1] for idx in hairs_idx]\n",
    "\n",
    "clusters = None\n",
    "\n",
    "n_particles = positions.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions[:3,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_idx = [0, n_particles]\n",
    "### object attributes\n",
    "count_nodes = n_particles\n",
    "attr = np.zeros((count_nodes, args.attr_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### add env specific graph components\n",
    "rels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr[:n_particles,0] = 1\n",
    "#rels += find_relations_neighbor(pos, queries, anchors, args.neighbor_radius, 2, var)\n",
    "#queries = set of hairs, anchors = stick\n",
    "for i in range(n_particles):\n",
    "    if i in hairs_idx_begin:\n",
    "        receiver = [i+1]\n",
    "        sender = [i]\n",
    "        relation_type = [1]\n",
    "    elif i in hairs_idx_end:\n",
    "        receiver = [i-1]\n",
    "        sender = [i]\n",
    "        relation_type = [1]\n",
    "    else:\n",
    "        receiver = [i-1,i+1]\n",
    "        sender = [i, i]\n",
    "        relation_type = [1,1]\n",
    "    rels.append(np.stack([receiver, sender, relation_type], axis=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels = np.concatenate(rels, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### construct relations\n",
    "Rr_idxs = []        # relation receiver idx list\n",
    "Rs_idxs = []        # relation sender idx list\n",
    "Ras = []            # relation attributes list\n",
    "values = []         # relation value list (should be 1)\n",
    "node_r_idxs = []    # list of corresponding receiver node idx\n",
    "node_s_idxs = []    # list of corresponding sender node idx\n",
    "psteps = []         # propagation steps\n",
    "\n",
    "\n",
    "Rr_idxs.append(torch.LongTensor([rels[:, 0], np.arange(rels.shape[0])]))\n",
    "Rs_idxs.append(torch.LongTensor([rels[:, 1], np.arange(rels.shape[0])]))\n",
    "Ra = np.zeros((rels.shape[0], args.relation_dim))\n",
    "Ras.append(torch.FloatTensor(Ra))\n",
    "values.append(torch.FloatTensor([1] * rels.shape[0]))\n",
    "node_r_idxs.append(np.arange(n_particles))\n",
    "node_s_idxs.append(np.arange(n_particles + n_shapes))\n",
    "psteps.append(args.pstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [positions, velocities]\n",
    "stat = datasets['train'].stat\n",
    "data = normalize(data, stat)\n",
    "positions, velocities = data[0], data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = torch.FloatTensor(attr)\n",
    "relations = [Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(data, stat, args, phases_dict, verbose=0, var=False):\n",
    "    \n",
    "    positions, velocities, hairs_idx = data\n",
    "    n_shapes = 1\n",
    "    hairs_idx_begin = [idx[0] for idx in hairs_idx]\n",
    "    clusters = None\n",
    "\n",
    "    count_nodes = positions.size(0) if var else positions.shape[0]\n",
    "    n_particles = count_nodes - n_shapes\n",
    "\n",
    "    ### instance idx\n",
    "    #   instance_idx (n_instance + 1): start idx of instance\n",
    "    if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "        instance_idx = [0, n_particles]\n",
    "    elif args.env == 'Hairs':\n",
    "        instance_idx = [0, count_nodes]\n",
    "    else:\n",
    "        instance_idx = phases_dict[\"instance_idx\"]\n",
    "    if verbose:\n",
    "        print(\"Instance_idx:\", instance_idx)\n",
    "\n",
    "\n",
    "    ### object attributes\n",
    "    #   dim 10: [rigid, fluid, root_0, root_1, gripper_0, gripper_1, mass_inv,\n",
    "    #            clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep]\n",
    "    attr = np.zeros((count_nodes, args.attr_dim))\n",
    "    # no need to include mass for now\n",
    "    # attr[:, 6] = positions[:, -1].data.cpu().numpy() if var else positions[:, -1] # mass_inv\n",
    "    if args.env == 'RiceGrip':\n",
    "        # clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep\n",
    "        attr[:, -3:] = scene_params[-3:]\n",
    "\n",
    "\n",
    "    ### construct relations\n",
    "    Rr_idxs = []        # relation receiver idx list\n",
    "    Rs_idxs = []        # relation sender idx list\n",
    "    Ras = []            # relation attributes list\n",
    "    values = []         # relation value list (should be 1)\n",
    "    node_r_idxs = []    # list of corresponding receiver node idx\n",
    "    node_s_idxs = []    # list of corresponding sender node idx\n",
    "    psteps = []         # propagation steps\n",
    "\n",
    "    ##### add env specific graph components\n",
    "    rels = []\n",
    "    if args.env == 'RiceGrip':\n",
    "        # nodes = np.arange(n_particles)\n",
    "        for i in range(n_shapes):\n",
    "            attr[n_particles + i, 2 + i] = 1\n",
    "\n",
    "            pos = positions.data.cpu().numpy() if var else positions\n",
    "            dis = np.linalg.norm(\n",
    "                pos[:n_particles, 3:6:2] - pos[n_particles + i, 3:6:2], axis=1)\n",
    "            nodes = np.nonzero(dis < 0.3)[0]\n",
    "\n",
    "            if verbose:\n",
    "                visualize_neighbors(positions, positions, 0, nodes)\n",
    "                print(np.sort(dis)[:10])\n",
    "\n",
    "            gripper = np.ones(nodes.shape[0], dtype=np.int) * (n_particles + i)\n",
    "            rels += [np.stack([nodes, gripper, np.ones(nodes.shape[0])], axis=1)]\n",
    "            \n",
    "    elif args.env = 'Hairs':\n",
    "        # TODO: add relations between the hairs and the stick\n",
    "        attr[:,0] = 1\n",
    "        pass\n",
    "\n",
    "    elif args.env == 'FluidShake':\n",
    "        for i in range(n_shapes):\n",
    "            attr[n_particles + i, 1 + i] = 1\n",
    "\n",
    "            pos = positions.data.cpu().numpy() if var else positions\n",
    "            if i == 0:\n",
    "                # floor\n",
    "                dis = pos[:n_particles, 1] - pos[n_particles + i, 1]\n",
    "            elif i == 1:\n",
    "                # left\n",
    "                dis = pos[:n_particles, 0] - pos[n_particles + i, 0]\n",
    "            elif i == 2:\n",
    "                # right\n",
    "                dis = pos[n_particles + i, 0] - pos[:n_particles, 0]\n",
    "            elif i == 3:\n",
    "                # back\n",
    "                dis = pos[:n_particles, 2] - pos[n_particles + i, 2]\n",
    "            elif i == 4:\n",
    "                # front\n",
    "                dis = pos[n_particles + i, 2] - pos[:n_particles, 2]\n",
    "            else:\n",
    "                raise AssertionError(\"more shapes than expected\")\n",
    "            nodes = np.nonzero(dis < 0.1)[0]\n",
    "\n",
    "            if verbose:\n",
    "                visualize_neighbors(positions, positions, 0, nodes)\n",
    "                print(np.sort(dis)[:10])\n",
    "\n",
    "            wall = np.ones(nodes.shape[0], dtype=np.int) * (n_particles + i)\n",
    "            rels += [np.stack([nodes, wall, np.ones(nodes.shape[0])], axis=1)]\n",
    "\n",
    "    if verbose and len(rels) > 0:\n",
    "        print(np.concatenate(rels, 0).shape)\n",
    "\n",
    "    ##### add relations between leaf particles\n",
    "    for i in range(len(instance_idx) - 1):\n",
    "        st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "\n",
    "        if verbose:\n",
    "            print('instance #%d' % i, st, ed)\n",
    "\n",
    "        if args.env == 'BoxBath':\n",
    "            if phases_dict['material'][i] == 'rigid':\n",
    "                attr[st:ed, 0] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.concatenate((np.arange(st), np.arange(ed, n_particles)))\n",
    "            elif phases_dict['material'][i] == 'fluid':\n",
    "                attr[st:ed, 1] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.arange(n_particles)\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported materials\")\n",
    "                \n",
    "        elif args.env == 'Hairs':\n",
    "            # TODO: add relations between the hairs and the stick\n",
    "            pass\n",
    "            \n",
    "          #  if ed not in hairs_idx_begin:\n",
    "             #   queries = np.arange(st, ed)\n",
    "              #  anchors = np.arange(n_particles)\n",
    "\n",
    "        elif args.env == 'FluidFall' or args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            if phases_dict['material'][i] == 'fluid':\n",
    "                attr[st:ed, 0] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.arange(n_particles)\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported materials\")\n",
    "\n",
    "        else:\n",
    "            raise AssertionError(\"Unsupported materials\")\n",
    "\n",
    "        # st_time = time.time()\n",
    "        pos = positions\n",
    "        pos = pos[:, -3:]\n",
    "        if args.env == 'Hairs':\n",
    "            #TODO\n",
    "            pass\n",
    "        else:\n",
    "            rels += find_relations_neighbor(pos, queries, anchors, args.neighbor_radius, 2, var)\n",
    "            # return list of [receiver, sender, relation_type]\n",
    "        # print(\"Time on neighbor search\", time.time() - st_time)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Attr shape (after add env specific graph components):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "\n",
    "    rels = np.concatenate(rels, 0)\n",
    "    if rels.shape[0] > 0:\n",
    "        if verbose:\n",
    "            print(\"Relations neighbor\", rels.shape)\n",
    "        Rr_idxs.append(torch.LongTensor([rels[:, 0], np.arange(rels.shape[0])]))\n",
    "        Rs_idxs.append(torch.LongTensor([rels[:, 1], np.arange(rels.shape[0])]))\n",
    "        Ra = np.zeros((rels.shape[0], args.relation_dim))\n",
    "        Ras.append(torch.FloatTensor(Ra))\n",
    "        values.append(torch.FloatTensor([1] * rels.shape[0]))\n",
    "        node_r_idxs.append(np.arange(n_particles))\n",
    "        node_s_idxs.append(np.arange(n_particles + n_shapes))\n",
    "        psteps.append(args.pstep)\n",
    "\n",
    "    if verbose:\n",
    "        print('clusters', clusters)\n",
    "\n",
    "    # add heirarchical relations per instance\n",
    "    cnt_clusters = 0\n",
    "    for i in range(len(instance_idx) - 1):\n",
    "        st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "        n_root_level = len(phases_dict[\"root_num\"][i])\n",
    "\n",
    "        if n_root_level > 0:\n",
    "            attr, positions, velocities, count_nodes, \\\n",
    "            rels, node_r_idx, node_s_idx, pstep = \\\n",
    "                    make_hierarchy(args.env, attr, positions, velocities, i, st, ed,\n",
    "                                   phases_dict, count_nodes, clusters[cnt_clusters], verbose, var)\n",
    "\n",
    "            for j in range(len(rels)):\n",
    "                if verbose:\n",
    "                    print(\"Relation instance\", j, rels[j].shape)\n",
    "                Rr_idxs.append(torch.LongTensor([rels[j][:, 0], np.arange(rels[j].shape[0])]))\n",
    "                Rs_idxs.append(torch.LongTensor([rels[j][:, 1], np.arange(rels[j].shape[0])]))\n",
    "                Ra = np.zeros((rels[j].shape[0], args.relation_dim)); Ra[:, 0] = 1\n",
    "                Ras.append(torch.FloatTensor(Ra))\n",
    "                values.append(torch.FloatTensor([1] * rels[j].shape[0]))\n",
    "                node_r_idxs.append(node_r_idx[j])\n",
    "                node_s_idxs.append(node_s_idx[j])\n",
    "                psteps.append(pstep[j])\n",
    "\n",
    "            cnt_clusters += 1\n",
    "\n",
    "    if verbose:\n",
    "        if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            print(\"Scene_params:\", scene_params)\n",
    "\n",
    "        print(\"Attr shape (after hierarchy building):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "        print(\"Particle attr:\", np.sum(attr[:n_particles], axis=0))\n",
    "        print(\"Shape attr:\", np.sum(attr[n_particles:n_particles+n_shapes], axis=0))\n",
    "        print(\"Roots attr:\", np.sum(attr[n_particles+n_shapes:], axis=0))\n",
    "\n",
    "    ### normalize data\n",
    "    data = [positions, velocities]\n",
    "    data = normalize(data, stat, var)\n",
    "    positions, velocities = data[0], data[1]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Particle positions stats\")\n",
    "        print(positions.shape)\n",
    "        print(np.min(positions[:n_particles], 0))\n",
    "        print(np.max(positions[:n_particles], 0))\n",
    "        print(np.mean(positions[:n_particles], 0))\n",
    "        print(np.std(positions[:n_particles], 0))\n",
    "\n",
    "        show_vel_dim = 6 if args.env == 'RiceGrip' else 3\n",
    "        print(\"Velocities stats\")\n",
    "        print(velocities.shape)\n",
    "        print(np.mean(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        print(np.std(velocities[:n_particles, :show_vel_dim], 0))\n",
    "\n",
    "    if args.env == 'RiceGrip':\n",
    "        if var:\n",
    "            quats = torch.cat(\n",
    "                [Variable(torch.zeros(n_particles, 4).cuda()), shape_quats,\n",
    "                 Variable(torch.zeros(count_nodes - n_particles - n_shapes, 4).cuda())], 0)\n",
    "            state = torch.cat([positions, velocities, quats], 1)\n",
    "        else:\n",
    "            quat_null = np.array([[0., 0., 0., 0.]])\n",
    "            quats = np.repeat(quat_null, [count_nodes], axis=0)\n",
    "            quats[n_particles:n_particles + n_shapes] = shape_quats\n",
    "            # if args.eval == 0:\n",
    "            # quats += np.random.randn(quats.shape[0], 4) * 0.05\n",
    "            state = torch.FloatTensor(np.concatenate([positions, velocities, quats], axis=1))\n",
    "    else:\n",
    "        if var:\n",
    "            state = torch.cat([positions, velocities], 1)\n",
    "        else:\n",
    "            state = torch.FloatTensor(np.concatenate([positions, velocities], axis=1))\n",
    "\n",
    "    if verbose:\n",
    "        for i in range(count_nodes - 1):\n",
    "            if np.sum(np.abs(attr[i] - attr[i + 1])) > 1e-6:\n",
    "                print(i, attr[i], attr[i + 1])\n",
    "\n",
    "        for i in range(len(Ras)):\n",
    "            print(i, np.min(node_r_idxs[i]), np.max(node_r_idxs[i]), np.min(node_s_idxs[i]), np.max(node_s_idxs[i]))\n",
    "\n",
    "    attr = torch.FloatTensor(attr)\n",
    "    relations = [Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps]\n",
    "\n",
    "    return attr, state, relations, n_particles, n_shapes, instance_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attr, state, relations, n_particles, n_shapes, instance_idx = \\\n",
    "        prepare_input(data, self.stat, self.args, self.phases_dict, self.verbose)\n",
    "\n",
    "### label\n",
    "data_nxt = normalize(load_data(self.data_names, data_nxt_path), self.stat)\n",
    "\n",
    "label = torch.FloatTensor(data_nxt[1][:n_particles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets['train'].__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
