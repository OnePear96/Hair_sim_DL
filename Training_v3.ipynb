{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from models import *\n",
    "from data import *\n",
    "#from data import PhysicsFleXDataset, collate_fn\n",
    "\n",
    "from utils import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--relation_dim'], dest='relation_dim', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--pstep', type=int, default=2)\n",
    "parser.add_argument('--n_rollout', type=int, default=0)\n",
    "parser.add_argument('--time_step', type=int, default=0)\n",
    "parser.add_argument('--time_step_clip', type=int, default=0)\n",
    "parser.add_argument('--dt', type=float, default=1./60.)\n",
    "parser.add_argument('--nf_relation', type=int, default=300)\n",
    "parser.add_argument('--nf_particle', type=int, default=200)\n",
    "parser.add_argument('--nf_effect', type=int, default=200)\n",
    "parser.add_argument('--env', default='')\n",
    "parser.add_argument('--train_valid_ratio', type=float, default=0.9)\n",
    "parser.add_argument('--outf', default='files')\n",
    "parser.add_argument('--dataf', default='data')\n",
    "parser.add_argument('--num_workers', type=int, default=10)\n",
    "parser.add_argument('--gen_data', type=int, default=0)\n",
    "parser.add_argument('--gen_stat', type=int, default=0)\n",
    "parser.add_argument('--log_per_iter', type=int, default=1000)\n",
    "parser.add_argument('--ckp_per_iter', type=int, default=10000)\n",
    "parser.add_argument('--eval', type=int, default=0)\n",
    "parser.add_argument('--verbose_data', type=int, default=1)\n",
    "parser.add_argument('--verbose_model', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--n_instance', type=int, default=0)\n",
    "parser.add_argument('--n_stages', type=int, default=0)\n",
    "parser.add_argument('--n_his', type=int, default=0)\n",
    "\n",
    "parser.add_argument('--n_epoch', type=int, default=1000)\n",
    "parser.add_argument('--beta1', type=float, default=0.9)\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--batch_size', type=int, default=1)\n",
    "parser.add_argument('--forward_times', type=int, default=2)\n",
    "\n",
    "parser.add_argument('--resume_epoch', type=int, default=0)\n",
    "parser.add_argument('--resume_iter', type=int, default=0)\n",
    "\n",
    "# shape state:\n",
    "# [x, y, z, x_last, y_last, z_last, quat(4), quat_last(4)]\n",
    "parser.add_argument('--shape_state_dim', type=int, default=14)\n",
    "\n",
    "# object attributes:\n",
    "parser.add_argument('--attr_dim', type=int, default=0)\n",
    "\n",
    "# object state:\n",
    "parser.add_argument('--state_dim', type=int, default=0)\n",
    "parser.add_argument('--position_dim', type=int, default=0)\n",
    "\n",
    "# relation attr:\n",
    "parser.add_argument('--relation_dim', type=int, default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"--env SingleHair --gen_data 0\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/data_SingleHair\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phases_dict = dict()\n",
    "\n",
    "args.n_rollout = 50\n",
    "args.num_workers = 5\n",
    "args.gen_stat = 1\n",
    "\n",
    "args.dataf = 'data'\n",
    "\n",
    "# object states:\n",
    "# [x, y, z, xdot, ydot, zdot]\n",
    "args.state_dim = 6\n",
    "args.position_dim = 3\n",
    "\n",
    "# object attr:\n",
    "# [rigid]\n",
    "args.attr_dim = 1\n",
    "\n",
    "# relation attr:\n",
    "# [none]\n",
    "args.relation_dim = 2\n",
    "\n",
    "args.time_step = 600\n",
    "args.time_step_clip = 100\n",
    "args.n_instance = 1\n",
    "args.n_stages = 1\n",
    "\n",
    "args.neighbor_radius = 0.08\n",
    "\n",
    "phases_dict[\"instance_idx\"] = [0, 31]\n",
    "phases_dict[\"root_num\"] = [[]]\n",
    "phases_dict[\"instance\"] = ['solid']\n",
    "phases_dict[\"material\"] = ['solid']\n",
    "\n",
    "data_names = ['positions', 'velocities','hair_idx']\n",
    "verbose = 0\n",
    "phase = 'train'\n",
    "data_dir = os.path.join(args.dataf, phase)\n",
    "stat_path = os.path.join(args.dataf, 'stat.h5')\n",
    "n_rollout = 10\n",
    "\n",
    "\n",
    "\n",
    "info = {\n",
    "    'env': args.env,\n",
    "    'root_num': phases_dict['root_num'],\n",
    "    'thread_idx': 0,\n",
    "    'data_dir': data_dir,\n",
    "    'data_names': data_names,\n",
    "    'n_rollout': n_rollout // args.num_workers,\n",
    "    'n_instance': args.n_instance,\n",
    "    'time_step': args.time_step,\n",
    "    'time_step_clip': args.time_step_clip,\n",
    "    'dt': args.dt,\n",
    "    'shape_state_dim': args.shape_state_dim}\n",
    "\n",
    "info['env_idx'] = 11\n",
    "\n",
    "args.outf = 'dump_SingleHair/' + args.outf\n",
    "\n",
    "args.outf = args.outf + '_' + args.env\n",
    "args.dataf = 'data/' + args.dataf + '_' + args.env\n",
    "print (args.dataf)\n",
    "os.system('mkdir -p ' + args.outf)\n",
    "os.system('mkdir -p ' + args.dataf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {phase: PhysicsFleXDataset(\n",
    "    args, phase, phases_dict, verbose=False) for phase in ['train', 'valid']}\n",
    "datasets['train'].load_data(args.env)\n",
    "datasets['valid'].load_data(args.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {x: torch.utils.data.DataLoader(\n",
    "    datasets[x], batch_size=args.batch_size,\n",
    "    shuffle=True if x == 'train' else False,\n",
    "    num_workers=args.num_workers,\n",
    "    collate_fn=collate_fn)\n",
    "    for x in ['train', 'valid']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IntNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 15918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IntNet(\n",
       "  (object_function): ParticleEncoder(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=22, out_features=100, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=100, out_features=3, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (relation_function): RelationEncoder(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=100, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=100, out_features=15, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_gpu = True\n",
    "args.lr = 0.001\n",
    "model = IntNet(args, datasets['train'].stat, phases_dict, residual=True, use_gpu=use_gpu)\n",
    "print(\"Number of parameters: %d\" % count_parameters(model))\n",
    "# criterion\n",
    "criterionMSE = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, betas=(args.beta1, 0.999))\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=3, verbose=True)\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    criterionMSE = criterionMSE.cuda()\n",
    "\n",
    "st_epoch = args.resume_epoch if args.resume_epoch > 0 else 0\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "model.train(phase=='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-step prediction\n",
    "\n",
    "phase = 'train'\n",
    "instance_idx = [0, 31]\n",
    "args.n_epoch = 500\n",
    "psteps = 0\n",
    "\n",
    "for epoch in range(args.n_epoch):\n",
    "    model.train(phase=='train')\n",
    "\n",
    "    losses = 0.\n",
    "    for i, data in enumerate(dataloaders[phase]):\n",
    "#         print ('i:',i)\n",
    "\n",
    "        attr, state, rels, n_particles, n_shapes, label = data\n",
    "        Ra, node_r_idx, node_s_idx, pstep = rels[3], rels[4], rels[5], rels[6]\n",
    "\n",
    "        Rr, Rs = [], []\n",
    "        Values = []\n",
    "        for j in range(len(rels[0])):\n",
    "            Rr_idx, Rs_idx, values = rels[0][j], rels[1][j], rels[2][j]\n",
    "            V = torch.ones(values.shape)\n",
    "            Values.append(values)\n",
    "            Rr.append(torch.sparse.FloatTensor(\n",
    "                Rr_idx, V, torch.Size([node_r_idx[j].shape[0], Ra[j].size(0)])))\n",
    "            Rs.append(torch.sparse.FloatTensor(\n",
    "                Rs_idx, V, torch.Size([node_s_idx[j].shape[0], Ra[j].size(0)])))\n",
    "\n",
    "        data = [attr, state, Rr, Rs, Ra,Values, label]\n",
    "        \n",
    "\n",
    "            # st_time = time.time()\n",
    "        with torch.set_grad_enabled(phase=='train'):\n",
    "            if use_gpu:\n",
    "                instance_idx = torch.Tensor(instance_idx)#.cuda()\n",
    "                for d in range(len(data)):\n",
    "                    if type(data[d]) == list:\n",
    "                        for t in range(len(data[d])):\n",
    "                            data[d][t] = Variable(data[d][t].cuda())\n",
    "                    else:\n",
    "                        data[d] = Variable(data[d].cuda())\n",
    "            else:\n",
    "                for d in range(len(data)):\n",
    "                    if type(data[d]) == list:\n",
    "                        for t in range(len(data[d])):\n",
    "                            data[d][t] = Variable(data[d][t])\n",
    "                    else:\n",
    "                        data[d] = Variable(data[d])\n",
    "\n",
    "            attr, state, Rr, Rs, Ra, Values, label = data\n",
    "            \n",
    "            pstep = 3\n",
    "\n",
    "            predicted = model(\n",
    "                attr, state, Rr, Rs, Ra,Values, n_particles,\n",
    "                node_r_idx, node_s_idx, pstep,\n",
    "                instance_idx, phases_dict, 0)\n",
    "            # print('Time forward', time.time() - st_time)\n",
    "\n",
    "       #     print(predicted.shape)\n",
    "       #     print(label.shape)\n",
    "\n",
    "        loss = criterionMSE(predicted, label)\n",
    "        losses += np.sqrt(loss.item())\n",
    "\n",
    "        if phase == 'train':\n",
    "            if i % 5 == 0:\n",
    "                # update parameters every args.forward_times\n",
    "          #      print ('update!')\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "          #      print ('done')\n",
    "\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            n_relations = 0\n",
    "            for j in range(len(Ra)):\n",
    "                n_relations += Ra[j].size(0)\n",
    "            print('%s [%d/%d][%d/%d] n_relations: %d, Loss: %.6f, Agg: %.6f' %\n",
    "                  (phase, epoch, args.n_epoch, i, len(dataloaders[phase]),\n",
    "                   n_relations, np.sqrt(loss.item()), losses / (i + 1)))\n",
    "\n",
    "      #  if phase == 'train' and i > 0 and i % args.ckp_per_iter == 0:\n",
    "    if epoch % 10 == 0 :\n",
    "        torch.save(model.state_dict(), '%s/IntNet_epoch_%d.pth' % (args.outf, epoch))\n",
    "\n",
    "    losses /= len(dataloaders[phase])\n",
    "    print('%s [%d/%d] Loss: %.4f' %\n",
    "          (phase, epoch, args.n_epoch, losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes from 12 to 13 positional arguments but 14 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-36d2fe088df4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mValues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_particles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0mnode_r_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_s_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                     instance_idx, phases_dict, 0)\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mvels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdenormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/onepear/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes from 12 to 13 positional arguments but 14 were given"
     ]
    }
   ],
   "source": [
    "# Multi-step Prediction\n",
    "\n",
    "phase = 'train'\n",
    "instance_idx = [0, 31]\n",
    "args.n_epoch = 500\n",
    "psteps = 0\n",
    "data_names = ['positions', 'velocities','hair_idx']\n",
    "\n",
    "for epoch in range(args.n_epoch):\n",
    "    model.train(phase=='train')\n",
    "    losses = 0.\n",
    "    for i in range(len(dataloaders[phase])):\n",
    "        idx_rollout = i // (args.time_step - 1)\n",
    "        idx_timestep = i % (args.time_step - 1)\n",
    "        if args.time_step - idx_timestep <= 3:\n",
    "            continue\n",
    "        data_path = os.path.join(args.dataf,'train', str(idx_rollout), str(idx_timestep) + '.h5')\n",
    "        data_nxt1_path = os.path.join(args.dataf,'train', str(idx_rollout), str(idx_timestep + 1) + '.h5')\n",
    "        data_nxt2_path = os.path.join(args.dataf,'train', str(idx_rollout), str(idx_timestep + 2) + '.h5')\n",
    "        data_nxt3_path = os.path.join(args.dataf,'train', str(idx_rollout), str(idx_timestep + 3) + '.h5')\n",
    "        Data = load_data(data_names, data_path)\n",
    "        data_nxt1 = normalize(load_data(data_names, data_nxt1_path), datasets['train'].stat)\n",
    "        data_nxt2 = normalize(load_data(data_names, data_nxt2_path), datasets['train'].stat)\n",
    "        data_nxt3 = normalize(load_data(data_names, data_nxt3_path), datasets['train'].stat)\n",
    "        label = torch.FloatTensor([data_nxt1[1][:n_particles],data_nxt2[1][:n_particles],data_nxt3[1][:n_particles]])\n",
    "        \n",
    "        p_pred = np.zeros((3, n_particles, args.position_dim))\n",
    "        label_pred = np.zeros((3, n_particles, args.position_dim))\n",
    "        for step in range(3):\n",
    "            p_pred[step] = Data[0][:n_particles]\n",
    "            attr, state, rels, n_particles, n_shapes = prepare_input(Data, datasets['train'].stat, datasets['train'].args, datasets['train'].phases_dict, 0)\n",
    "            Ra, node_r_idx, node_s_idx, pstep = rels[3], rels[4], rels[5], rels[6]\n",
    "        \n",
    "            Rr, Rs = [], []\n",
    "            Values = []\n",
    "            for j in range(len(rels[0])):\n",
    "                Rr_idx, Rs_idx, values = rels[0][j], rels[1][j], rels[2][j]\n",
    "                V = torch.ones(values.shape)\n",
    "                Values.append(values)\n",
    "                Rr.append(torch.sparse.FloatTensor(\n",
    "                    Rr_idx, V, torch.Size([node_r_idx[j].shape[0], Ra[j].size(0)])))\n",
    "                Rs.append(torch.sparse.FloatTensor(\n",
    "                    Rs_idx, V, torch.Size([node_s_idx[j].shape[0], Ra[j].size(0)])))\n",
    "\n",
    "            data = [attr, state, Rr, Rs, Ra,Values, label]\n",
    "\n",
    "\n",
    "                # st_time = time.time()\n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "                if use_gpu:\n",
    "                    instance_idx = torch.Tensor(instance_idx)#.cuda()\n",
    "                    for d in range(len(data)):\n",
    "                        if type(data[d]) == list:\n",
    "                            for t in range(len(data[d])):\n",
    "                                data[d][t] = Variable(data[d][t].cuda())\n",
    "                        else:\n",
    "                            data[d] = Variable(data[d].cuda())\n",
    "                else:\n",
    "                    for d in range(len(data)):\n",
    "                        if type(data[d]) == list:\n",
    "                            for t in range(len(data[d])):\n",
    "                                data[d][t] = Variable(data[d][t])\n",
    "                        else:\n",
    "                            data[d] = Variable(data[d])\n",
    "\n",
    "                attr, state, Rr, Rs, Ra, Values, label = data\n",
    "\n",
    "                pstep = 3\n",
    "\n",
    "                predicted = model(\n",
    "                    attr, state, Rr, Rs, Ra,Values, n_particles,\n",
    "                    node_r_idx, node_s_idx, pstep,\n",
    "                    instance_idx, phases_dict, 0)\n",
    "                \n",
    "            vels = denormalize([predicted.data.cpu().numpy()], [datasets['train'].stat[1]])[0]\n",
    "            Data[0][:n_particles] += vels * args.dt\n",
    "            Data[1][:n_particles] = vels\n",
    "            label_pred[step] = vels\n",
    "          #  vels = torch.Tensor(vels).cuda()\n",
    "            loss = criterionMSE(predicted, label[step])\n",
    "            losses += np.sqrt(loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        '''    \n",
    "        label_pred = torch.Tensor(label_pred).cuda()\n",
    "        loss = criterionMSE(label_pred, label)\n",
    "        losses += np.sqrt(loss.item())\n",
    "    #    print (label_pred.shape)\n",
    "        if phase == 'train':\n",
    "            if i % 5 == 0:\n",
    "                # update parameters every args.forward_times\n",
    "          #      print ('update!')\n",
    "                print (loss)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "          #      print ('done')\n",
    "        '''\n",
    "\n",
    "\n",
    "        if i % 5000 == 0:\n",
    "            n_relations = 0\n",
    "            for j in range(len(Ra)):\n",
    "                n_relations += Ra[j].size(0)\n",
    "            print('%s [%d/%d][%d/%d] n_relations: %d, Loss: %.6f, Agg: %.6f' %\n",
    "                  (phase, epoch, args.n_epoch, i, len(dataloaders[phase]),\n",
    "                   n_relations, np.sqrt(loss.item()), losses / (i + 1)/3))\n",
    "\n",
    "      #  if phase == 'train' and i > 0 and i % args.ckp_per_iter == 0:\n",
    "    if epoch % 10 == 0 :\n",
    "        torch.save(model.state_dict(), '%s/IntNet_epoch_%d.pth' % (args.outf, epoch))\n",
    "\n",
    "    losses /= len(dataloaders[phase])\n",
    "    print('%s [%d/%d] Loss: %.4f' %\n",
    "          (phase, epoch, args.n_epoch, losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DPI-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 222353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DPINet2(\n",
       "  (particle_encoder): ParticleEncoder(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=7, out_features=100, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=100, out_features=150, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (relation_encoder): RelationEncoder(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=150, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=150, out_features=150, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=150, out_features=150, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (relation_propagator): Propagator(\n",
       "    (linear): Linear(in_features=450, out_features=150, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (particle_propagator): Propagator(\n",
       "    (linear): Linear(in_features=300, out_features=150, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (particle_predictor): ParticlePredictor(\n",
       "    (linear_0): Linear(in_features=150, out_features=150, bias=True)\n",
       "    (linear_1): Linear(in_features=150, out_features=150, bias=True)\n",
       "    (linear_2): Linear(in_features=150, out_features=3, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_gpu = True\n",
    "args.lr = 0.001\n",
    "model = DPINet2(args, datasets['train'].stat, phases_dict, residual=True, use_gpu=use_gpu)\n",
    "print(\"Number of parameters: %d\" % count_parameters(model))\n",
    "# criterion\n",
    "criterionMSE = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, betas=(args.beta1, 0.999))\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=3, verbose=True)\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    criterionMSE = criterionMSE.cuda()\n",
    "\n",
    "st_epoch = args.resume_epoch if args.resume_epoch > 0 else 0\n",
    "best_valid_loss = np.inf\n",
    "model_file = os.path.join(args.outf, 'DPINet3_epoch_%d.pth' % (150))\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "\n",
    "model.train(phase=='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single-Step Prediction\n",
    "phase = 'train'\n",
    "instance_idx = [0, 31]\n",
    "args.n_epoch = 500\n",
    "psteps = 3\n",
    "\n",
    "for epoch in range(args.n_epoch):\n",
    "    model.train(phase=='train')\n",
    "\n",
    "    losses = 0.\n",
    "    for i, data in enumerate(dataloaders[phase]):\n",
    "#         print ('i:',i)\n",
    "\n",
    "        attr, state, rels, n_particles, n_shapes, label = data\n",
    "        Ra, node_r_idx, node_s_idx, pstep = rels[3], rels[4], rels[5], rels[6]\n",
    "        Rr, Rs = [], []\n",
    "        Values = []\n",
    "\n",
    "        for j in range(len(rels[0])):\n",
    "            Rr_idx, Rs_idx, values = rels[0][j], rels[1][j], rels[2][j]\n",
    "            V = torch.ones(values.shape)\n",
    "            Values.append(values)\n",
    "            Rr.append(torch.sparse.FloatTensor(\n",
    "                Rr_idx, V, torch.Size([node_r_idx[j].shape[0], Ra[j].size(0)])))\n",
    "            Rs.append(torch.sparse.FloatTensor(\n",
    "                Rs_idx, V, torch.Size([node_s_idx[j].shape[0], Ra[j].size(0)])))\n",
    "        data = [attr, state, Rr, Rs, Ra, Values, label]\n",
    "\n",
    "\n",
    "            # st_time = time.time()\n",
    "        with torch.set_grad_enabled(phase=='train'):\n",
    "            if use_gpu:\n",
    "                for d in range(len(data)):\n",
    "                    if type(data[d]) == list:\n",
    "                        for t in range(len(data[d])):\n",
    "                            data[d][t] = Variable(data[d][t].cuda())\n",
    "                    else:\n",
    "                        data[d] = Variable(data[d].cuda())\n",
    "            else:\n",
    "                for d in range(len(data)):\n",
    "                    if type(data[d]) == list:\n",
    "                        for t in range(len(data[d])):\n",
    "                            data[d][t] = Variable(data[d][t])\n",
    "                    else:\n",
    "                        data[d] = Variable(data[d])\n",
    "\n",
    "            attr, state, Rr, Rs, Ra, Values, label = data\n",
    "            \n",
    "            pstep = 3\n",
    "\n",
    "            predicted = model(\n",
    "                attr, state, Rr, Rs, Ra, Values, n_particles,\n",
    "                node_r_idx, node_s_idx, pstep,\n",
    "                instance_idx, phases_dict, 0)\n",
    "            # print('Time forward', time.time() - st_time)\n",
    "\n",
    "       #     print(predicted.shape)\n",
    "       #     print(label.shape)\n",
    "\n",
    "        loss = criterionMSE(predicted, label)\n",
    "        losses += np.sqrt(loss.item())\n",
    "\n",
    "        if phase == 'train':\n",
    "            if i % 5 == 0:\n",
    "                # update parameters every args.forward_times\n",
    "          #      print ('update!')\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "          #      print ('done')\n",
    "\n",
    "\n",
    "        if i % 5000 == 0:\n",
    "            n_relations = 0\n",
    "            for j in range(len(Ra)):\n",
    "                n_relations += Ra[j].size(0)\n",
    "            print('%s [%d/%d][%d/%d] n_relations: %d, Loss: %.6f, Agg: %.6f' %\n",
    "                  (phase, epoch, args.n_epoch, i, len(dataloaders[phase]),\n",
    "                   n_relations, np.sqrt(loss.item()), losses / (i + 1)))\n",
    "\n",
    "      #  if phase == 'train' and i > 0 and i % args.ckp_per_iter == 0:\n",
    "    if epoch % 10 == 0 :\n",
    "        torch.save(model.state_dict(), '%s/DPINet2_epoch_%d.pth' % (args.outf, epoch))\n",
    "\n",
    "    losses /= len(dataloaders[phase])\n",
    "    print('%s [%d/%d] Loss: %.4f' %\n",
    "          (phase, epoch, args.n_epoch, losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [150/500][0/26955] n_relations: 60, Loss: 0.166653, Agg: 0.424504\n",
      "train [150/500][5000/26955] n_relations: 62, Loss: 0.130836, Agg: 0.233931\n",
      "train [150/500][10000/26955] n_relations: 60, Loss: 0.160931, Agg: 0.237458\n",
      "train [150/500][15000/26955] n_relations: 60, Loss: 0.958392, Agg: 0.235404\n",
      "train [150/500][20000/26955] n_relations: 62, Loss: 0.213339, Agg: 0.235109\n",
      "train [150/500][25000/26955] n_relations: 60, Loss: 0.238285, Agg: 0.242729\n",
      "train [150/500] Loss: 0.2491\n",
      "train [151/500][0/26955] n_relations: 60, Loss: 0.473313, Agg: 0.704887\n",
      "train [151/500][5000/26955] n_relations: 62, Loss: 0.248180, Agg: 0.298452\n",
      "train [151/500][10000/26955] n_relations: 60, Loss: 0.327631, Agg: 0.295735\n",
      "train [151/500][15000/26955] n_relations: 60, Loss: 1.133573, Agg: 0.287698\n",
      "train [151/500][20000/26955] n_relations: 62, Loss: 0.214131, Agg: 0.279999\n",
      "train [151/500][25000/26955] n_relations: 60, Loss: 0.216772, Agg: 0.271021\n",
      "train [151/500] Loss: 0.2693\n",
      "train [152/500][0/26955] n_relations: 60, Loss: 0.274459, Agg: 0.516839\n",
      "train [152/500][5000/26955] n_relations: 62, Loss: 0.123123, Agg: 0.245772\n",
      "train [152/500][10000/26955] n_relations: 60, Loss: 0.159123, Agg: 0.237957\n",
      "train [152/500][15000/26955] n_relations: 60, Loss: 0.901680, Agg: 0.239029\n",
      "train [152/500][20000/26955] n_relations: 62, Loss: 0.203163, Agg: 0.237702\n",
      "train [152/500][25000/26955] n_relations: 60, Loss: 0.187567, Agg: 0.234703\n",
      "train [152/500] Loss: 0.2345\n",
      "train [153/500][0/26955] n_relations: 60, Loss: 0.408600, Agg: 0.682437\n",
      "train [153/500][5000/26955] n_relations: 62, Loss: 0.171054, Agg: 0.260746\n",
      "train [153/500][10000/26955] n_relations: 60, Loss: 0.200712, Agg: 0.251888\n",
      "train [153/500][15000/26955] n_relations: 60, Loss: 1.003190, Agg: 0.245346\n",
      "train [153/500][20000/26955] n_relations: 62, Loss: 0.168636, Agg: 0.242414\n",
      "train [153/500][25000/26955] n_relations: 60, Loss: 0.211402, Agg: 0.239071\n",
      "train [153/500] Loss: 0.2392\n",
      "train [154/500][0/26955] n_relations: 60, Loss: 0.245602, Agg: 0.498073\n",
      "train [154/500][5000/26955] n_relations: 62, Loss: 0.166509, Agg: 0.243567\n",
      "train [154/500][10000/26955] n_relations: 60, Loss: 0.345557, Agg: 0.247372\n",
      "train [154/500][15000/26955] n_relations: 60, Loss: 0.944243, Agg: 0.253398\n",
      "train [154/500][20000/26955] n_relations: 62, Loss: 0.293321, Agg: 0.251446\n",
      "train [154/500][25000/26955] n_relations: 60, Loss: 0.194657, Agg: 0.246157\n",
      "train [154/500] Loss: 0.2523\n",
      "train [155/500][0/26955] n_relations: 60, Loss: 0.391084, Agg: 0.588949\n",
      "train [155/500][5000/26955] n_relations: 62, Loss: 0.159144, Agg: 0.286183\n",
      "train [155/500][10000/26955] n_relations: 60, Loss: 0.330497, Agg: 0.282024\n",
      "train [155/500][15000/26955] n_relations: 60, Loss: 0.928512, Agg: 0.297143\n",
      "train [155/500][20000/26955] n_relations: 62, Loss: 0.298141, Agg: 0.290240\n",
      "train [155/500][25000/26955] n_relations: 60, Loss: 0.208481, Agg: 0.282691\n",
      "train [155/500] Loss: 0.2801\n",
      "train [156/500][0/26955] n_relations: 60, Loss: 0.215195, Agg: 0.486637\n",
      "train [156/500][5000/26955] n_relations: 62, Loss: 0.162870, Agg: 0.254184\n",
      "train [156/500][10000/26955] n_relations: 60, Loss: 0.156860, Agg: 0.249049\n",
      "train [156/500][15000/26955] n_relations: 60, Loss: 0.931245, Agg: 0.245402\n",
      "train [156/500][20000/26955] n_relations: 62, Loss: 0.191605, Agg: 0.248068\n",
      "train [156/500][25000/26955] n_relations: 60, Loss: 0.161704, Agg: 0.248758\n",
      "train [156/500] Loss: 0.2481\n",
      "train [157/500][0/26955] n_relations: 60, Loss: 0.241076, Agg: 0.500502\n",
      "train [157/500][5000/26955] n_relations: 62, Loss: 0.168917, Agg: 0.251445\n",
      "train [157/500][10000/26955] n_relations: 60, Loss: 0.207323, Agg: 0.251689\n",
      "train [157/500][15000/26955] n_relations: 60, Loss: 0.913468, Agg: 0.246347\n",
      "train [157/500][20000/26955] n_relations: 62, Loss: 0.320577, Agg: 0.253821\n",
      "train [157/500][25000/26955] n_relations: 60, Loss: 0.191177, Agg: 0.259753\n",
      "train [157/500] Loss: 0.2585\n",
      "train [158/500][0/26955] n_relations: 60, Loss: 0.231616, Agg: 0.486708\n",
      "train [158/500][5000/26955] n_relations: 62, Loss: 0.167703, Agg: 0.248767\n",
      "train [158/500][10000/26955] n_relations: 60, Loss: 0.206103, Agg: 0.249005\n",
      "train [158/500][15000/26955] n_relations: 60, Loss: 0.976381, Agg: 0.251383\n",
      "train [158/500][20000/26955] n_relations: 62, Loss: 0.205815, Agg: 0.255782\n",
      "train [158/500][25000/26955] n_relations: 60, Loss: 0.164337, Agg: 0.254004\n",
      "train [158/500] Loss: 0.2535\n",
      "train [159/500][0/26955] n_relations: 60, Loss: 0.443670, Agg: 0.608599\n",
      "train [159/500][5000/26955] n_relations: 62, Loss: 0.229681, Agg: 0.255198\n",
      "train [159/500][10000/26955] n_relations: 60, Loss: 0.296002, Agg: 0.247146\n",
      "train [159/500][15000/26955] n_relations: 60, Loss: 0.913562, Agg: 0.241572\n",
      "train [159/500][20000/26955] n_relations: 62, Loss: 0.213151, Agg: 0.242369\n",
      "train [159/500][25000/26955] n_relations: 60, Loss: 0.162197, Agg: 0.241007\n",
      "train [159/500] Loss: 0.2404\n",
      "train [160/500][0/26955] n_relations: 60, Loss: 0.243618, Agg: 0.528236\n",
      "train [160/500][5000/26955] n_relations: 62, Loss: 0.122126, Agg: 0.240208\n",
      "train [160/500][10000/26955] n_relations: 60, Loss: 0.223136, Agg: 0.264412\n",
      "train [160/500][15000/26955] n_relations: 60, Loss: 1.057613, Agg: 0.265680\n",
      "train [160/500][20000/26955] n_relations: 62, Loss: 0.227592, Agg: 0.269173\n",
      "train [160/500][25000/26955] n_relations: 60, Loss: 0.246340, Agg: 0.271558\n",
      "train [160/500] Loss: 0.2714\n",
      "train [161/500][0/26955] n_relations: 60, Loss: 0.349403, Agg: 0.563177\n",
      "train [161/500][5000/26955] n_relations: 62, Loss: 0.161036, Agg: 0.269850\n",
      "train [161/500][10000/26955] n_relations: 60, Loss: 0.209036, Agg: 0.259206\n",
      "train [161/500][15000/26955] n_relations: 60, Loss: 0.913263, Agg: 0.251915\n",
      "train [161/500][20000/26955] n_relations: 62, Loss: 0.254494, Agg: 0.253405\n",
      "train [161/500][25000/26955] n_relations: 60, Loss: 0.186177, Agg: 0.250064\n",
      "train [161/500] Loss: 0.2482\n",
      "train [162/500][0/26955] n_relations: 60, Loss: 0.262834, Agg: 0.537861\n",
      "train [162/500][5000/26955] n_relations: 62, Loss: 0.157236, Agg: 0.237350\n",
      "train [162/500][10000/26955] n_relations: 60, Loss: 0.201276, Agg: 0.256508\n",
      "train [162/500][15000/26955] n_relations: 60, Loss: 1.105261, Agg: 0.261552\n",
      "train [162/500][20000/26955] n_relations: 62, Loss: 0.167183, Agg: 0.263190\n",
      "train [162/500][25000/26955] n_relations: 60, Loss: 0.204743, Agg: 0.262817\n",
      "train [162/500] Loss: 0.2628\n",
      "train [163/500][0/26955] n_relations: 60, Loss: 0.702256, Agg: 0.974187\n",
      "train [163/500][5000/26955] n_relations: 62, Loss: 0.232892, Agg: 0.271527\n",
      "train [163/500][10000/26955] n_relations: 60, Loss: 0.184443, Agg: 0.263977\n",
      "train [163/500][15000/26955] n_relations: 60, Loss: 0.882725, Agg: 0.258706\n",
      "train [163/500][20000/26955] n_relations: 62, Loss: 0.190678, Agg: 0.271008\n",
      "train [163/500][25000/26955] n_relations: 60, Loss: 0.434631, Agg: 0.289584\n",
      "train [163/500] Loss: 0.2910\n",
      "train [164/500][0/26955] n_relations: 60, Loss: 0.595256, Agg: 0.907891\n",
      "train [164/500][5000/26955] n_relations: 62, Loss: 0.264526, Agg: 0.311115\n",
      "train [164/500][10000/26955] n_relations: 60, Loss: 0.307495, Agg: 0.309047\n",
      "train [164/500][15000/26955] n_relations: 60, Loss: 1.006559, Agg: 0.302961\n",
      "train [164/500][20000/26955] n_relations: 62, Loss: 0.233377, Agg: 0.300421\n",
      "train [164/500][25000/26955] n_relations: 60, Loss: 0.250038, Agg: 0.295625\n",
      "train [164/500] Loss: 0.2934\n",
      "train [165/500][0/26955] n_relations: 60, Loss: 0.541368, Agg: 0.823290\n",
      "train [165/500][5000/26955] n_relations: 62, Loss: 0.224884, Agg: 0.278695\n",
      "train [165/500][10000/26955] n_relations: 60, Loss: 0.263355, Agg: 0.270738\n",
      "train [165/500][15000/26955] n_relations: 60, Loss: 1.038312, Agg: 0.263757\n",
      "train [165/500][20000/26955] n_relations: 62, Loss: 0.189800, Agg: 0.260570\n",
      "train [165/500][25000/26955] n_relations: 60, Loss: 0.215796, Agg: 0.257926\n",
      "train [165/500] Loss: 0.2560\n",
      "train [166/500][0/26955] n_relations: 60, Loss: 0.445319, Agg: 0.720755\n",
      "train [166/500][5000/26955] n_relations: 62, Loss: 0.259490, Agg: 0.253038\n",
      "train [166/500][10000/26955] n_relations: 60, Loss: 0.174882, Agg: 0.242044\n",
      "train [166/500][15000/26955] n_relations: 60, Loss: 1.179510, Agg: 0.239208\n",
      "train [166/500][20000/26955] n_relations: 62, Loss: 0.184071, Agg: 0.237855\n",
      "train [166/500][25000/26955] n_relations: 60, Loss: 0.213951, Agg: 0.237844\n",
      "train [166/500] Loss: 0.2374\n",
      "train [167/500][0/26955] n_relations: 60, Loss: 0.389451, Agg: 0.653698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [167/500][5000/26955] n_relations: 62, Loss: 0.227745, Agg: 0.238269\n",
      "train [167/500][10000/26955] n_relations: 60, Loss: 0.177116, Agg: 0.234639\n",
      "train [167/500][15000/26955] n_relations: 60, Loss: 0.980100, Agg: 0.235448\n",
      "train [167/500][20000/26955] n_relations: 62, Loss: 0.205412, Agg: 0.237142\n",
      "train [167/500][25000/26955] n_relations: 60, Loss: 0.186241, Agg: 0.238524\n",
      "train [167/500] Loss: 0.2377\n",
      "train [168/500][0/26955] n_relations: 60, Loss: 0.377032, Agg: 0.629209\n",
      "train [168/500][5000/26955] n_relations: 62, Loss: 0.194134, Agg: 0.244581\n",
      "train [168/500][10000/26955] n_relations: 60, Loss: 0.145722, Agg: 0.234852\n",
      "train [168/500][15000/26955] n_relations: 60, Loss: 0.574589, Agg: 0.233844\n",
      "train [168/500][20000/26955] n_relations: 62, Loss: 0.144501, Agg: 0.234221\n",
      "train [168/500][25000/26955] n_relations: 60, Loss: 0.155560, Agg: 0.233150\n",
      "train [168/500] Loss: 0.2323\n",
      "train [169/500][0/26955] n_relations: 60, Loss: 0.381401, Agg: 0.673224\n",
      "train [169/500][5000/26955] n_relations: 62, Loss: 0.163790, Agg: 0.235186\n",
      "train [169/500][10000/26955] n_relations: 60, Loss: 0.156729, Agg: 0.226679\n",
      "train [169/500][15000/26955] n_relations: 60, Loss: 0.600712, Agg: 0.224204\n",
      "train [169/500][20000/26955] n_relations: 62, Loss: 0.209273, Agg: 0.225242\n",
      "train [169/500][25000/26955] n_relations: 60, Loss: 0.165261, Agg: 0.223464\n",
      "train [169/500] Loss: 0.2225\n",
      "train [170/500][0/26955] n_relations: 60, Loss: 0.306945, Agg: 0.623373\n",
      "train [170/500][5000/26955] n_relations: 62, Loss: 0.197164, Agg: 0.225871\n",
      "train [170/500][10000/26955] n_relations: 60, Loss: 0.179679, Agg: 0.220478\n",
      "train [170/500][15000/26955] n_relations: 60, Loss: 0.869571, Agg: 0.218411\n",
      "train [170/500][20000/26955] n_relations: 62, Loss: 0.165810, Agg: 0.219021\n",
      "train [170/500][25000/26955] n_relations: 60, Loss: 0.125610, Agg: 0.217231\n",
      "train [170/500] Loss: 0.2167\n",
      "train [171/500][0/26955] n_relations: 60, Loss: 0.443792, Agg: 0.712925\n",
      "train [171/500][5000/26955] n_relations: 62, Loss: 0.180770, Agg: 0.218179\n",
      "train [171/500][10000/26955] n_relations: 60, Loss: 0.154499, Agg: 0.214779\n",
      "train [171/500][15000/26955] n_relations: 60, Loss: 1.196199, Agg: 0.214376\n",
      "train [171/500][20000/26955] n_relations: 62, Loss: 0.207958, Agg: 0.215211\n",
      "train [171/500][25000/26955] n_relations: 60, Loss: 0.157447, Agg: 0.214214\n",
      "train [171/500] Loss: 0.2133\n",
      "train [172/500][0/26955] n_relations: 60, Loss: 0.381997, Agg: 0.619356\n",
      "train [172/500][5000/26955] n_relations: 62, Loss: 0.178870, Agg: 0.215743\n",
      "train [172/500][10000/26955] n_relations: 60, Loss: 0.199189, Agg: 0.210344\n",
      "train [172/500][15000/26955] n_relations: 60, Loss: 0.445627, Agg: 0.208934\n",
      "train [172/500][20000/26955] n_relations: 62, Loss: 0.137484, Agg: 0.211545\n",
      "train [172/500][25000/26955] n_relations: 60, Loss: 0.142681, Agg: 0.209968\n",
      "train [172/500] Loss: 0.2092\n",
      "train [173/500][0/26955] n_relations: 60, Loss: 0.373655, Agg: 0.615150\n",
      "train [173/500][5000/26955] n_relations: 62, Loss: 0.148548, Agg: 0.210613\n",
      "train [173/500][10000/26955] n_relations: 60, Loss: 0.307042, Agg: 0.213879\n",
      "train [173/500][15000/26955] n_relations: 60, Loss: 0.808203, Agg: 0.230734\n",
      "train [173/500][20000/26955] n_relations: 62, Loss: 0.261827, Agg: 0.232428\n",
      "train [173/500][25000/26955] n_relations: 60, Loss: 0.152800, Agg: 0.228266\n",
      "train [173/500] Loss: 0.2264\n",
      "train [174/500][0/26955] n_relations: 60, Loss: 0.517132, Agg: 0.752127\n",
      "train [174/500][5000/26955] n_relations: 62, Loss: 0.162060, Agg: 0.238507\n",
      "train [174/500][10000/26955] n_relations: 60, Loss: 0.153732, Agg: 0.233830\n",
      "train [174/500][15000/26955] n_relations: 60, Loss: 0.692571, Agg: 0.233325\n",
      "train [174/500][20000/26955] n_relations: 62, Loss: 0.144213, Agg: 0.230403\n",
      "train [174/500][25000/26955] n_relations: 60, Loss: 0.151265, Agg: 0.226264\n",
      "train [174/500] Loss: 0.2249\n",
      "train [175/500][0/26955] n_relations: 60, Loss: 0.499339, Agg: 0.653803\n",
      "train [175/500][5000/26955] n_relations: 62, Loss: 0.206196, Agg: 0.220025\n",
      "train [175/500][10000/26955] n_relations: 60, Loss: 0.220862, Agg: 0.228262\n",
      "train [175/500][15000/26955] n_relations: 60, Loss: 1.355599, Agg: 0.235858\n",
      "train [175/500][20000/26955] n_relations: 62, Loss: 0.167171, Agg: 0.245275\n",
      "train [175/500][25000/26955] n_relations: 60, Loss: 0.308718, Agg: 0.255951\n",
      "train [175/500] Loss: 0.2567\n",
      "train [176/500][0/26955] n_relations: 60, Loss: 0.498789, Agg: 0.724048\n",
      "train [176/500][5000/26955] n_relations: 62, Loss: 0.200171, Agg: 0.248497\n",
      "train [176/500][10000/26955] n_relations: 60, Loss: 0.199331, Agg: 0.240972\n",
      "train [176/500][15000/26955] n_relations: 60, Loss: 0.614969, Agg: 0.240846\n",
      "train [176/500][20000/26955] n_relations: 62, Loss: 0.216813, Agg: 0.249397\n",
      "train [176/500][25000/26955] n_relations: 60, Loss: 0.247006, Agg: 0.248446\n",
      "train [176/500] Loss: 0.2475\n",
      "train [177/500][0/26955] n_relations: 60, Loss: 0.863544, Agg: 0.858370\n",
      "train [177/500][5000/26955] n_relations: 62, Loss: 0.159076, Agg: 0.240823\n",
      "train [177/500][10000/26955] n_relations: 60, Loss: 0.186754, Agg: 0.249060\n",
      "train [177/500][15000/26955] n_relations: 60, Loss: 1.245204, Agg: 0.260509\n",
      "train [177/500][20000/26955] n_relations: 62, Loss: 0.255853, Agg: 0.272011\n",
      "train [177/500][25000/26955] n_relations: 60, Loss: 0.199369, Agg: 0.272631\n",
      "train [177/500] Loss: 0.2729\n",
      "train [178/500][0/26955] n_relations: 60, Loss: 3.991805, Agg: 2.423701\n",
      "train [178/500][5000/26955] n_relations: 62, Loss: 0.217521, Agg: 0.287360\n",
      "train [178/500][10000/26955] n_relations: 60, Loss: 0.152375, Agg: 0.284270\n",
      "train [178/500][15000/26955] n_relations: 60, Loss: 0.951985, Agg: 0.277014\n",
      "train [178/500][20000/26955] n_relations: 62, Loss: 0.215873, Agg: 0.273371\n",
      "train [178/500][25000/26955] n_relations: 60, Loss: 0.205461, Agg: 0.268750\n",
      "train [178/500] Loss: 0.2671\n",
      "train [179/500][0/26955] n_relations: 60, Loss: 0.777872, Agg: 0.873132\n",
      "train [179/500][5000/26955] n_relations: 62, Loss: 0.210340, Agg: 0.267246\n",
      "train [179/500][10000/26955] n_relations: 60, Loss: 0.200617, Agg: 0.252109\n",
      "train [179/500][15000/26955] n_relations: 60, Loss: 0.979920, Agg: 0.249716\n",
      "train [179/500][20000/26955] n_relations: 62, Loss: 0.164483, Agg: 0.251798\n",
      "train [179/500][25000/26955] n_relations: 60, Loss: 0.213761, Agg: 0.249101\n",
      "train [179/500] Loss: 0.2472\n",
      "train [180/500][0/26955] n_relations: 60, Loss: 0.478730, Agg: 0.748158\n",
      "train [180/500][5000/26955] n_relations: 62, Loss: 0.168426, Agg: 0.233322\n",
      "train [180/500][10000/26955] n_relations: 60, Loss: 0.173549, Agg: 0.227139\n",
      "train [180/500][15000/26955] n_relations: 60, Loss: 0.744003, Agg: 0.227742\n",
      "train [180/500][20000/26955] n_relations: 62, Loss: 0.178307, Agg: 0.225908\n",
      "train [180/500][25000/26955] n_relations: 60, Loss: 0.205638, Agg: 0.225431\n",
      "train [180/500] Loss: 0.2248\n",
      "train [181/500][0/26955] n_relations: 60, Loss: 1.182019, Agg: 1.197616\n",
      "train [181/500][5000/26955] n_relations: 62, Loss: 0.184620, Agg: 0.227108\n",
      "train [181/500][10000/26955] n_relations: 60, Loss: 0.185754, Agg: 0.227590\n",
      "train [181/500][15000/26955] n_relations: 60, Loss: 0.629639, Agg: 0.228942\n",
      "train [181/500][20000/26955] n_relations: 62, Loss: 0.177220, Agg: 0.228460\n",
      "train [181/500][25000/26955] n_relations: 60, Loss: 0.216799, Agg: 0.227143\n",
      "train [181/500] Loss: 0.2286\n",
      "train [182/500][0/26955] n_relations: 60, Loss: 0.495909, Agg: 0.603481\n",
      "train [182/500][5000/26955] n_relations: 62, Loss: 0.281058, Agg: 0.306243\n",
      "train [182/500][10000/26955] n_relations: 60, Loss: 0.233509, Agg: 0.278727\n",
      "train [182/500][15000/26955] n_relations: 60, Loss: 0.875122, Agg: 0.266240\n",
      "train [182/500][20000/26955] n_relations: 62, Loss: 0.180922, Agg: 0.259701\n",
      "train [182/500][25000/26955] n_relations: 60, Loss: 0.332489, Agg: 0.262588\n",
      "train [182/500] Loss: 0.2694\n",
      "train [183/500][0/26955] n_relations: 60, Loss: 0.457412, Agg: 0.704910\n",
      "train [183/500][5000/26955] n_relations: 62, Loss: 0.174502, Agg: 0.281270\n",
      "train [183/500][10000/26955] n_relations: 60, Loss: 0.163619, Agg: 0.260605\n",
      "train [183/500][15000/26955] n_relations: 60, Loss: 0.827771, Agg: 0.251309\n",
      "train [183/500][20000/26955] n_relations: 62, Loss: 0.166426, Agg: 0.247719\n",
      "train [183/500][25000/26955] n_relations: 60, Loss: 0.165828, Agg: 0.241501\n",
      "train [183/500] Loss: 0.2395\n",
      "train [184/500][0/26955] n_relations: 60, Loss: 0.427825, Agg: 0.710514\n",
      "train [184/500][5000/26955] n_relations: 62, Loss: 0.194588, Agg: 0.223842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [184/500][10000/26955] n_relations: 60, Loss: 0.242962, Agg: 0.246709\n",
      "train [184/500][15000/26955] n_relations: 60, Loss: 0.749386, Agg: 0.248030\n",
      "train [184/500][20000/26955] n_relations: 62, Loss: 0.110063, Agg: 0.244679\n",
      "train [184/500][25000/26955] n_relations: 60, Loss: 0.174335, Agg: 0.239002\n",
      "train [184/500] Loss: 0.2368\n",
      "train [185/500][0/26955] n_relations: 60, Loss: 0.554203, Agg: 0.678140\n",
      "train [185/500][5000/26955] n_relations: 62, Loss: 0.155136, Agg: 0.218479\n",
      "train [185/500][10000/26955] n_relations: 60, Loss: 0.168782, Agg: 0.215075\n",
      "train [185/500][15000/26955] n_relations: 60, Loss: 0.531818, Agg: 0.212594\n",
      "train [185/500][20000/26955] n_relations: 62, Loss: 0.153106, Agg: 0.211493\n",
      "train [185/500][25000/26955] n_relations: 60, Loss: 0.155851, Agg: 0.213616\n",
      "train [185/500] Loss: 0.2133\n",
      "train [186/500][0/26955] n_relations: 60, Loss: 0.537041, Agg: 0.739338\n",
      "train [186/500][5000/26955] n_relations: 62, Loss: 0.192180, Agg: 0.216065\n",
      "train [186/500][10000/26955] n_relations: 60, Loss: 0.135389, Agg: 0.210943\n",
      "train [186/500][15000/26955] n_relations: 60, Loss: 0.969423, Agg: 0.209694\n",
      "train [186/500][20000/26955] n_relations: 62, Loss: 0.167825, Agg: 0.210502\n",
      "train [186/500][25000/26955] n_relations: 60, Loss: 0.152273, Agg: 0.208872\n",
      "train [186/500] Loss: 0.2083\n",
      "train [187/500][0/26955] n_relations: 60, Loss: 0.460428, Agg: 0.705980\n",
      "train [187/500][5000/26955] n_relations: 62, Loss: 0.253860, Agg: 0.222745\n",
      "train [187/500][10000/26955] n_relations: 60, Loss: 0.172237, Agg: 0.215631\n",
      "train [187/500][15000/26955] n_relations: 60, Loss: 0.910873, Agg: 0.211135\n",
      "train [187/500][20000/26955] n_relations: 62, Loss: 0.143052, Agg: 0.210226\n",
      "train [187/500][25000/26955] n_relations: 60, Loss: 0.126703, Agg: 0.208046\n",
      "train [187/500] Loss: 0.2075\n",
      "train [188/500][0/26955] n_relations: 60, Loss: 0.351461, Agg: 0.516102\n",
      "train [188/500][5000/26955] n_relations: 62, Loss: 0.145233, Agg: 0.220349\n",
      "train [188/500][10000/26955] n_relations: 60, Loss: 0.145939, Agg: 0.209477\n",
      "train [188/500][15000/26955] n_relations: 60, Loss: 0.788041, Agg: 0.206884\n",
      "train [188/500][20000/26955] n_relations: 62, Loss: 0.169401, Agg: 0.212174\n",
      "train [188/500][25000/26955] n_relations: 60, Loss: 0.158326, Agg: 0.213629\n",
      "train [188/500] Loss: 0.2142\n",
      "train [189/500][0/26955] n_relations: 60, Loss: 0.272034, Agg: 0.544207\n",
      "train [189/500][5000/26955] n_relations: 62, Loss: 0.234872, Agg: 0.222863\n",
      "train [189/500][10000/26955] n_relations: 60, Loss: 0.114505, Agg: 0.214612\n",
      "train [189/500][15000/26955] n_relations: 60, Loss: 0.847666, Agg: 0.211199\n",
      "train [189/500][20000/26955] n_relations: 62, Loss: 0.200702, Agg: 0.211994\n",
      "train [189/500][25000/26955] n_relations: 60, Loss: 0.165693, Agg: 0.210832\n",
      "train [189/500] Loss: 0.2102\n",
      "train [190/500][0/26955] n_relations: 60, Loss: 0.226233, Agg: 0.557165\n",
      "train [190/500][5000/26955] n_relations: 62, Loss: 0.173035, Agg: 0.224026\n",
      "train [190/500][10000/26955] n_relations: 60, Loss: 0.167672, Agg: 0.214877\n",
      "train [190/500][15000/26955] n_relations: 60, Loss: 0.257439, Agg: 0.214065\n",
      "train [190/500][20000/26955] n_relations: 62, Loss: 0.130253, Agg: 0.213906\n",
      "train [190/500][25000/26955] n_relations: 60, Loss: 0.166848, Agg: 0.211759\n",
      "train [190/500] Loss: 0.2108\n",
      "train [191/500][0/26955] n_relations: 60, Loss: 0.762752, Agg: 0.724178\n",
      "train [191/500][5000/26955] n_relations: 62, Loss: 0.165107, Agg: 0.208885\n",
      "train [191/500][10000/26955] n_relations: 60, Loss: 0.196115, Agg: 0.229989\n",
      "train [191/500][15000/26955] n_relations: 60, Loss: 0.782975, Agg: 0.222873\n",
      "train [191/500][20000/26955] n_relations: 62, Loss: 0.167023, Agg: 0.224383\n",
      "train [191/500][25000/26955] n_relations: 60, Loss: 0.162550, Agg: 0.226782\n",
      "train [191/500] Loss: 0.2256\n",
      "train [192/500][0/26955] n_relations: 60, Loss: 0.473773, Agg: 0.643603\n",
      "train [192/500][5000/26955] n_relations: 62, Loss: 0.178994, Agg: 0.219125\n",
      "train [192/500][10000/26955] n_relations: 60, Loss: 0.128441, Agg: 0.213187\n",
      "train [192/500][15000/26955] n_relations: 60, Loss: 0.876079, Agg: 0.210023\n",
      "train [192/500][20000/26955] n_relations: 62, Loss: 0.143352, Agg: 0.211593\n",
      "train [192/500][25000/26955] n_relations: 60, Loss: 0.103129, Agg: 0.208774\n",
      "train [192/500] Loss: 0.2078\n",
      "train [193/500][0/26955] n_relations: 60, Loss: 0.347186, Agg: 0.544251\n",
      "train [193/500][5000/26955] n_relations: 62, Loss: 0.148784, Agg: 0.210581\n",
      "train [193/500][10000/26955] n_relations: 60, Loss: 0.111887, Agg: 0.203988\n",
      "train [193/500][15000/26955] n_relations: 60, Loss: 0.934051, Agg: 0.201059\n",
      "train [193/500][20000/26955] n_relations: 62, Loss: 0.243399, Agg: 0.201223\n",
      "train [193/500][25000/26955] n_relations: 60, Loss: 0.136899, Agg: 0.199705\n",
      "train [193/500] Loss: 0.2001\n",
      "train [194/500][0/26955] n_relations: 60, Loss: 0.362335, Agg: 0.569573\n",
      "train [194/500][5000/26955] n_relations: 62, Loss: 0.158233, Agg: 0.225169\n",
      "train [194/500][10000/26955] n_relations: 60, Loss: 0.133530, Agg: 0.218887\n",
      "train [194/500][15000/26955] n_relations: 60, Loss: 0.848927, Agg: 0.214624\n",
      "train [194/500][20000/26955] n_relations: 62, Loss: 0.209903, Agg: 0.212821\n",
      "train [194/500][25000/26955] n_relations: 60, Loss: 0.178407, Agg: 0.210161\n",
      "train [194/500] Loss: 0.2107\n",
      "train [195/500][0/26955] n_relations: 60, Loss: 0.270614, Agg: 0.503853\n",
      "train [195/500][5000/26955] n_relations: 62, Loss: 0.154159, Agg: 0.228468\n",
      "train [195/500][10000/26955] n_relations: 60, Loss: 0.126682, Agg: 0.211672\n",
      "train [195/500][15000/26955] n_relations: 60, Loss: 0.825949, Agg: 0.206496\n",
      "train [195/500][20000/26955] n_relations: 62, Loss: 0.180073, Agg: 0.206288\n",
      "train [195/500][25000/26955] n_relations: 60, Loss: 0.280898, Agg: 0.208661\n",
      "train [195/500] Loss: 0.2089\n",
      "train [196/500][0/26955] n_relations: 60, Loss: 0.479499, Agg: 0.655038\n",
      "train [196/500][5000/26955] n_relations: 62, Loss: 0.395583, Agg: 0.265011\n",
      "train [196/500][10000/26955] n_relations: 60, Loss: 0.147757, Agg: 0.253761\n",
      "train [196/500][15000/26955] n_relations: 60, Loss: 0.765227, Agg: 0.244187\n",
      "train [196/500][20000/26955] n_relations: 62, Loss: 0.240867, Agg: 0.242508\n",
      "train [196/500][25000/26955] n_relations: 60, Loss: 0.136445, Agg: 0.234754\n",
      "train [196/500] Loss: 0.2316\n",
      "train [197/500][0/26955] n_relations: 60, Loss: 0.279980, Agg: 0.568893\n",
      "train [197/500][5000/26955] n_relations: 62, Loss: 0.147385, Agg: 0.204433\n",
      "train [197/500][10000/26955] n_relations: 60, Loss: 0.182338, Agg: 0.216956\n",
      "train [197/500][15000/26955] n_relations: 60, Loss: 1.005005, Agg: 0.232992\n",
      "train [197/500][20000/26955] n_relations: 62, Loss: 0.198984, Agg: 0.240116\n",
      "train [197/500][25000/26955] n_relations: 60, Loss: 0.244542, Agg: 0.241887\n",
      "train [197/500] Loss: 0.2407\n",
      "train [198/500][0/26955] n_relations: 60, Loss: 0.462618, Agg: 0.729321\n",
      "train [198/500][5000/26955] n_relations: 62, Loss: 0.272905, Agg: 0.225888\n",
      "train [198/500][10000/26955] n_relations: 60, Loss: 0.212512, Agg: 0.222805\n",
      "train [198/500][15000/26955] n_relations: 60, Loss: 0.972901, Agg: 0.221487\n",
      "train [198/500][20000/26955] n_relations: 62, Loss: 0.153937, Agg: 0.240420\n",
      "train [198/500][25000/26955] n_relations: 60, Loss: 0.152885, Agg: 0.236576\n",
      "train [198/500] Loss: 0.2348\n",
      "train [199/500][0/26955] n_relations: 60, Loss: 0.404856, Agg: 0.561929\n",
      "train [199/500][5000/26955] n_relations: 62, Loss: 0.174021, Agg: 0.214041\n",
      "train [199/500][10000/26955] n_relations: 60, Loss: 0.142854, Agg: 0.206430\n",
      "train [199/500][15000/26955] n_relations: 60, Loss: 0.723704, Agg: 0.204338\n",
      "train [199/500][20000/26955] n_relations: 62, Loss: 0.145075, Agg: 0.204459\n",
      "train [199/500][25000/26955] n_relations: 60, Loss: 0.136244, Agg: 0.202923\n",
      "train [199/500] Loss: 0.2025\n",
      "train [200/500][0/26955] n_relations: 60, Loss: 0.611580, Agg: 0.741738\n",
      "train [200/500][5000/26955] n_relations: 62, Loss: 0.144054, Agg: 0.206107\n",
      "train [200/500][10000/26955] n_relations: 60, Loss: 0.121973, Agg: 0.201167\n",
      "train [200/500][15000/26955] n_relations: 60, Loss: 0.956851, Agg: 0.201420\n",
      "train [200/500][20000/26955] n_relations: 62, Loss: 0.132344, Agg: 0.210909\n",
      "train [200/500][25000/26955] n_relations: 60, Loss: 0.129799, Agg: 0.212074\n",
      "train [200/500] Loss: 0.2116\n",
      "train [201/500][0/26955] n_relations: 60, Loss: 0.616961, Agg: 0.708408\n",
      "train [201/500][5000/26955] n_relations: 62, Loss: 0.168854, Agg: 0.237707\n",
      "train [201/500][10000/26955] n_relations: 60, Loss: 0.167102, Agg: 0.229670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [201/500][15000/26955] n_relations: 60, Loss: 0.853839, Agg: 0.227337\n",
      "train [201/500][20000/26955] n_relations: 62, Loss: 0.259911, Agg: 0.249702\n",
      "train [201/500][25000/26955] n_relations: 60, Loss: 0.212739, Agg: 0.246886\n",
      "train [201/500] Loss: 0.2451\n",
      "train [202/500][0/26955] n_relations: 60, Loss: 0.366868, Agg: 0.601443\n",
      "train [202/500][5000/26955] n_relations: 62, Loss: 0.245616, Agg: 0.259980\n",
      "train [202/500][10000/26955] n_relations: 60, Loss: 0.144721, Agg: 0.250570\n",
      "train [202/500][15000/26955] n_relations: 60, Loss: 1.458047, Agg: 0.244525\n",
      "train [202/500][20000/26955] n_relations: 62, Loss: 0.241194, Agg: 0.258412\n",
      "train [202/500][25000/26955] n_relations: 60, Loss: 0.264058, Agg: 0.257545\n",
      "train [202/500] Loss: 0.2568\n",
      "train [203/500][0/26955] n_relations: 60, Loss: 0.620641, Agg: 0.746608\n",
      "train [203/500][5000/26955] n_relations: 62, Loss: 0.206355, Agg: 0.269748\n",
      "train [203/500][10000/26955] n_relations: 60, Loss: 0.220040, Agg: 0.252342\n",
      "train [203/500][15000/26955] n_relations: 60, Loss: 0.777133, Agg: 0.245026\n",
      "train [203/500][20000/26955] n_relations: 62, Loss: 0.230969, Agg: 0.240491\n",
      "train [203/500][25000/26955] n_relations: 60, Loss: 0.217535, Agg: 0.238281\n",
      "train [203/500] Loss: 0.2379\n",
      "train [204/500][0/26955] n_relations: 60, Loss: 0.213446, Agg: 0.404021\n",
      "train [204/500][5000/26955] n_relations: 62, Loss: 0.223099, Agg: 0.234097\n",
      "train [204/500][10000/26955] n_relations: 60, Loss: 0.499346, Agg: 0.238428\n",
      "train [204/500][15000/26955] n_relations: 60, Loss: 1.456055, Agg: 0.254316\n",
      "train [204/500][20000/26955] n_relations: 62, Loss: 0.330499, Agg: 0.267723\n",
      "train [204/500][25000/26955] n_relations: 60, Loss: 0.239777, Agg: 0.275871\n",
      "train [204/500] Loss: 0.2771\n",
      "train [205/500][0/26955] n_relations: 60, Loss: 0.300934, Agg: 0.438087\n",
      "train [205/500][5000/26955] n_relations: 62, Loss: 0.329351, Agg: 0.299537\n",
      "train [205/500][10000/26955] n_relations: 60, Loss: 0.237943, Agg: 0.297661\n",
      "train [205/500][15000/26955] n_relations: 60, Loss: 0.731695, Agg: 0.300051\n",
      "train [205/500][20000/26955] n_relations: 62, Loss: 0.262013, Agg: 0.295202\n",
      "train [205/500][25000/26955] n_relations: 60, Loss: 0.210305, Agg: 0.288824\n",
      "train [205/500] Loss: 0.2864\n",
      "train [206/500][0/26955] n_relations: 60, Loss: 0.273595, Agg: 0.439410\n",
      "train [206/500][5000/26955] n_relations: 62, Loss: 0.243459, Agg: 0.268315\n",
      "train [206/500][10000/26955] n_relations: 60, Loss: 0.291932, Agg: 0.258821\n",
      "train [206/500][15000/26955] n_relations: 60, Loss: 0.725724, Agg: 0.252206\n",
      "train [206/500][20000/26955] n_relations: 62, Loss: 0.185831, Agg: 0.253226\n",
      "train [206/500][25000/26955] n_relations: 60, Loss: 0.192659, Agg: 0.250092\n",
      "train [206/500] Loss: 0.2484\n",
      "train [207/500][0/26955] n_relations: 60, Loss: 0.281677, Agg: 0.404515\n",
      "train [207/500][5000/26955] n_relations: 62, Loss: 0.231623, Agg: 0.262260\n",
      "train [207/500][10000/26955] n_relations: 60, Loss: 0.222739, Agg: 0.252479\n",
      "train [207/500][15000/26955] n_relations: 60, Loss: 0.938591, Agg: 0.245031\n",
      "train [207/500][20000/26955] n_relations: 62, Loss: 0.159637, Agg: 0.240749\n",
      "train [207/500][25000/26955] n_relations: 60, Loss: 0.273084, Agg: 0.244510\n",
      "train [207/500] Loss: 0.2465\n",
      "train [208/500][0/26955] n_relations: 60, Loss: 0.562411, Agg: 0.666231\n",
      "train [208/500][5000/26955] n_relations: 62, Loss: 0.209701, Agg: 0.292878\n",
      "train [208/500][10000/26955] n_relations: 60, Loss: 0.202946, Agg: 0.285366\n",
      "train [208/500][15000/26955] n_relations: 60, Loss: 0.507161, Agg: 0.273771\n",
      "train [208/500][20000/26955] n_relations: 62, Loss: 0.117527, Agg: 0.266251\n",
      "train [208/500][25000/26955] n_relations: 60, Loss: 0.160448, Agg: 0.260418\n",
      "train [208/500] Loss: 0.2584\n",
      "train [209/500][0/26955] n_relations: 60, Loss: 0.375590, Agg: 0.587683\n",
      "train [209/500][5000/26955] n_relations: 62, Loss: 0.160120, Agg: 0.239390\n",
      "train [209/500][10000/26955] n_relations: 60, Loss: 0.196360, Agg: 0.240595\n",
      "train [209/500][15000/26955] n_relations: 60, Loss: 1.031311, Agg: 0.237382\n",
      "train [209/500][20000/26955] n_relations: 62, Loss: 0.162661, Agg: 0.235372\n",
      "train [209/500][25000/26955] n_relations: 60, Loss: 0.171958, Agg: 0.232180\n",
      "train [209/500] Loss: 0.2311\n",
      "train [210/500][0/26955] n_relations: 60, Loss: 0.171956, Agg: 0.433420\n",
      "train [210/500][5000/26955] n_relations: 62, Loss: 0.147138, Agg: 0.236877\n",
      "train [210/500][10000/26955] n_relations: 60, Loss: 0.162658, Agg: 0.230460\n",
      "train [210/500][15000/26955] n_relations: 60, Loss: 0.988591, Agg: 0.232318\n",
      "train [210/500][20000/26955] n_relations: 62, Loss: 0.196220, Agg: 0.231037\n",
      "train [210/500][25000/26955] n_relations: 60, Loss: 0.147936, Agg: 0.231080\n",
      "train [210/500] Loss: 0.2302\n",
      "train [211/500][0/26955] n_relations: 60, Loss: 0.170549, Agg: 0.345984\n",
      "train [211/500][5000/26955] n_relations: 62, Loss: 0.158616, Agg: 0.229221\n",
      "train [211/500][10000/26955] n_relations: 60, Loss: 0.152131, Agg: 0.222756\n",
      "train [211/500][15000/26955] n_relations: 60, Loss: 1.096841, Agg: 0.224691\n",
      "train [211/500][20000/26955] n_relations: 62, Loss: 0.157777, Agg: 0.226488\n",
      "train [211/500][25000/26955] n_relations: 60, Loss: 0.183899, Agg: 0.225480\n",
      "train [211/500] Loss: 0.2248\n",
      "train [212/500][0/26955] n_relations: 60, Loss: 0.170031, Agg: 0.399271\n",
      "train [212/500][5000/26955] n_relations: 62, Loss: 0.163496, Agg: 0.228214\n",
      "train [212/500][10000/26955] n_relations: 60, Loss: 0.159475, Agg: 0.239617\n",
      "train [212/500][15000/26955] n_relations: 60, Loss: 1.365316, Agg: 0.240762\n",
      "train [212/500][20000/26955] n_relations: 62, Loss: 0.248896, Agg: 0.240230\n",
      "train [212/500][25000/26955] n_relations: 60, Loss: 0.179100, Agg: 0.236566\n",
      "train [212/500] Loss: 0.2351\n",
      "train [213/500][0/26955] n_relations: 60, Loss: 0.195990, Agg: 0.319628\n",
      "train [213/500][5000/26955] n_relations: 62, Loss: 0.227850, Agg: 0.253271\n",
      "train [213/500][10000/26955] n_relations: 60, Loss: 0.237778, Agg: 0.269477\n",
      "train [213/500][15000/26955] n_relations: 60, Loss: 1.011501, Agg: 0.263493\n",
      "train [213/500][20000/26955] n_relations: 62, Loss: 0.265014, Agg: 0.259802\n",
      "train [213/500][25000/26955] n_relations: 60, Loss: 0.186596, Agg: 0.254688\n",
      "train [213/500] Loss: 0.2538\n",
      "train [214/500][0/26955] n_relations: 60, Loss: 0.395624, Agg: 0.597528\n",
      "train [214/500][5000/26955] n_relations: 62, Loss: 0.161929, Agg: 0.260034\n",
      "train [214/500][10000/26955] n_relations: 60, Loss: 0.264530, Agg: 0.254294\n",
      "train [214/500][15000/26955] n_relations: 60, Loss: 0.951034, Agg: 0.255530\n",
      "train [214/500][20000/26955] n_relations: 62, Loss: 0.182652, Agg: 0.253327\n",
      "train [214/500][25000/26955] n_relations: 60, Loss: 0.178171, Agg: 0.248288\n",
      "train [214/500] Loss: 0.2466\n",
      "train [215/500][0/26955] n_relations: 60, Loss: 0.364123, Agg: 0.480139\n",
      "train [215/500][5000/26955] n_relations: 62, Loss: 0.183809, Agg: 0.240435\n",
      "train [215/500][10000/26955] n_relations: 60, Loss: 0.157916, Agg: 0.230980\n",
      "train [215/500][15000/26955] n_relations: 60, Loss: 0.826475, Agg: 0.228531\n",
      "train [215/500][20000/26955] n_relations: 62, Loss: 0.148075, Agg: 0.226682\n",
      "train [215/500][25000/26955] n_relations: 60, Loss: 0.149069, Agg: 0.226240\n",
      "train [215/500] Loss: 0.2257\n",
      "train [216/500][0/26955] n_relations: 60, Loss: 0.610460, Agg: 0.763559\n",
      "train [216/500][5000/26955] n_relations: 62, Loss: 0.172643, Agg: 0.256271\n",
      "train [216/500][10000/26955] n_relations: 60, Loss: 0.145966, Agg: 0.247478\n",
      "train [216/500][15000/26955] n_relations: 60, Loss: 1.054724, Agg: 0.238574\n",
      "train [216/500][20000/26955] n_relations: 62, Loss: 0.136830, Agg: 0.238612\n",
      "train [216/500][25000/26955] n_relations: 60, Loss: 0.232827, Agg: 0.234849\n",
      "train [216/500] Loss: 0.2336\n",
      "train [217/500][0/26955] n_relations: 60, Loss: 0.317459, Agg: 0.537898\n",
      "train [217/500][5000/26955] n_relations: 62, Loss: 0.191743, Agg: 0.237158\n",
      "train [217/500][10000/26955] n_relations: 60, Loss: 0.196055, Agg: 0.245948\n",
      "train [217/500][15000/26955] n_relations: 60, Loss: 1.059632, Agg: 0.244035\n",
      "train [217/500][20000/26955] n_relations: 62, Loss: 0.162041, Agg: 0.239859\n",
      "train [217/500][25000/26955] n_relations: 60, Loss: 0.167610, Agg: 0.238517\n",
      "train [217/500] Loss: 0.2368\n",
      "train [218/500][0/26955] n_relations: 60, Loss: 0.330053, Agg: 0.448802\n",
      "train [218/500][5000/26955] n_relations: 62, Loss: 0.279992, Agg: 0.225963\n",
      "train [218/500][10000/26955] n_relations: 60, Loss: 0.218342, Agg: 0.230630\n",
      "train [218/500][15000/26955] n_relations: 60, Loss: 0.746509, Agg: 0.229207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [218/500][20000/26955] n_relations: 62, Loss: 0.201430, Agg: 0.231622\n",
      "train [218/500][25000/26955] n_relations: 60, Loss: 0.174934, Agg: 0.231594\n",
      "train [218/500] Loss: 0.2305\n",
      "train [219/500][0/26955] n_relations: 60, Loss: 0.139301, Agg: 0.283731\n",
      "train [219/500][5000/26955] n_relations: 62, Loss: 0.265698, Agg: 0.222924\n",
      "train [219/500][10000/26955] n_relations: 60, Loss: 0.163736, Agg: 0.217765\n",
      "train [219/500][15000/26955] n_relations: 60, Loss: 1.074718, Agg: 0.219490\n",
      "train [219/500][20000/26955] n_relations: 62, Loss: 0.272819, Agg: 0.222973\n",
      "train [219/500][25000/26955] n_relations: 60, Loss: 0.159881, Agg: 0.224028\n",
      "train [219/500] Loss: 0.2242\n",
      "train [220/500][0/26955] n_relations: 60, Loss: 0.534250, Agg: 0.621423\n",
      "train [220/500][5000/26955] n_relations: 62, Loss: 0.232584, Agg: 0.260185\n",
      "train [220/500][10000/26955] n_relations: 60, Loss: 0.237152, Agg: 0.265173\n",
      "train [220/500][15000/26955] n_relations: 60, Loss: 0.949512, Agg: 0.255571\n",
      "train [220/500][20000/26955] n_relations: 62, Loss: 0.147671, Agg: 0.250719\n",
      "train [220/500][25000/26955] n_relations: 60, Loss: 0.301729, Agg: 0.248419\n",
      "train [220/500] Loss: 0.2473\n",
      "train [221/500][0/26955] n_relations: 60, Loss: 0.324192, Agg: 0.423981\n",
      "train [221/500][5000/26955] n_relations: 62, Loss: 0.181271, Agg: 0.239834\n",
      "train [221/500][10000/26955] n_relations: 60, Loss: 0.247954, Agg: 0.258522\n",
      "train [221/500][15000/26955] n_relations: 60, Loss: 0.388884, Agg: 0.257968\n",
      "train [221/500][20000/26955] n_relations: 62, Loss: 0.255875, Agg: 0.272457\n",
      "train [221/500][25000/26955] n_relations: 60, Loss: 0.221100, Agg: 0.271148\n",
      "train [221/500] Loss: 0.2703\n",
      "train [222/500][0/26955] n_relations: 60, Loss: 0.690289, Agg: 0.702116\n",
      "train [222/500][5000/26955] n_relations: 62, Loss: 0.192588, Agg: 0.267782\n",
      "train [222/500][10000/26955] n_relations: 60, Loss: 0.188065, Agg: 0.250648\n",
      "train [222/500][15000/26955] n_relations: 60, Loss: 1.001002, Agg: 0.249855\n",
      "train [222/500][20000/26955] n_relations: 62, Loss: 0.286174, Agg: 0.248422\n",
      "train [222/500][25000/26955] n_relations: 60, Loss: 0.186297, Agg: 0.245356\n",
      "train [222/500] Loss: 0.2469\n",
      "train [223/500][0/26955] n_relations: 60, Loss: 0.761443, Agg: 1.034268\n",
      "train [223/500][5000/26955] n_relations: 62, Loss: 0.179228, Agg: 0.254144\n",
      "train [223/500][10000/26955] n_relations: 60, Loss: 0.271663, Agg: 0.257610\n",
      "train [223/500][15000/26955] n_relations: 60, Loss: 0.947554, Agg: 0.272780\n",
      "train [223/500][20000/26955] n_relations: 62, Loss: 0.309239, Agg: 0.281327\n",
      "train [223/500][25000/26955] n_relations: 60, Loss: 0.266497, Agg: 0.282366\n",
      "train [223/500] Loss: 0.2814\n",
      "train [224/500][0/26955] n_relations: 60, Loss: 0.367074, Agg: 0.532363\n",
      "train [224/500][5000/26955] n_relations: 62, Loss: 0.231687, Agg: 0.284762\n",
      "train [224/500][10000/26955] n_relations: 60, Loss: 0.178355, Agg: 0.278923\n",
      "train [224/500][15000/26955] n_relations: 60, Loss: 1.084012, Agg: 0.279146\n",
      "train [224/500][20000/26955] n_relations: 62, Loss: 0.253776, Agg: 0.273554\n",
      "train [224/500][25000/26955] n_relations: 60, Loss: 0.145828, Agg: 0.268162\n",
      "train [224/500] Loss: 0.2669\n",
      "train [225/500][0/26955] n_relations: 60, Loss: 0.519138, Agg: 0.661504\n",
      "train [225/500][5000/26955] n_relations: 62, Loss: 0.245157, Agg: 0.282115\n",
      "train [225/500][10000/26955] n_relations: 60, Loss: 0.305928, Agg: 0.269269\n",
      "train [225/500][15000/26955] n_relations: 60, Loss: 1.130396, Agg: 0.261225\n",
      "train [225/500][20000/26955] n_relations: 62, Loss: 0.189048, Agg: 0.260720\n",
      "train [225/500][25000/26955] n_relations: 60, Loss: 0.198493, Agg: 0.256734\n",
      "train [225/500] Loss: 0.2560\n",
      "train [226/500][0/26955] n_relations: 60, Loss: 0.261497, Agg: 0.394556\n",
      "train [226/500][5000/26955] n_relations: 62, Loss: 0.132423, Agg: 0.273182\n",
      "train [226/500][10000/26955] n_relations: 60, Loss: 0.173524, Agg: 0.258331\n",
      "train [226/500][15000/26955] n_relations: 60, Loss: 0.978402, Agg: 0.253396\n",
      "train [226/500][20000/26955] n_relations: 62, Loss: 0.234404, Agg: 0.256681\n",
      "train [226/500][25000/26955] n_relations: 60, Loss: 0.202806, Agg: 0.259366\n",
      "train [226/500] Loss: 0.2660\n",
      "train [227/500][0/26955] n_relations: 60, Loss: 0.601291, Agg: 0.614333\n",
      "train [227/500][5000/26955] n_relations: 62, Loss: 0.537627, Agg: 0.350125\n",
      "train [227/500][10000/26955] n_relations: 60, Loss: 0.446916, Agg: 0.380106\n",
      "train [227/500][15000/26955] n_relations: 60, Loss: 0.888481, Agg: 0.372031\n",
      "train [227/500][20000/26955] n_relations: 62, Loss: 0.221109, Agg: 0.372551\n",
      "train [227/500][25000/26955] n_relations: 60, Loss: 0.222331, Agg: 0.371835\n",
      "train [227/500] Loss: 0.3738\n",
      "train [228/500][0/26955] n_relations: 60, Loss: 0.626607, Agg: 0.877666\n",
      "train [228/500][5000/26955] n_relations: 62, Loss: 0.233413, Agg: 0.388699\n",
      "train [228/500][10000/26955] n_relations: 60, Loss: 0.301612, Agg: 0.394884\n",
      "train [228/500][15000/26955] n_relations: 60, Loss: 0.901260, Agg: 0.394727\n",
      "train [228/500][20000/26955] n_relations: 62, Loss: 0.226178, Agg: 0.392198\n",
      "train [228/500][25000/26955] n_relations: 60, Loss: 0.276577, Agg: 0.389209\n",
      "train [228/500] Loss: 0.3869\n",
      "train [229/500][0/26955] n_relations: 60, Loss: 0.663055, Agg: 0.830824\n",
      "train [229/500][5000/26955] n_relations: 62, Loss: 0.242955, Agg: 0.365139\n",
      "train [229/500][10000/26955] n_relations: 60, Loss: 0.309267, Agg: 0.361488\n",
      "train [229/500][15000/26955] n_relations: 60, Loss: 1.313199, Agg: 0.363963\n",
      "train [229/500][20000/26955] n_relations: 62, Loss: 0.199413, Agg: 0.364417\n",
      "train [229/500][25000/26955] n_relations: 60, Loss: 0.235948, Agg: 0.362639\n",
      "train [229/500] Loss: 0.3630\n",
      "train [230/500][0/26955] n_relations: 60, Loss: 0.401444, Agg: 0.638637\n",
      "train [230/500][5000/26955] n_relations: 62, Loss: 0.234056, Agg: 0.357938\n",
      "train [230/500][10000/26955] n_relations: 60, Loss: 0.281474, Agg: 0.353287\n",
      "train [230/500][15000/26955] n_relations: 60, Loss: 0.982033, Agg: 0.351524\n",
      "train [230/500][20000/26955] n_relations: 62, Loss: 0.214148, Agg: 0.351030\n",
      "train [230/500][25000/26955] n_relations: 60, Loss: 0.326463, Agg: 0.346911\n",
      "train [230/500] Loss: 0.3449\n",
      "train [231/500][0/26955] n_relations: 60, Loss: 0.259309, Agg: 0.502278\n",
      "train [231/500][5000/26955] n_relations: 62, Loss: 0.188635, Agg: 0.324339\n",
      "train [231/500][10000/26955] n_relations: 60, Loss: 0.265131, Agg: 0.316725\n",
      "train [231/500][15000/26955] n_relations: 60, Loss: 0.860119, Agg: 0.312504\n",
      "train [231/500][20000/26955] n_relations: 62, Loss: 0.305852, Agg: 0.310641\n",
      "train [231/500][25000/26955] n_relations: 60, Loss: 0.303155, Agg: 0.325055\n",
      "train [231/500] Loss: 0.3316\n",
      "train [232/500][0/26955] n_relations: 60, Loss: 0.796310, Agg: 0.999414\n",
      "train [232/500][5000/26955] n_relations: 62, Loss: 0.208141, Agg: 0.395507\n",
      "train [232/500][10000/26955] n_relations: 60, Loss: 0.237421, Agg: 0.359392\n",
      "train [232/500][15000/26955] n_relations: 60, Loss: 0.917420, Agg: 0.343610\n",
      "train [232/500][20000/26955] n_relations: 62, Loss: 0.218532, Agg: 0.337140\n",
      "train [232/500][25000/26955] n_relations: 60, Loss: 0.172496, Agg: 0.330747\n",
      "train [232/500] Loss: 0.3288\n",
      "train [233/500][0/26955] n_relations: 60, Loss: 0.279926, Agg: 0.475832\n",
      "train [233/500][5000/26955] n_relations: 62, Loss: 0.187493, Agg: 0.302028\n",
      "train [233/500][10000/26955] n_relations: 60, Loss: 0.281222, Agg: 0.300824\n",
      "train [233/500][15000/26955] n_relations: 60, Loss: 0.876344, Agg: 0.295047\n",
      "train [233/500][20000/26955] n_relations: 62, Loss: 0.279509, Agg: 0.295803\n",
      "train [233/500][25000/26955] n_relations: 60, Loss: 0.147784, Agg: 0.293879\n",
      "train [233/500] Loss: 0.2926\n",
      "train [234/500][0/26955] n_relations: 60, Loss: 0.270117, Agg: 0.444313\n",
      "train [234/500][5000/26955] n_relations: 62, Loss: 0.180418, Agg: 0.290923\n",
      "train [234/500][10000/26955] n_relations: 60, Loss: 0.247779, Agg: 0.284872\n",
      "train [234/500][15000/26955] n_relations: 60, Loss: 0.827735, Agg: 0.284456\n",
      "train [234/500][20000/26955] n_relations: 62, Loss: 0.161347, Agg: 0.283775\n",
      "train [234/500][25000/26955] n_relations: 60, Loss: 0.211822, Agg: 0.281968\n",
      "train [234/500] Loss: 0.2807\n",
      "train [235/500][0/26955] n_relations: 60, Loss: 0.371987, Agg: 0.490144\n",
      "train [235/500][5000/26955] n_relations: 62, Loss: 0.209333, Agg: 0.277287\n",
      "train [235/500][10000/26955] n_relations: 60, Loss: 0.208215, Agg: 0.277148\n",
      "train [235/500][15000/26955] n_relations: 60, Loss: 0.823492, Agg: 0.282789\n",
      "train [235/500][20000/26955] n_relations: 62, Loss: 0.408454, Agg: 0.281914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [235/500][25000/26955] n_relations: 60, Loss: 0.168600, Agg: 0.284139\n",
      "train [235/500] Loss: 0.2832\n",
      "train [236/500][0/26955] n_relations: 60, Loss: 0.419700, Agg: 0.522241\n",
      "train [236/500][5000/26955] n_relations: 62, Loss: 0.161806, Agg: 0.265890\n",
      "train [236/500][10000/26955] n_relations: 60, Loss: 0.255718, Agg: 0.275058\n",
      "train [236/500][15000/26955] n_relations: 60, Loss: 0.803257, Agg: 0.269535\n",
      "train [236/500][20000/26955] n_relations: 62, Loss: 0.170313, Agg: 0.267452\n",
      "train [236/500][25000/26955] n_relations: 60, Loss: 0.213669, Agg: 0.269367\n",
      "train [236/500] Loss: 0.2683\n",
      "train [237/500][0/26955] n_relations: 60, Loss: 0.237165, Agg: 0.419420\n",
      "train [237/500][5000/26955] n_relations: 62, Loss: 0.172910, Agg: 0.297372\n",
      "train [237/500][10000/26955] n_relations: 60, Loss: 0.180713, Agg: 0.283459\n",
      "train [237/500][15000/26955] n_relations: 60, Loss: 0.821908, Agg: 0.288682\n",
      "train [237/500][20000/26955] n_relations: 62, Loss: 0.243620, Agg: 0.300167\n",
      "train [237/500][25000/26955] n_relations: 60, Loss: 0.282267, Agg: 0.300004\n",
      "train [237/500] Loss: 0.2991\n",
      "train [238/500][0/26955] n_relations: 60, Loss: 0.569302, Agg: 0.679322\n",
      "train [238/500][5000/26955] n_relations: 62, Loss: 0.239799, Agg: 0.290673\n",
      "train [238/500][10000/26955] n_relations: 60, Loss: 0.220851, Agg: 0.281196\n",
      "train [238/500][15000/26955] n_relations: 60, Loss: 0.811655, Agg: 0.273225\n",
      "train [238/500][20000/26955] n_relations: 62, Loss: 0.211325, Agg: 0.271357\n",
      "train [238/500][25000/26955] n_relations: 60, Loss: 0.192116, Agg: 0.269111\n",
      "train [238/500] Loss: 0.2680\n",
      "train [239/500][0/26955] n_relations: 60, Loss: 0.291034, Agg: 0.459442\n",
      "train [239/500][5000/26955] n_relations: 62, Loss: 0.175495, Agg: 0.268980\n",
      "train [239/500][10000/26955] n_relations: 60, Loss: 0.182997, Agg: 0.261520\n",
      "train [239/500][15000/26955] n_relations: 60, Loss: 0.818515, Agg: 0.263647\n",
      "train [239/500][20000/26955] n_relations: 62, Loss: 0.250589, Agg: 0.265019\n",
      "train [239/500][25000/26955] n_relations: 60, Loss: 0.154240, Agg: 0.263566\n",
      "train [239/500] Loss: 0.2616\n",
      "train [240/500][0/26955] n_relations: 60, Loss: 0.306769, Agg: 0.453756\n",
      "train [240/500][5000/26955] n_relations: 62, Loss: 0.170429, Agg: 0.259814\n",
      "train [240/500][10000/26955] n_relations: 60, Loss: 0.171336, Agg: 0.260416\n",
      "train [240/500][15000/26955] n_relations: 60, Loss: 0.804463, Agg: 0.257906\n",
      "train [240/500][20000/26955] n_relations: 62, Loss: 0.213721, Agg: 0.257669\n",
      "train [240/500][25000/26955] n_relations: 60, Loss: 0.187327, Agg: 0.262725\n",
      "train [240/500] Loss: 0.2631\n",
      "train [241/500][0/26955] n_relations: 60, Loss: 0.752244, Agg: 0.682255\n",
      "train [241/500][5000/26955] n_relations: 62, Loss: 0.198643, Agg: 0.282490\n",
      "train [241/500][10000/26955] n_relations: 60, Loss: 0.212177, Agg: 0.277229\n",
      "train [241/500][15000/26955] n_relations: 60, Loss: 0.803947, Agg: 0.273106\n",
      "train [241/500][20000/26955] n_relations: 62, Loss: 0.144063, Agg: 0.270863\n",
      "train [241/500][25000/26955] n_relations: 60, Loss: 0.238067, Agg: 0.271683\n",
      "train [241/500] Loss: 0.2725\n",
      "train [242/500][0/26955] n_relations: 60, Loss: 0.332712, Agg: 0.460772\n",
      "train [242/500][5000/26955] n_relations: 62, Loss: 0.191391, Agg: 0.275564\n",
      "train [242/500][10000/26955] n_relations: 60, Loss: 0.471920, Agg: 0.289837\n",
      "train [242/500][15000/26955] n_relations: 60, Loss: 0.784411, Agg: 0.292788\n",
      "train [242/500][20000/26955] n_relations: 62, Loss: 0.307156, Agg: 0.288673\n",
      "train [242/500][25000/26955] n_relations: 60, Loss: 0.131320, Agg: 0.282845\n",
      "train [242/500] Loss: 0.2807\n",
      "train [243/500][0/26955] n_relations: 60, Loss: 0.401599, Agg: 0.485321\n",
      "train [243/500][5000/26955] n_relations: 62, Loss: 0.185667, Agg: 0.260919\n",
      "train [243/500][10000/26955] n_relations: 60, Loss: 0.226963, Agg: 0.261656\n",
      "train [243/500][15000/26955] n_relations: 60, Loss: 0.749411, Agg: 0.265299\n",
      "train [243/500][20000/26955] n_relations: 62, Loss: 0.245002, Agg: 0.267161\n",
      "train [243/500][25000/26955] n_relations: 60, Loss: 0.168704, Agg: 0.267136\n",
      "train [243/500] Loss: 0.2669\n",
      "train [244/500][0/26955] n_relations: 60, Loss: 0.553555, Agg: 0.508485\n",
      "train [244/500][5000/26955] n_relations: 62, Loss: 0.168627, Agg: 0.273275\n",
      "train [244/500][10000/26955] n_relations: 60, Loss: 0.203883, Agg: 0.270908\n",
      "train [244/500][15000/26955] n_relations: 60, Loss: 0.735252, Agg: 0.269998\n",
      "train [244/500][20000/26955] n_relations: 62, Loss: 0.263413, Agg: 0.275340\n",
      "train [244/500][25000/26955] n_relations: 60, Loss: 0.195360, Agg: 0.275783\n",
      "train [244/500] Loss: 0.2747\n",
      "train [245/500][0/26955] n_relations: 60, Loss: 0.615676, Agg: 0.537229\n",
      "train [245/500][5000/26955] n_relations: 62, Loss: 0.207066, Agg: 0.269349\n",
      "train [245/500][10000/26955] n_relations: 60, Loss: 0.196212, Agg: 0.270896\n",
      "train [245/500][15000/26955] n_relations: 60, Loss: 0.782406, Agg: 0.268869\n",
      "train [245/500][20000/26955] n_relations: 62, Loss: 0.271617, Agg: 0.269745\n",
      "train [245/500][25000/26955] n_relations: 60, Loss: 0.185128, Agg: 0.269828\n",
      "train [245/500] Loss: 0.2704\n",
      "train [246/500][0/26955] n_relations: 60, Loss: 0.378486, Agg: 0.517628\n",
      "train [246/500][5000/26955] n_relations: 62, Loss: 0.196245, Agg: 0.278101\n",
      "train [246/500][10000/26955] n_relations: 60, Loss: 0.209558, Agg: 0.272126\n",
      "train [246/500][15000/26955] n_relations: 60, Loss: 0.807830, Agg: 0.285132\n",
      "train [246/500][20000/26955] n_relations: 62, Loss: 0.200702, Agg: 0.286439\n",
      "train [246/500][25000/26955] n_relations: 60, Loss: 0.173061, Agg: 0.282906\n",
      "train [246/500] Loss: 0.2809\n",
      "train [247/500][0/26955] n_relations: 60, Loss: 0.662357, Agg: 0.563851\n",
      "train [247/500][5000/26955] n_relations: 62, Loss: 0.193735, Agg: 0.263519\n",
      "train [247/500][10000/26955] n_relations: 60, Loss: 0.197429, Agg: 0.259400\n",
      "train [247/500][15000/26955] n_relations: 60, Loss: 0.713479, Agg: 0.259471\n",
      "train [247/500][20000/26955] n_relations: 62, Loss: 0.304826, Agg: 0.258585\n",
      "train [247/500][25000/26955] n_relations: 60, Loss: 0.227986, Agg: 0.262999\n",
      "train [247/500] Loss: 0.2629\n",
      "train [248/500][0/26955] n_relations: 60, Loss: 0.594791, Agg: 0.511535\n",
      "train [248/500][5000/26955] n_relations: 62, Loss: 0.191794, Agg: 0.267544\n",
      "train [248/500][10000/26955] n_relations: 60, Loss: 0.170752, Agg: 0.259510\n",
      "train [248/500][15000/26955] n_relations: 60, Loss: 0.664906, Agg: 0.258140\n",
      "train [248/500][20000/26955] n_relations: 62, Loss: 0.271335, Agg: 0.256952\n",
      "train [248/500][25000/26955] n_relations: 60, Loss: 0.195154, Agg: 0.257208\n",
      "train [248/500] Loss: 0.2585\n",
      "train [249/500][0/26955] n_relations: 60, Loss: 0.514882, Agg: 0.497403\n",
      "train [249/500][5000/26955] n_relations: 62, Loss: 0.289545, Agg: 0.307533\n",
      "train [249/500][10000/26955] n_relations: 60, Loss: 0.680318, Agg: 0.367567\n",
      "train [249/500][15000/26955] n_relations: 60, Loss: 0.804569, Agg: 0.386461\n",
      "train [249/500][20000/26955] n_relations: 62, Loss: 0.242755, Agg: 0.381917\n",
      "train [249/500][25000/26955] n_relations: 60, Loss: 0.335611, Agg: 0.375677\n",
      "train [249/500] Loss: 0.3729\n",
      "train [250/500][0/26955] n_relations: 60, Loss: 0.854358, Agg: 0.713720\n",
      "train [250/500][5000/26955] n_relations: 62, Loss: 0.247304, Agg: 0.335413\n",
      "train [250/500][10000/26955] n_relations: 60, Loss: 0.302779, Agg: 0.329782\n",
      "train [250/500][15000/26955] n_relations: 60, Loss: 0.731020, Agg: 0.324904\n",
      "train [250/500][20000/26955] n_relations: 62, Loss: 0.268878, Agg: 0.319599\n",
      "train [250/500][25000/26955] n_relations: 60, Loss: 0.266734, Agg: 0.316134\n",
      "train [250/500] Loss: 0.3168\n",
      "train [251/500][0/26955] n_relations: 60, Loss: 0.793464, Agg: 0.715205\n",
      "train [251/500][5000/26955] n_relations: 62, Loss: 0.213887, Agg: 0.347346\n",
      "train [251/500][10000/26955] n_relations: 60, Loss: 0.312693, Agg: 0.339104\n",
      "train [251/500][15000/26955] n_relations: 60, Loss: 0.916647, Agg: 0.341380\n",
      "train [251/500][20000/26955] n_relations: 62, Loss: 0.242822, Agg: 0.335691\n",
      "train [251/500][25000/26955] n_relations: 60, Loss: 0.246046, Agg: 0.331216\n",
      "train [251/500] Loss: 0.3296\n",
      "train [252/500][0/26955] n_relations: 60, Loss: 0.764455, Agg: 0.754144\n",
      "train [252/500][5000/26955] n_relations: 62, Loss: 0.233974, Agg: 0.318887\n",
      "train [252/500][10000/26955] n_relations: 60, Loss: 0.292533, Agg: 0.325750\n",
      "train [252/500][15000/26955] n_relations: 60, Loss: 0.688120, Agg: 0.323065\n",
      "train [252/500][20000/26955] n_relations: 62, Loss: 0.255472, Agg: 0.319812\n",
      "train [252/500][25000/26955] n_relations: 60, Loss: 0.243472, Agg: 0.317933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [252/500] Loss: 0.3176\n",
      "train [253/500][0/26955] n_relations: 60, Loss: 0.724877, Agg: 0.678467\n",
      "train [253/500][5000/26955] n_relations: 62, Loss: 0.229678, Agg: 0.312714\n",
      "train [253/500][10000/26955] n_relations: 60, Loss: 0.341828, Agg: 0.306272\n",
      "train [253/500][15000/26955] n_relations: 60, Loss: 0.633906, Agg: 0.305549\n",
      "train [253/500][20000/26955] n_relations: 62, Loss: 0.234411, Agg: 0.305729\n",
      "train [253/500][25000/26955] n_relations: 60, Loss: 0.255013, Agg: 0.305079\n",
      "train [253/500] Loss: 0.3038\n",
      "train [254/500][0/26955] n_relations: 60, Loss: 0.470459, Agg: 0.512568\n",
      "train [254/500][5000/26955] n_relations: 62, Loss: 0.202602, Agg: 0.302298\n",
      "train [254/500][10000/26955] n_relations: 60, Loss: 0.323574, Agg: 0.296415\n",
      "train [254/500][15000/26955] n_relations: 60, Loss: 0.586166, Agg: 0.298072\n",
      "train [254/500][20000/26955] n_relations: 62, Loss: 0.240207, Agg: 0.306467\n",
      "train [254/500][25000/26955] n_relations: 60, Loss: 0.450325, Agg: 0.316345\n",
      "train [254/500] Loss: 0.3165\n",
      "train [255/500][0/26955] n_relations: 60, Loss: 0.435206, Agg: 0.547015\n",
      "train [255/500][5000/26955] n_relations: 62, Loss: 0.261353, Agg: 0.322379\n",
      "train [255/500][10000/26955] n_relations: 60, Loss: 0.508172, Agg: 0.343977\n",
      "train [255/500][15000/26955] n_relations: 60, Loss: 0.835361, Agg: 0.352393\n",
      "train [255/500][20000/26955] n_relations: 62, Loss: 0.178047, Agg: 0.346800\n",
      "train [255/500][25000/26955] n_relations: 60, Loss: 0.376243, Agg: 0.356970\n",
      "train [255/500] Loss: 0.3575\n",
      "train [256/500][0/26955] n_relations: 60, Loss: 0.298141, Agg: 0.381645\n",
      "train [256/500][5000/26955] n_relations: 62, Loss: 0.254083, Agg: 0.346434\n",
      "train [256/500][10000/26955] n_relations: 60, Loss: 0.322563, Agg: 0.340486\n",
      "train [256/500][15000/26955] n_relations: 60, Loss: 0.844507, Agg: 0.330555\n",
      "train [256/500][20000/26955] n_relations: 62, Loss: 0.205308, Agg: 0.320934\n",
      "train [256/500][25000/26955] n_relations: 60, Loss: 0.253522, Agg: 0.312968\n",
      "train [256/500] Loss: 0.3107\n",
      "train [257/500][0/26955] n_relations: 60, Loss: 0.507077, Agg: 0.554284\n",
      "train [257/500][5000/26955] n_relations: 62, Loss: 0.569357, Agg: 0.314555\n",
      "train [257/500][10000/26955] n_relations: 60, Loss: 0.257634, Agg: 0.325045\n",
      "train [257/500][15000/26955] n_relations: 60, Loss: 0.728707, Agg: 0.306788\n",
      "train [257/500][20000/26955] n_relations: 62, Loss: 0.205791, Agg: 0.295958\n",
      "train [257/500][25000/26955] n_relations: 60, Loss: 0.204500, Agg: 0.287407\n",
      "train [257/500] Loss: 0.2843\n",
      "train [258/500][0/26955] n_relations: 60, Loss: 0.457625, Agg: 0.487340\n",
      "train [258/500][5000/26955] n_relations: 62, Loss: 0.179608, Agg: 0.251870\n",
      "train [258/500][10000/26955] n_relations: 60, Loss: 0.248799, Agg: 0.248631\n",
      "train [258/500][15000/26955] n_relations: 60, Loss: 0.729863, Agg: 0.246089\n",
      "train [258/500][20000/26955] n_relations: 62, Loss: 0.203126, Agg: 0.243379\n",
      "train [258/500][25000/26955] n_relations: 60, Loss: 0.187276, Agg: 0.242765\n",
      "train [258/500] Loss: 0.2431\n",
      "train [259/500][0/26955] n_relations: 60, Loss: 0.733710, Agg: 0.655404\n",
      "train [259/500][5000/26955] n_relations: 62, Loss: 0.239218, Agg: 0.242929\n",
      "train [259/500][10000/26955] n_relations: 60, Loss: 0.258534, Agg: 0.240864\n",
      "train [259/500][15000/26955] n_relations: 60, Loss: 0.544970, Agg: 0.238015\n",
      "train [259/500][20000/26955] n_relations: 62, Loss: 0.185418, Agg: 0.236590\n",
      "train [259/500][25000/26955] n_relations: 60, Loss: 0.192434, Agg: 0.234412\n",
      "train [259/500] Loss: 0.2327\n",
      "train [260/500][0/26955] n_relations: 60, Loss: 0.535247, Agg: 0.595551\n",
      "train [260/500][5000/26955] n_relations: 62, Loss: 0.190209, Agg: 0.225121\n",
      "train [260/500][10000/26955] n_relations: 60, Loss: 0.231678, Agg: 0.223663\n",
      "train [260/500][15000/26955] n_relations: 60, Loss: 0.648166, Agg: 0.225368\n",
      "train [260/500][20000/26955] n_relations: 62, Loss: 0.211942, Agg: 0.224500\n",
      "train [260/500][25000/26955] n_relations: 60, Loss: 0.135428, Agg: 0.224041\n",
      "train [260/500] Loss: 0.2239\n",
      "train [261/500][0/26955] n_relations: 60, Loss: 0.643045, Agg: 0.586181\n",
      "train [261/500][5000/26955] n_relations: 62, Loss: 0.171570, Agg: 0.259845\n",
      "train [261/500][10000/26955] n_relations: 60, Loss: 0.217452, Agg: 0.242117\n",
      "train [261/500][15000/26955] n_relations: 60, Loss: 0.502729, Agg: 0.232739\n",
      "train [261/500][20000/26955] n_relations: 62, Loss: 0.215746, Agg: 0.229207\n",
      "train [261/500][25000/26955] n_relations: 60, Loss: 0.145767, Agg: 0.228217\n",
      "train [261/500] Loss: 0.2280\n",
      "train [262/500][0/26955] n_relations: 60, Loss: 0.636447, Agg: 0.601863\n",
      "train [262/500][5000/26955] n_relations: 62, Loss: 0.188699, Agg: 0.233506\n",
      "train [262/500][10000/26955] n_relations: 60, Loss: 0.185944, Agg: 0.230659\n",
      "train [262/500][15000/26955] n_relations: 60, Loss: 0.494627, Agg: 0.230270\n",
      "train [262/500][20000/26955] n_relations: 62, Loss: 0.167237, Agg: 0.229721\n",
      "train [262/500][25000/26955] n_relations: 60, Loss: 0.177880, Agg: 0.227360\n",
      "train [262/500] Loss: 0.2277\n",
      "train [263/500][0/26955] n_relations: 60, Loss: 0.586227, Agg: 0.583851\n",
      "train [263/500][5000/26955] n_relations: 62, Loss: 0.186230, Agg: 0.225877\n",
      "train [263/500][10000/26955] n_relations: 60, Loss: 0.257678, Agg: 0.220516\n",
      "train [263/500][15000/26955] n_relations: 60, Loss: 0.558871, Agg: 0.218626\n",
      "train [263/500][20000/26955] n_relations: 62, Loss: 0.166360, Agg: 0.216466\n",
      "train [263/500][25000/26955] n_relations: 60, Loss: 0.140024, Agg: 0.215610\n",
      "train [263/500] Loss: 0.2153\n",
      "train [264/500][0/26955] n_relations: 60, Loss: 0.557080, Agg: 0.532360\n",
      "train [264/500][5000/26955] n_relations: 62, Loss: 0.194915, Agg: 0.240714\n",
      "train [264/500][10000/26955] n_relations: 60, Loss: 0.243546, Agg: 0.228003\n",
      "train [264/500][15000/26955] n_relations: 60, Loss: 0.485491, Agg: 0.222508\n",
      "train [264/500][20000/26955] n_relations: 62, Loss: 0.177868, Agg: 0.219663\n",
      "train [264/500][25000/26955] n_relations: 60, Loss: 0.191915, Agg: 0.222268\n",
      "train [264/500] Loss: 0.2233\n",
      "train [265/500][0/26955] n_relations: 60, Loss: 0.613269, Agg: 0.586167\n",
      "train [265/500][5000/26955] n_relations: 62, Loss: 0.183148, Agg: 0.218466\n",
      "train [265/500][10000/26955] n_relations: 60, Loss: 0.301018, Agg: 0.217540\n",
      "train [265/500][15000/26955] n_relations: 60, Loss: 0.524196, Agg: 0.225382\n",
      "train [265/500][20000/26955] n_relations: 62, Loss: 0.165031, Agg: 0.225446\n",
      "train [265/500][25000/26955] n_relations: 60, Loss: 0.161726, Agg: 0.225937\n",
      "train [265/500] Loss: 0.2255\n",
      "train [266/500][0/26955] n_relations: 60, Loss: 0.551805, Agg: 0.526487\n"
     ]
    }
   ],
   "source": [
    "## Multi-Step Prediction\n",
    "\n",
    "phase = 'train'\n",
    "instance_idx = [0, 31]\n",
    "args.n_epoch = 500\n",
    "n_particles = 31\n",
    "psteps = 0\n",
    "data_names = ['positions', 'velocities','hair_idx']\n",
    "\n",
    "for epoch in range(args.n_epoch):\n",
    "    model.train(phase=='train')\n",
    "    losses = 0.\n",
    "    for i in range(len(dataloaders[phase])):\n",
    "        idx_rollout = i // (args.time_step - 1)\n",
    "        idx_timestep = i % (args.time_step - 1)\n",
    "        if args.time_step - idx_timestep <= 3:\n",
    "            continue\n",
    "        data_path = os.path.join(args.dataf,'train', str(idx_rollout), str(idx_timestep) + '.h5')\n",
    "        data_nxt1_path = os.path.join(args.dataf,'train', str(idx_rollout), str(idx_timestep + 1) + '.h5')\n",
    "        data_nxt2_path = os.path.join(args.dataf,'train', str(idx_rollout), str(idx_timestep + 2) + '.h5')\n",
    "        data_nxt3_path = os.path.join(args.dataf,'train', str(idx_rollout), str(idx_timestep + 3) + '.h5')\n",
    "        Data = load_data(data_names, data_path)\n",
    "        data_nxt1 = normalize(load_data(data_names, data_nxt1_path), datasets['train'].stat)\n",
    "        data_nxt2 = normalize(load_data(data_names, data_nxt2_path), datasets['train'].stat)\n",
    "        data_nxt3 = normalize(load_data(data_names, data_nxt3_path), datasets['train'].stat)\n",
    "        label = torch.FloatTensor([data_nxt1[1][:n_particles],data_nxt2[1][:n_particles],data_nxt3[1][:n_particles]])\n",
    "        \n",
    "        p_pred = np.zeros((3, n_particles, args.position_dim))\n",
    "        label_pred = np.zeros((3, n_particles, args.position_dim))\n",
    "        for step in range(3):\n",
    "            p_pred[step] = Data[0][:n_particles]\n",
    "            attr, state, rels, n_particles, n_shapes = prepare_input(Data, datasets['train'].stat, datasets['train'].args, datasets['train'].phases_dict, 0)\n",
    "            Ra, node_r_idx, node_s_idx, pstep = rels[3], rels[4], rels[5], rels[6]\n",
    "        \n",
    "            Rr, Rs = [], []\n",
    "            Values = []\n",
    "            for j in range(len(rels[0])):\n",
    "                Rr_idx, Rs_idx, values = rels[0][j], rels[1][j], rels[2][j]\n",
    "                V = torch.ones(values.shape)\n",
    "                Values.append(values)\n",
    "                Rr.append(torch.sparse.FloatTensor(\n",
    "                    Rr_idx, V, torch.Size([node_r_idx[j].shape[0], Ra[j].size(0)])))\n",
    "                Rs.append(torch.sparse.FloatTensor(\n",
    "                    Rs_idx, V, torch.Size([node_s_idx[j].shape[0], Ra[j].size(0)])))\n",
    "\n",
    "            data = [attr, state, Rr, Rs, Ra,Values, label]\n",
    "\n",
    "\n",
    "                # st_time = time.time()\n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "                if use_gpu:\n",
    "                    instance_idx = torch.Tensor(instance_idx)#.cuda()\n",
    "                    for d in range(len(data)):\n",
    "                        if type(data[d]) == list:\n",
    "                            for t in range(len(data[d])):\n",
    "                                data[d][t] = Variable(data[d][t].cuda())\n",
    "                        else:\n",
    "                            data[d] = Variable(data[d].cuda())\n",
    "                else:\n",
    "                    for d in range(len(data)):\n",
    "                        if type(data[d]) == list:\n",
    "                            for t in range(len(data[d])):\n",
    "                                data[d][t] = Variable(data[d][t])\n",
    "                        else:\n",
    "                            data[d] = Variable(data[d])\n",
    "\n",
    "                attr, state, Rr, Rs, Ra, Values, label = data\n",
    "\n",
    "                pstep = 6\n",
    "\n",
    "                predicted = model(\n",
    "                    attr, state, Rr, Rs, Ra,Values, n_particles,\n",
    "                    node_r_idx, node_s_idx, pstep,\n",
    "                    instance_idx, phases_dict, 0)\n",
    "                \n",
    "            vels = denormalize([predicted.data.cpu().numpy()], [datasets['train'].stat[1]])[0]\n",
    "            Data[0][:n_particles] += vels * args.dt\n",
    "            Data[1][:n_particles] = vels\n",
    "            label_pred[step] = vels\n",
    "          #  vels = torch.Tensor(vels).cuda()\n",
    "            loss = criterionMSE(predicted, label[step])\n",
    "            losses += np.sqrt(loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        '''    \n",
    "        label_pred = torch.Tensor(label_pred).cuda()\n",
    "        loss = criterionMSE(label_pred, label)\n",
    "        losses += np.sqrt(loss.item())\n",
    "    #    print (label_pred.shape)\n",
    "        if phase == 'train':\n",
    "            if i % 5 == 0:\n",
    "                # update parameters every args.forward_times\n",
    "          #      print ('update!')\n",
    "                print (loss)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "          #      print ('done')\n",
    "        '''\n",
    "\n",
    "\n",
    "        if i % 5000 == 0:\n",
    "            n_relations = 0\n",
    "            for j in range(len(Ra)):\n",
    "                n_relations += Ra[j].size(0)\n",
    "            print('%s [%d/%d][%d/%d] n_relations: %d, Loss: %.6f, Agg: %.6f' %\n",
    "                  (phase, epoch+150, args.n_epoch, i, len(dataloaders[phase]),\n",
    "                   n_relations, np.sqrt(loss.item()), losses / (i + 1)/3))\n",
    "\n",
    "      #  if phase == 'train' and i > 0 and i % args.ckp_per_iter == 0:\n",
    "    if epoch % 10 == 0 :\n",
    "        torch.save(model.state_dict(), '%s/DPINet3_epoch_%d.pth' % (args.outf, epoch+150))\n",
    "\n",
    "    losses /= len(dataloaders[phase])*3\n",
    "    print('%s [%d/%d] Loss: %.4f' %\n",
    "          (phase, epoch+150, args.n_epoch, losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_path = stat_path = os.path.join(args.dataf, 'stat.h5')\n",
    "stat = load_data(data_names[:2], stat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network from dump_SingleHair/files_SingleHair/DPINet3_epoch_150.pth\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "model = DPINet2(args, stat, phases_dict, residual=True, use_gpu=use_gpu)\n",
    "model_file = os.path.join(args.outf, 'DPINet3_epoch_%d.pth' % (150))\n",
    "print(\"Loading network from %s\" % model_file)\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "model.eval()\n",
    "\n",
    "criterionMSE = nn.MSELoss()\n",
    "\n",
    "if use_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "\n",
    "\n",
    "for step in range(args.time_step - 1):\n",
    "    data_path = os.path.join(args.dataf, 'valid', str(idx), str(step) + '.h5')\n",
    "    data_nxt_path = os.path.join(args.dataf, 'valid', str(idx), str(step + 1) + '.h5')\n",
    "\n",
    "    data = load_data(data_names, data_path)\n",
    "    data_nxt = load_data(data_names, data_nxt_path)\n",
    "    velocities_nxt = data_nxt[1]\n",
    "\n",
    "    if step == 0:\n",
    "        positions, velocities, hairs_idx = data\n",
    "        n_shapes = 1\n",
    "        scene_params = 11\n",
    "        count_nodes = positions.shape[0]\n",
    "        n_particles = count_nodes - n_shapes\n",
    "        p_gt = np.zeros((args.time_step - 1, n_particles + n_shapes, args.position_dim))\n",
    "        s_gt = np.zeros((args.time_step - 1, n_shapes, args.shape_state_dim))\n",
    "        v_nxt_gt = np.zeros((args.time_step - 1, n_particles + n_shapes, args.position_dim))\n",
    "\n",
    "        p_pred = np.zeros((args.time_step - 1, n_particles + n_shapes, args.position_dim))\n",
    "\n",
    "    p_gt[step] = positions[:, -args.position_dim:]\n",
    "    v_nxt_gt[step] = velocities_nxt[:, -args.position_dim:]\n",
    "    \n",
    "    s_gt[step, :, :3] = positions[n_particles:, :3]\n",
    "    s_gt[step, :, 3:6] = p_gt[max(0, step-1), n_particles:, :3]\n",
    "    s_gt[step, :, 6:] = np.array( [[0.,0.70710677,0,0.70710677,0,0.70710677, 0, 0.70710677]])\n",
    "    positions = positions + velocities_nxt * args.dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass = np.zeros((n_particles, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "data_path = os.path.join(args.dataf, 'valid', str(idx), '0.h5')\n",
    "data = load_data(data_names, data_path)\n",
    "instance_idx = [0, 31]\n",
    "p_pred = np.zeros((args.time_step - 1, n_particles, args.position_dim))\n",
    "\n",
    "for step in range(args.time_step - 1):\n",
    "    p_pred[step] = data[0][:n_particles]\n",
    "    attr, state, rels, n_particles, n_shapes = prepare_input(data, stat, args, phases_dict, 0)\n",
    "    Ra, node_r_idx, node_s_idx, pstep = rels[3], rels[4], rels[5], rels[6]\n",
    "    \n",
    "    '''\n",
    "    Rr, Rs = [], []\n",
    "    for j in range(len(rels[0])):\n",
    "        Rr_idx, Rs_idx, values = rels[0][j], rels[1][j], rels[2][j]\n",
    "        Rr.append(torch.sparse.FloatTensor(\n",
    "            Rr_idx, values, torch.Size([node_r_idx[j].shape[0], Ra[j].size(0)])))\n",
    "        Rs.append(torch.sparse.FloatTensor(\n",
    "            Rs_idx, values, torch.Size([node_s_idx[j].shape[0], Ra[j].size(0)])))\n",
    "\n",
    "    buf = [attr, state, Rr, Rs, Ra]\n",
    "    '''\n",
    "    \n",
    "    Rr, Rs = [], []\n",
    "    Values = []\n",
    "\n",
    "    for j in range(len(rels[0])):\n",
    "        Rr_idx, Rs_idx, values = rels[0][j], rels[1][j], rels[2][j]\n",
    "        V = torch.ones(values.shape)\n",
    "        Values.append(values)\n",
    "        Rr.append(torch.sparse.FloatTensor(\n",
    "            Rr_idx, V, torch.Size([node_r_idx[j].shape[0], Ra[j].size(0)])))\n",
    "        Rs.append(torch.sparse.FloatTensor(\n",
    "            Rs_idx, V, torch.Size([node_s_idx[j].shape[0], Ra[j].size(0)])))\n",
    "    buf = [attr, state, Rr, Rs, Ra, Values]\n",
    "\n",
    "\n",
    "\n",
    "        # st_time = time.time()\n",
    "    with torch.set_grad_enabled(phase=='train'):\n",
    "        if use_gpu:\n",
    "            for d in range(len(buf)):\n",
    "                if type(buf[d]) == list:\n",
    "                    for t in range(len(buf[d])):\n",
    "                        buf[d][t] = Variable(buf[d][t].cuda())\n",
    "                else:\n",
    "                    buf[d] = Variable(buf[d].cuda())\n",
    "        else:\n",
    "            for d in range(len(buf)):\n",
    "                if type(buf[d]) == list:\n",
    "                    for t in range(len(buf[d])):\n",
    "                        buf[d][t] = Variable(buf[d][t])\n",
    "                else:\n",
    "                    buf[d] = Variable(buf[d])\n",
    "\n",
    "        attr, state, Rr, Rs, Ra, Values = buf\n",
    "\n",
    "        pstep = 3\n",
    "\n",
    "        predicted = model(\n",
    "            attr, state, Rr, Rs, Ra,Values, n_particles,\n",
    "            node_r_idx, node_s_idx, pstep,\n",
    "            instance_idx, phases_dict, 0)\n",
    "        \n",
    "        vels = denormalize([predicted.data.cpu().numpy()], [stat[1]])[0]\n",
    "        data[0][:n_particles] += vels * args.dt\n",
    "        data[1][:n_particles] = vels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyflex\n",
    "pyflex.init()\n",
    "\n",
    "cap_size = [0.1,1.5]\n",
    "N_hairs = 1\n",
    "env_idx = 12\n",
    "scene_params = np.array(cap_size)\n",
    "pyflex.set_scene(env_idx, scene_params, 0)\n",
    "\n",
    "for step in range(args.time_step - 1):\n",
    "    pyflex.set_shape_states(s_gt[step])\n",
    "\n",
    "    mass = np.zeros((n_particles, 1))\n",
    "    p = np.concatenate([p_gt[step, :n_particles], mass], 1)\n",
    "\n",
    "    pyflex.set_positions(p)\n",
    "    pyflex.render()\n",
    "    \n",
    "for step in range(args.time_step - 1):\n",
    "    pyflex.set_shape_states(s_gt[step])\n",
    "\n",
    "    mass = np.zeros((n_particles, 1))\n",
    "    p = np.concatenate([p_pred[step, :n_particles], mass], 1)\n",
    "\n",
    "    pyflex.set_positions(p)\n",
    "    pyflex.render()\n",
    "    \n",
    "pyflex.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.94671471e-02, -1.70888901e-02,  2.86665687e-04],\n",
       "       [-7.72059895e-03, -3.52163315e-02,  5.53823105e-04],\n",
       "       [-8.26193020e-03, -6.74045086e-02,  7.94695516e-04],\n",
       "       [-5.89844584e-03, -8.18157196e-02,  1.01434581e-03],\n",
       "       [-2.44137719e-02, -1.56035900e-01,  1.35363863e-03],\n",
       "       [ 1.99027658e-02, -1.39880657e-01,  1.02593155e-03],\n",
       "       [ 2.20143199e-02, -1.39371872e-01,  7.10276014e-04],\n",
       "       [ 3.57583761e-02, -1.44644976e-01,  4.06099571e-04],\n",
       "       [ 6.19480908e-02, -1.43819809e-01,  2.29113677e-04],\n",
       "       [ 6.26351833e-02, -2.87568569e-01,  7.74490181e-05],\n",
       "       [ 6.99596107e-02, -2.97364473e-01, -2.98930972e-05],\n",
       "       [ 1.59377426e-01, -3.01493168e-01, -2.21755923e-04],\n",
       "       [ 2.71295443e-01, -2.96638966e-01, -6.03932567e-04],\n",
       "       [ 3.45316276e-01, -3.34534407e-01, -8.92355922e-04],\n",
       "       [ 3.66531968e-01, -3.33156586e-01, -9.14194781e-04],\n",
       "       [ 3.65721405e-01, -3.34820032e-01, -1.21022452e-03],\n",
       "       [ 3.32504570e-01, -2.67438650e-01, -1.38501636e-03],\n",
       "       [ 2.66377926e-01, -2.02408314e-01, -1.62127284e-03],\n",
       "       [ 3.47131729e-01, -2.26504803e-01, -1.61434352e-03],\n",
       "       [ 3.64933252e-01, -2.07675338e-01, -1.74116966e-03],\n",
       "       [ 3.76902997e-01, -1.74611330e-01, -1.66337567e-03],\n",
       "       [ 3.08959424e-01, -1.44541144e-01, -1.60309477e-03],\n",
       "       [ 1.02210641e-01, -4.90387678e-02, -1.49082928e-03],\n",
       "       [-8.16264153e-02,  8.91002417e-02, -9.69856046e-04],\n",
       "       [-2.06791878e-01,  6.31041527e-02, -2.00029230e-04],\n",
       "       [-2.88263202e-01, -4.50811386e-02,  1.54328300e-04],\n",
       "       [-3.81298542e-01, -1.64107919e-01, -7.38387462e-05],\n",
       "       [-4.18386459e-01, -1.22458935e-01, -8.94533005e-04],\n",
       "       [-3.74126494e-01, -1.49078965e-01, -4.95300163e-04],\n",
       "       [-3.99626493e-02, -7.84702301e-02, -1.89908885e-03],\n",
       "       [-2.32903957e-01, -2.48626709e-01,  5.40727749e-04]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 300\n",
    "p_pred[step]-p_gt[step][:n_particles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [0/500][0/26955] n_relations: 60, Loss: 1.509959, Agg: 1.509959\n",
      "train [0/500][10000/26955] n_relations: 60, Loss: 0.083814, Agg: 0.664806\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-16630e2c0626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mValues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_particles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mnode_r_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_s_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 instance_idx, phases_dict, 0)\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;31m# print('Time forward', time.time() - st_time)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/onepear/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/onepear/Documents/Experiments/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, attr, state, Rr, Rs, Ra, Values, n_particles, node_r_idx, node_s_idx, pstep, instance_idx, phases_dict, verbose)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# receiver_attr, sender_attr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mattr_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_r_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         \u001b[0mattr_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_s_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0mattr_r_rel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "phase = 'train'\n",
    "instance_idx = [0, 31]\n",
    "args.n_epoch = 500\n",
    "psteps = 0\n",
    "\n",
    "for epoch in range(args.n_epoch):\n",
    "    model.train(phase=='train')\n",
    "\n",
    "    losses = 0.\n",
    "    for i, data in enumerate(dataloaders[phase]):\n",
    "#         print ('i:',i)\n",
    "\n",
    "        attr, state, rels, n_particles, n_shapes, label = data\n",
    "        Ra, node_r_idx, node_s_idx, pstep = rels[3], rels[4], rels[5], rels[6]\n",
    "\n",
    "        Rr, Rs = [], []\n",
    "        Values = []\n",
    "        for j in range(len(rels[0])):\n",
    "            Rr_idx, Rs_idx, values = rels[0][j], rels[1][j], rels[2][j]\n",
    "            V = torch.ones(values.shape)\n",
    "            Values.append(values)\n",
    "            Rr.append(torch.sparse.FloatTensor(\n",
    "                Rr_idx, V, torch.Size([node_r_idx[j].shape[0], Ra[j].size(0)])))\n",
    "            Rs.append(torch.sparse.FloatTensor(\n",
    "                Rs_idx, V, torch.Size([node_s_idx[j].shape[0], Ra[j].size(0)])))\n",
    "\n",
    "        data = [attr, state, Rr, Rs, Ra,Values, label]\n",
    "        \n",
    "\n",
    "            # st_time = time.time()\n",
    "        with torch.set_grad_enabled(phase=='train'):\n",
    "            if use_gpu:\n",
    "                instance_idx = torch.Tensor(instance_idx)#.cuda()\n",
    "                for d in range(len(data)):\n",
    "                    if type(data[d]) == list:\n",
    "                        for t in range(len(data[d])):\n",
    "                            data[d][t] = Variable(data[d][t].cuda())\n",
    "                    else:\n",
    "                        data[d] = Variable(data[d].cuda())\n",
    "            else:\n",
    "                for d in range(len(data)):\n",
    "                    if type(data[d]) == list:\n",
    "                        for t in range(len(data[d])):\n",
    "                            data[d][t] = Variable(data[d][t])\n",
    "                    else:\n",
    "                        data[d] = Variable(data[d])\n",
    "\n",
    "            attr, state, Rr, Rs, Ra, Values, label = data\n",
    "            \n",
    "            pstep = 3\n",
    "\n",
    "            predicted = model(\n",
    "                attr, state, Rr, Rs, Ra,Values, n_particles,\n",
    "                node_r_idx, node_s_idx, pstep,\n",
    "                instance_idx, phases_dict, 0)\n",
    "            # print('Time forward', time.time() - st_time)\n",
    "\n",
    "       #     print(predicted.shape)\n",
    "       #     print(label.shape)\n",
    "\n",
    "        loss = criterionMSE(predicted, label)\n",
    "        losses += np.sqrt(loss.item())\n",
    "\n",
    "        if phase == 'train':\n",
    "            if i % 5 == 0:\n",
    "                # update parameters every args.forward_times\n",
    "          #      print ('update!')\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "          #      print ('done')\n",
    "\n",
    "\n",
    "        if i % 5000 == 0:\n",
    "            n_relations = 0\n",
    "            for j in range(len(Ra)):\n",
    "                n_relations += Ra[j].size(0)\n",
    "            print('%s [%d/%d][%d/%d] n_relations: %d, Loss: %.6f, Agg: %.6f' %\n",
    "                  (phase, epoch, args.n_epoch, i, len(dataloaders[phase]),\n",
    "                   n_relations, np.sqrt(loss.item()), losses / (i + 1)))\n",
    "\n",
    "      #  if phase == 'train' and i > 0 and i % args.ckp_per_iter == 0:\n",
    "    if epoch % 10 == 0 :\n",
    "        torch.save(model.state_dict(), '%s/IntNet_epoch_%d.pth' % (args.outf, epoch))\n",
    "\n",
    "    losses /= len(dataloaders[phase])\n",
    "    print('%s [%d/%d] Loss: %.4f' %\n",
    "          (phase, epoch, args.n_epoch, losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
