{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from models import DPINet\n",
    "#from data import PhysicsFleXDataset, collate_fn\n",
    "\n",
    "from utils import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--relation_dim'], dest='relation_dim', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--pstep', type=int, default=2)\n",
    "parser.add_argument('--n_rollout', type=int, default=0)\n",
    "parser.add_argument('--time_step', type=int, default=0)\n",
    "parser.add_argument('--time_step_clip', type=int, default=0)\n",
    "parser.add_argument('--dt', type=float, default=1./60.)\n",
    "parser.add_argument('--nf_relation', type=int, default=300)\n",
    "parser.add_argument('--nf_particle', type=int, default=200)\n",
    "parser.add_argument('--nf_effect', type=int, default=200)\n",
    "parser.add_argument('--env', default='')\n",
    "parser.add_argument('--train_valid_ratio', type=float, default=0.9)\n",
    "parser.add_argument('--outf', default='files')\n",
    "parser.add_argument('--dataf', default='data')\n",
    "parser.add_argument('--num_workers', type=int, default=10)\n",
    "parser.add_argument('--gen_data', type=int, default=0)\n",
    "parser.add_argument('--gen_stat', type=int, default=0)\n",
    "parser.add_argument('--log_per_iter', type=int, default=1000)\n",
    "parser.add_argument('--ckp_per_iter', type=int, default=10000)\n",
    "parser.add_argument('--eval', type=int, default=0)\n",
    "parser.add_argument('--verbose_data', type=int, default=1)\n",
    "parser.add_argument('--verbose_model', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--n_instance', type=int, default=0)\n",
    "parser.add_argument('--n_stages', type=int, default=0)\n",
    "parser.add_argument('--n_his', type=int, default=0)\n",
    "\n",
    "parser.add_argument('--n_epoch', type=int, default=1000)\n",
    "parser.add_argument('--beta1', type=float, default=0.9)\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--batch_size', type=int, default=1)\n",
    "parser.add_argument('--forward_times', type=int, default=2)\n",
    "\n",
    "parser.add_argument('--resume_epoch', type=int, default=0)\n",
    "parser.add_argument('--resume_iter', type=int, default=0)\n",
    "\n",
    "# shape state:\n",
    "# [x, y, z, x_last, y_last, z_last, quat(4), quat_last(4)]\n",
    "parser.add_argument('--shape_state_dim', type=int, default=14)\n",
    "\n",
    "# object attributes:\n",
    "parser.add_argument('--attr_dim', type=int, default=0)\n",
    "\n",
    "# object state:\n",
    "parser.add_argument('--state_dim', type=int, default=0)\n",
    "parser.add_argument('--position_dim', type=int, default=0)\n",
    "\n",
    "# relation attr:\n",
    "parser.add_argument('--relation_dim', type=int, default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"--env SingleHair --gen_data 1\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases_dict = dict()\n",
    "\n",
    "args.n_rollout = 50\n",
    "args.num_workers = 3\n",
    "args.gen_stat = 1\n",
    "\n",
    "# object states:\n",
    "# [x, y, z, xdot, ydot, zdot]\n",
    "args.state_dim = 6\n",
    "args.position_dim = 3\n",
    "\n",
    "# object attr:\n",
    "# [rigid]\n",
    "args.attr_dim = 1\n",
    "\n",
    "# relation attr:\n",
    "# [none]\n",
    "args.relation_dim = 1\n",
    "\n",
    "args.time_step = 500\n",
    "args.time_step_clip = 20\n",
    "args.n_instance = 1\n",
    "args.n_stages = 1\n",
    "\n",
    "args.neighbor_radius = 0.08\n",
    "\n",
    "phases_dict[\"instance_idx\"] = [0, 30]\n",
    "phases_dict[\"root_num\"] = [[]]\n",
    "phases_dict[\"instance\"] = ['solid']\n",
    "phases_dict[\"material\"] = ['solid']\n",
    "\n",
    "args.outf = 'dump_SingleHair/' + args.outf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/data_SingleHair\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.outf = args.outf + '_' + args.env\n",
    "args.dataf = 'data/' + args.dataf + '_' + args.env\n",
    "print (args.dataf)\n",
    "os.system('mkdir -p ' + args.outf)\n",
    "os.system('mkdir -p ' + args.dataf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_PyFleX(info):\n",
    "\n",
    "    env, root_num = info['env'], info['root_num']\n",
    "    thread_idx, data_dir, data_names = info['thread_idx'], info['data_dir'], info['data_names']\n",
    "    n_rollout, n_instance = info['n_rollout'], info['n_instance']\n",
    "    time_step, time_step_clip = info['time_step'], info['time_step_clip']\n",
    "    shape_state_dim, dt = info['shape_state_dim'], info['dt']\n",
    "\n",
    "    env_idx = info['env_idx'] # =11\n",
    "\n",
    "    np.random.seed(round(time.time() * 1000 + thread_idx) % 2**32)\n",
    "    \n",
    "    stats = [init_stat(3), init_stat(3)]\n",
    "\n",
    "    import pyflex\n",
    "    pyflex.init()\n",
    "\n",
    "    for i in range(n_rollout):\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"%d / %d\" % (i, n_rollout))\n",
    "\n",
    "        rollout_idx = thread_idx * n_rollout + i\n",
    "        rollout_dir = os.path.join(data_dir, str(rollout_idx))\n",
    "        os.system('mkdir -p ' + rollout_dir)\n",
    "        \n",
    "        # scene_params: [len(box) at dim x,len(box) at dim y,len(box) at dim z, num_hair per circle, num_circle]\n",
    "        cap_size = [0.1,1.5]\n",
    "        N_hairs = 1\n",
    "\n",
    "\n",
    "        scene_params = np.array(cap_size)\n",
    "\n",
    "        pyflex.set_scene(env_idx, scene_params, thread_idx)\n",
    "        n_particles = pyflex.get_n_particles()\n",
    "        n_shapes = 1\n",
    "        N_particles_per_hair = int(n_particles/N_hairs)\n",
    "        idx_begins = np.arange(N_hairs)*N_particles_per_hair\n",
    "        idx_hairs = [[i,i+N_particles_per_hair-1] for i in idx_begins]\n",
    "\n",
    "        positions = np.zeros((time_step, n_particles, 3), dtype=np.float32)\n",
    "        velocities = np.zeros((time_step, n_particles, 3), dtype=np.float32)\n",
    "        shape_position = np.zeros((time_step, n_shapes, 3), dtype=np.float32)\n",
    "\n",
    "        for j in range(time_step_clip):\n",
    "            p_clip = pyflex.get_positions().reshape(-1, 4)[:, :3]\n",
    "            pyflex.step()\n",
    "\n",
    "        for j in range(time_step):\n",
    "            positions[j] = pyflex.get_positions().reshape(-1, 4)[:, :3]\n",
    "            shape_position[j] = pyflex.get_shape_states()[:3].reshape(-1,3)\n",
    "            if j == 0:\n",
    "                velocities[j] = (positions[j] - p_clip) / dt\n",
    "            else:\n",
    "                velocities[j] = (positions[j] - positions[j - 1]) / dt\n",
    "\n",
    "            pyflex.step()\n",
    "            data = [positions[j], velocities[j], idx_hairs, shape_position[j]]\n",
    "            store_data(data_names, data, os.path.join(rollout_dir, str(j) + '.h5'))\n",
    "        \n",
    "        # change dtype for more accurate stat calculation\n",
    "        # only normalize positions and velocities\n",
    "        datas = [positions.astype(np.float64), velocities.astype(np.float64)]\n",
    "\n",
    "        for j in range(len(stats)): \n",
    "            # here j = 2, refers to positions and velocities\n",
    "            stat = init_stat(stats[j].shape[0]) \n",
    "            # stat= np.zeros((3,3))\n",
    "            stat[:, 0] = np.mean(datas[j], axis=(0, 1))[:]\n",
    "            stat[:, 1] = np.std(datas[j], axis=(0, 1))[:]\n",
    "            stat[:, 2] = datas[j].shape[0] * datas[j].shape[1] # time_step*n_particles\n",
    "            stats[j] = combine_stat(stats[j], stat)\n",
    "\n",
    "    pyflex.clean()\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instance_idx': [0, 30],\n",
       " 'root_num': [[]],\n",
       " 'instance': ['solid'],\n",
       " 'material': ['solid']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phases_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'train'\n",
    "verbose = 0\n",
    "data_dir = os.path.join(args.dataf, phase)\n",
    "stat_path = os.path.join(args.dataf, 'stat.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(data_dir):\n",
    "    os.system('mkdir -p ' + data_dir)\n",
    "\n",
    "#    self.data_names = ['positions', 'velocities', 'shape_quats', 'clusters', 'scene_params']\n",
    "data_names = ['positions', 'velocities','hair_idx', 'shape_position']\n",
    "\n",
    "ratio = args.train_valid_ratio\n",
    "if phase == 'train':\n",
    "    n_rollout = int(args.n_rollout * ratio)\n",
    "elif phase == 'valid':\n",
    "    n_rollout = args.n_rollout - int(args.n_rollout * ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data ... n_rollout=45, time_step=500\n",
      "0 / 150 / 150 / 15\n",
      "\n",
      "\n",
      "10 / 1510 / 15\n",
      "\n",
      "10 / 15\n",
      "Training data generated, warpping up stats ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if the data hasn't been generated, generate the data\n",
    "print(\"Generating data ... n_rollout=%d, time_step=%d\" % (n_rollout, args.time_step))\n",
    "\n",
    "infos = []\n",
    "for i in range(args.num_workers):\n",
    "    info = {\n",
    "        'env': args.env,\n",
    "        'root_num': phases_dict['root_num'],\n",
    "        'thread_idx': i,\n",
    "        'data_dir': data_dir,\n",
    "        'data_names': data_names,\n",
    "        'n_rollout': n_rollout // args.num_workers,\n",
    "        'n_instance': args.n_instance,\n",
    "        'time_step': args.time_step,\n",
    "        'time_step_clip': args.time_step_clip,\n",
    "        'dt': args.dt,\n",
    "        'shape_state_dim': args.shape_state_dim}\n",
    "\n",
    "    info['env_idx'] = 11\n",
    "    infos.append(info)\n",
    "\n",
    "cores = args.num_workers\n",
    "pool = mp.Pool(processes=cores)\n",
    "data = pool.map(gen_PyFleX, infos)\n",
    "\n",
    "print(\"Training data generated, warpping up stats ...\")\n",
    "\n",
    "if phase == 'train' and args.gen_stat:\n",
    "    # positions [x, y, z], velocities[xdot, ydot, zdot]\n",
    "    if args.env == 'RiceGrip':\n",
    "        stat = [init_stat(6), init_stat(6)]\n",
    "    else:\n",
    "        stat = [init_stat(3), init_stat(3)]\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(stat)):\n",
    "            stat[j] = combine_stat(stat[j], data[i][j])\n",
    "    store_data(data_names[:2], stat, stat_path)\n",
    "else:\n",
    "    print(\"Loading stat from %s ...\" % stat_path)\n",
    "    stat = load_data(data_names[:2], stat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stat = load_data(data_names[:2], stat_path)\n",
    "for i in range(len(stat)):\n",
    "    stat[i] = stat[i][-args.position_dim:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-1.57374079e-01,  3.40204598e-01,  6.97500000e+05],\n",
      "       [ 2.38205924e+00,  9.55687699e-01,  6.97500000e+05],\n",
      "       [ 1.85907086e-04,  8.82408743e-04,  6.97500000e+05]]), array([[ 4.19261717e-02,  4.89498493e-01,  6.97500000e+05],\n",
      "       [ 1.73621921e-02,  5.47750267e-01,  6.97500000e+05],\n",
      "       [-6.74591762e-05,  4.11798187e-03,  6.97500000e+05]])]\n"
     ]
    }
   ],
   "source": [
    "print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "\n",
    "idx_rollout = idx // (args.time_step - 1)\n",
    "idx_timestep = idx % (args.time_step - 1)\n",
    "\n",
    "data_path = os.path.join(data_dir, str(idx_rollout), str(idx_timestep) + '.h5')\n",
    "data_nxt_path = os.path.join(data_dir, str(idx_rollout), str(idx_timestep + 1) + '.h5')\n",
    "\n",
    "data = load_data(data_names, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.n_his = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_his = []\n",
    "for i in range(args.n_his):\n",
    "    path = os.path.join(data_dir, str(idx_rollout), str(max(1, idx_timestep - i - 1)) + '.h5')\n",
    "    data_his = load_data(data_names, path)\n",
    "    vel_his.append(data_his[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions, velocities, hairs_idx, shape_position = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  2. , -0.5]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 4.        , 0.        ],\n",
       "       [0.        , 3.885385  , 0.        ],\n",
       "       [0.        , 3.7718565 , 0.        ],\n",
       "       [0.        , 3.654622  , 0.        ],\n",
       "       [0.        , 3.5386329 , 0.        ],\n",
       "       [0.        , 3.4226508 , 0.        ],\n",
       "       [0.        , 3.3070297 , 0.        ],\n",
       "       [0.        , 3.191694  , 0.        ],\n",
       "       [0.        , 3.0766892 , 0.        ],\n",
       "       [0.        , 2.9620218 , 0.        ],\n",
       "       [0.        , 2.847705  , 0.        ],\n",
       "       [0.        , 2.7337463 , 0.        ],\n",
       "       [0.        , 2.6201515 , 0.        ],\n",
       "       [0.        , 2.5069234 , 0.        ],\n",
       "       [0.        , 2.3940625 , 0.        ],\n",
       "       [0.        , 2.2815673 , 0.        ],\n",
       "       [0.        , 2.1694343 , 0.        ],\n",
       "       [0.        , 2.0576594 , 0.        ],\n",
       "       [0.        , 1.9462372 , 0.        ],\n",
       "       [0.        , 1.8351617 , 0.        ],\n",
       "       [0.        , 1.7244266 , 0.        ],\n",
       "       [0.        , 1.6140257 , 0.        ],\n",
       "       [0.        , 1.5039533 , 0.        ],\n",
       "       [0.        , 1.3942037 , 0.        ],\n",
       "       [0.        , 1.2847717 , 0.        ],\n",
       "       [0.        , 1.1756531 , 0.        ],\n",
       "       [0.        , 1.0668429 , 0.        ],\n",
       "       [0.        , 0.9583417 , 0.        ],\n",
       "       [0.        , 0.85013264, 0.        ],\n",
       "       [0.        , 0.74226826, 0.        ],\n",
       "       [0.        , 0.63454425, 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11461496, 0.11352849, 0.11723447, 0.11598921, 0.11598206,\n",
       "       0.11562109, 0.1153357 , 0.11500478, 0.11466742, 0.11431694,\n",
       "       0.1139586 , 0.11359477, 0.11322808, 0.11286092, 0.11249518,\n",
       "       0.11213303, 0.11177492, 0.11142218, 0.11107552, 0.11073506,\n",
       "       0.11040092, 0.11007237, 0.10974967, 0.10943198, 0.10911858,\n",
       "       0.10881019, 0.1085012 , 0.10820907, 0.10786438, 0.10772401],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(positions[1:]-positions[:-1],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  2. , -0.5]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ,  2.        ],\n",
       "       [-1.        ,  1.885385  ],\n",
       "       [-1.        ,  1.7718565 ],\n",
       "       [-1.        ,  1.6546221 ],\n",
       "       [-1.        ,  1.5386329 ],\n",
       "       [-1.        ,  1.4226508 ],\n",
       "       [-1.        ,  1.3070297 ],\n",
       "       [-1.        ,  1.191694  ],\n",
       "       [-1.        ,  1.0766892 ],\n",
       "       [-1.        ,  0.9620218 ],\n",
       "       [-1.        ,  0.8477049 ],\n",
       "       [-1.        ,  0.7337463 ],\n",
       "       [-1.        ,  0.6201515 ],\n",
       "       [-1.        ,  0.50692344],\n",
       "       [-1.        ,  0.39406252],\n",
       "       [-1.        ,  0.28156734],\n",
       "       [-1.        ,  0.16943431],\n",
       "       [-1.        ,  0.05765939],\n",
       "       [-1.        , -0.05376279],\n",
       "       [-1.        , -0.16483831],\n",
       "       [-1.        , -0.27557337],\n",
       "       [-1.        , -0.3859743 ],\n",
       "       [-1.        , -0.49604666],\n",
       "       [-1.        , -0.60579634],\n",
       "       [-1.        , -0.7152283 ],\n",
       "       [-1.        , -0.8243469 ],\n",
       "       [-1.        , -0.9331571 ],\n",
       "       [-1.        , -1.0416583 ],\n",
       "       [-1.        , -1.1498673 ],\n",
       "       [-1.        , -1.2577317 ],\n",
       "       [-1.        , -1.3654557 ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions[:,:2]- shape_position[0,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.236068 , 2.1341689, 2.0345702, 1.9333324, 1.8350452, 1.7389466,\n",
       "       1.6456995, 1.5556781, 1.469442 , 1.3876188, 1.3109553, 1.2403159,\n",
       "       1.1766851, 1.1211474, 1.074842 , 1.038884 , 1.0142524, 1.001661 ,\n",
       "       1.0014442, 1.0134947, 1.0372757, 1.071903 , 1.1162716, 1.1691831,\n",
       "       1.2294518, 1.2959737, 1.3677654, 1.4439709, 1.5238749, 1.6068257,\n",
       "       1.6924744], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis = np.linalg.norm(positions[:,:2]- shape_position[0,:2],axis = 1)\n",
    "dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = 0.1\n",
    "nodes = np.nonzero(dis < 0.1)[0]\n",
    "#return a list of index of nodes with dis less than 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "       20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(dis < 2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 3., 32.,  1.],\n",
       "        [ 4., 32.,  1.],\n",
       "        [ 5., 32.,  1.],\n",
       "        [ 6., 32.,  1.],\n",
       "        [ 7., 32.,  1.],\n",
       "        [ 8., 32.,  1.],\n",
       "        [ 9., 32.,  1.],\n",
       "        [10., 32.,  1.],\n",
       "        [11., 32.,  1.],\n",
       "        [12., 32.,  1.],\n",
       "        [13., 32.,  1.],\n",
       "        [14., 32.,  1.],\n",
       "        [15., 32.,  1.],\n",
       "        [16., 32.,  1.],\n",
       "        [17., 32.,  1.],\n",
       "        [18., 32.,  1.],\n",
       "        [19., 32.,  1.],\n",
       "        [20., 32.,  1.],\n",
       "        [21., 32.,  1.],\n",
       "        [22., 32.,  1.],\n",
       "        [23., 32.,  1.],\n",
       "        [24., 32.,  1.],\n",
       "        [25., 32.,  1.],\n",
       "        [26., 32.,  1.],\n",
       "        [27., 32.,  1.],\n",
       "        [28., 32.,  1.],\n",
       "        [29., 32.,  1.],\n",
       "        [30., 32.,  1.]])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nod = np.nonzero(dis < 2)[0]\n",
    "gripper = np.ones(nod.shape[0], dtype=np.int) * (31 + 1)\n",
    "#gripper: a list of 32, 32: the index of the shape, n_particles+i_shape\n",
    "[np.stack([nod, gripper, np.ones(nod.shape[0])], axis=1)]\n",
    "# a list of relations, [idx_node, idx_gripper, type_rel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.],\n",
       "       [ 1.,  2.,  0.],\n",
       "       [ 2.,  3.,  0.],\n",
       "       [ 3.,  4.,  0.],\n",
       "       [ 4.,  5.,  0.],\n",
       "       [ 5.,  6.,  0.],\n",
       "       [ 6.,  7.,  0.],\n",
       "       [ 7.,  8.,  0.],\n",
       "       [ 8.,  9.,  0.],\n",
       "       [ 9., 10.,  0.],\n",
       "       [10., 11.,  0.],\n",
       "       [11., 12.,  0.],\n",
       "       [12., 13.,  0.],\n",
       "       [13., 14.,  0.],\n",
       "       [14., 15.,  0.],\n",
       "       [15., 16.,  0.],\n",
       "       [16., 17.,  0.],\n",
       "       [17., 18.,  0.],\n",
       "       [18., 19.,  0.],\n",
       "       [19., 20.,  0.],\n",
       "       [20., 21.,  0.],\n",
       "       [21., 22.,  0.],\n",
       "       [22., 23.,  0.],\n",
       "       [23., 24.,  0.],\n",
       "       [24., 25.,  0.],\n",
       "       [25., 26.,  0.],\n",
       "       [26., 27.,  0.],\n",
       "       [27., 28.,  0.],\n",
       "       [28., 29.,  0.],\n",
       "       [29., 30.,  0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_nodes = 31\n",
    "nodes_p = np.arange(n_nodes-1)\n",
    "R1 = np.stack([nodes_p,nodes_p+1, np.zeros(n_nodes-1)],axis = 1)\n",
    "R2 = np.stack([nodes_p+1,nodes_p, np.zeros(n_nodes-1)],axis = 1)\n",
    "R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.],\n",
       "       [ 1.,  2.,  0.],\n",
       "       [ 2.,  3.,  0.],\n",
       "       [ 3.,  4.,  0.],\n",
       "       [ 4.,  5.,  0.],\n",
       "       [ 5.,  6.,  0.],\n",
       "       [ 6.,  7.,  0.],\n",
       "       [ 7.,  8.,  0.],\n",
       "       [ 8.,  9.,  0.],\n",
       "       [ 9., 10.,  0.],\n",
       "       [10., 11.,  0.],\n",
       "       [11., 12.,  0.],\n",
       "       [12., 13.,  0.],\n",
       "       [13., 14.,  0.],\n",
       "       [14., 15.,  0.],\n",
       "       [15., 16.,  0.],\n",
       "       [16., 17.,  0.],\n",
       "       [17., 18.,  0.],\n",
       "       [18., 19.,  0.],\n",
       "       [19., 20.,  0.],\n",
       "       [20., 21.,  0.],\n",
       "       [21., 22.,  0.],\n",
       "       [22., 23.,  0.],\n",
       "       [23., 24.,  0.],\n",
       "       [24., 25.,  0.],\n",
       "       [25., 26.,  0.],\n",
       "       [26., 27.,  0.],\n",
       "       [27., 28.,  0.],\n",
       "       [28., 29.,  0.],\n",
       "       [29., 30.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 2.,  1.,  0.],\n",
       "       [ 3.,  2.,  0.],\n",
       "       [ 4.,  3.,  0.],\n",
       "       [ 5.,  4.,  0.],\n",
       "       [ 6.,  5.,  0.],\n",
       "       [ 7.,  6.,  0.],\n",
       "       [ 8.,  7.,  0.],\n",
       "       [ 9.,  8.,  0.],\n",
       "       [10.,  9.,  0.],\n",
       "       [11., 10.,  0.],\n",
       "       [12., 11.,  0.],\n",
       "       [13., 12.,  0.],\n",
       "       [14., 13.,  0.],\n",
       "       [15., 14.,  0.],\n",
       "       [16., 15.,  0.],\n",
       "       [17., 16.,  0.],\n",
       "       [18., 17.,  0.],\n",
       "       [19., 18.,  0.],\n",
       "       [20., 19.,  0.],\n",
       "       [21., 20.,  0.],\n",
       "       [22., 21.,  0.],\n",
       "       [23., 22.,  0.],\n",
       "       [24., 23.,  0.],\n",
       "       [25., 24.,  0.],\n",
       "       [26., 25.,  0.],\n",
       "       [27., 26.,  0.],\n",
       "       [28., 27.,  0.],\n",
       "       [29., 28.,  0.],\n",
       "       [30., 29.,  0.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([R1,R2],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "info = {\n",
    "    'env': args.env,\n",
    "    'root_num': phases_dict['root_num'],\n",
    "    'thread_idx': 0,\n",
    "    'data_dir': data_dir,\n",
    "    'data_names': data_names,\n",
    "    'n_rollout': n_rollout // args.num_workers,\n",
    "    'n_instance': args.n_instance,\n",
    "    'time_step': args.time_step,\n",
    "    'time_step_clip': args.time_step_clip,\n",
    "    'dt': args.dt,\n",
    "    'shape_state_dim': args.shape_state_dim}\n",
    "\n",
    "info['env_idx'] = 11\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = gen_PyFleX(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, datas = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions, velocities, hair_idx, shape_position = datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(data, stat, args, phases_dict, verbose=0, var=False):\n",
    "    '''\n",
    "    for a single hair\n",
    "    '''\n",
    "    positions, velocities, hairs_idx, shape_position = data\n",
    "    n_shapes = 1\n",
    "    hairs_idx_begin = [idx[0] for idx in hairs_idx]\n",
    "    n_particles = positions.shape[0]\n",
    "    R = 0.1\n",
    "    R = 2\n",
    "    \n",
    "    ### object attributes\n",
    "    #   dim 10: [rigid, fluid, root_0, root_1, gripper_0, gripper_1, mass_inv,\n",
    "    #            clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep]\n",
    "    #   here we only consider the hairs but not the gripper, attr_dim = 1, attr = 0 for hair, attr = 1 for shapes\n",
    "    attr = np.zeros((n_particles+n_shapes, args.attr_dim))\n",
    "    \n",
    "    ### construct relations\n",
    "    Rr_idxs = []        # relation receiver idx list\n",
    "    Rs_idxs = []        # relation sender idx list\n",
    "    Ras = []            # relation attributes list\n",
    "    values = []         # relation value list (should be 1)\n",
    "    node_r_idxs = []    # list of corresponding receiver node idx\n",
    "    node_s_idxs = []    # list of corresponding sender node idx\n",
    "    psteps = []         # propagation steps\n",
    "    \n",
    "    ##### add env specific graph components\n",
    "    ### specify for shapes\n",
    "    rels = []\n",
    "    vals = []\n",
    "    \n",
    "    for i in range(n_shapes):\n",
    "        attr[n_particles+i, 0] = 1\n",
    "        dis = np.linalg.norm(positions[:,:2]- shape_position[0,:2],axis = 1)\n",
    "        nodes_rel = np.nonzero(dis <= R)[0]\n",
    "        # for relation between hair nodes and a gripper, we note it as 1\n",
    "        gripper = np.ones(nodes_rel.shape[0], dtype=np.int) * (n_particles+i)\n",
    "        rels += [np.stack([nodes_rel, gripper, np.ones(nodes_rel.shape[0])], axis=1)]\n",
    "        vals += [np.ones(nodes_rel.shape[0], dtype=np.int)]\n",
    "        \n",
    "    \n",
    "    ##### add relations between leaf particles\n",
    "    ## here we only consider the relations in a hair: the relation between a node and the nodes nearby\n",
    "    ## simple case for one hair, TEMPORARY 2 rels for one link\n",
    "    nodes_p = np.arange(n_particles-1)\n",
    "    val = np.linalg.norm(positions[1:]-positions[:-1],axis = 1)\n",
    "    R1 = np.stack([nodes_p,nodes_p+1, np.zeros(n_particles-1)],axis = 1)\n",
    "    R2 = np.stack([nodes_p+1,nodes_p, np.zeros(n_particles-1)],axis = 1)\n",
    "    rels += [np.concatenate([R1,R2],axis = 0)]\n",
    "    vals += [val,val]\n",
    "    \n",
    "    rels = np.concatenate(rels, 0)\n",
    "    vals = np.concatenate(vals, 0)\n",
    "    \n",
    "  #  print (vals.shape)\n",
    " #   print (rels.shape)\n",
    "    \n",
    "    \n",
    "    if rels.shape[0] > 0:\n",
    "        if verbose:\n",
    "            print(\"Relations neighbor\", rels.shape)\n",
    "        Rr_idxs.append(torch.LongTensor([rels[:, 0], np.arange(rels.shape[0])]))\n",
    "        Rs_idxs.append(torch.LongTensor([rels[:, 1], np.arange(rels.shape[0])]))\n",
    "        # Ra: relation attributes\n",
    "    #    Ra = np.zeros((rels.shape[0], args.relation_dim))  \n",
    "        Ra = rels[:,2]\n",
    "        Ras.append(torch.FloatTensor(Ra))\n",
    "        # values could be changed\n",
    "     #   values.append(torch.FloatTensor([1] * rels.shape[0]))\n",
    "      #  values.append(rels[:,2])\n",
    "        #### for hairs: values equals to the length of this segment\n",
    "        values.append(vals)\n",
    "        node_r_idxs.append(np.arange(n_particles))\n",
    "        node_s_idxs.append(np.arange(n_particles + n_shapes))\n",
    "        psteps.append(args.pstep)\n",
    "        \n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Attr shape (after hierarchy building):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "        print(\"Particle attr:\", np.sum(attr[:n_particles], axis=0))\n",
    "        print(\"Shape attr:\", np.sum(attr[n_particles:n_particles+n_shapes], axis=0))\n",
    "        print(\"Roots attr:\", np.sum(attr[n_particles+n_shapes:], axis=0))\n",
    "        \n",
    "        \n",
    "    ### normalize data\n",
    "    data = [positions, velocities]\n",
    "    data = normalize(data, stat, var)\n",
    "    positions, velocities = data[0], data[1]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Particle positions stats\")\n",
    "        print(positions.shape)\n",
    "        print(np.min(positions[:n_particles], 0))\n",
    "        print(np.max(positions[:n_particles], 0))\n",
    "        print(np.mean(positions[:n_particles], 0))\n",
    "        print(np.std(positions[:n_particles], 0))\n",
    "\n",
    "        show_vel_dim = 6 if args.env == 'RiceGrip' else 3\n",
    "        print(\"Velocities stats\")\n",
    "        print(velocities.shape)\n",
    "        print(np.mean(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        print(np.std(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        \n",
    "    state = torch.FloatTensor(np.concatenate([positions, velocities], axis=1))\n",
    "    attr = torch.FloatTensor(attr)\n",
    "    relations = [Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps]\n",
    "\n",
    "    return attr, state, relations, n_particles, n_shapes#, instance_idx\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr, state, relations, n_particles, n_shapes = prepare_input(data, stat, args, phases_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps = relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "          31, 31, 31, 31, 31, 31, 31, 31, 31, 31,  1,  2,  3,  4,  5,  6,  7,  8,\n",
       "           9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "          27, 28, 29, 30,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "          14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "         [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "          36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "          54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
       "          72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]])]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rs_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.11461496, 0.11352849,\n",
       "        0.11723447, 0.11598921, 0.11598206, 0.11562109, 0.1153357 ,\n",
       "        0.11500478, 0.11466742, 0.11431694, 0.1139586 , 0.11359477,\n",
       "        0.11322808, 0.11286092, 0.11249518, 0.11213303, 0.11177492,\n",
       "        0.11142218, 0.11107552, 0.11073506, 0.11040092, 0.11007237,\n",
       "        0.10974967, 0.10943198, 0.10911858, 0.10881019, 0.1085012 ,\n",
       "        0.10820907, 0.10786438, 0.10772401, 0.11461496, 0.11352849,\n",
       "        0.11723447, 0.11598921, 0.11598206, 0.11562109, 0.1153357 ,\n",
       "        0.11500478, 0.11466742, 0.11431694, 0.1139586 , 0.11359477,\n",
       "        0.11322808, 0.11286092, 0.11249518, 0.11213303, 0.11177492,\n",
       "        0.11142218, 0.11107552, 0.11073506, 0.11040092, 0.11007237,\n",
       "        0.10974967, 0.10943198, 0.10911858, 0.10881019, 0.1085012 ,\n",
       "        0.10820907, 0.10786438, 0.10772401])]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(data, stat, args, phases_dict, verbose=0, var=False):\n",
    "\n",
    "    # Arrangement:\n",
    "    # particles, shapes, roots\n",
    "\n",
    "    if args.env == 'RiceGrip':\n",
    "        positions, velocities, shape_quats, clusters, scene_params = data\n",
    "        n_shapes = shape_quats.size(0) if var else shape_quats.shape[0]\n",
    "    elif args.env == 'FluidShake':\n",
    "        positions, velocities, shape_quats, scene_params = data\n",
    "        n_shapes = shape_quats.size(0) if var else shape_quats.shape[0]\n",
    "        clusters = None\n",
    "    elif args.env == 'BoxBath':\n",
    "        positions, velocities, clusters = data\n",
    "        n_shapes = 0\n",
    "    elif args.env == 'FluidFall':\n",
    "        positions, velocities = data\n",
    "        n_shapes = 0\n",
    "        clusters = None\n",
    "    elif args.env == 'Hairs':\n",
    "        positions, velocities, hairs_idx = data\n",
    "        n_shapes = 1\n",
    "        hairs_idx_begin = [idx[0] for idx in hairs_idx]\n",
    "        clusters = None\n",
    "\n",
    "    count_nodes = positions.size(0) if var else positions.shape[0]\n",
    "    n_particles = count_nodes - n_shapes\n",
    "\n",
    "    if verbose:\n",
    "        print(\"positions\", positions.shape)\n",
    "        print(\"velocities\", velocities.shape)\n",
    "\n",
    "        print(\"n_particles\", n_particles)\n",
    "        print(\"n_shapes\", n_shapes)\n",
    "        if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            print(\"shape_quats\", shape_quats.shape)\n",
    "\n",
    "    ### instance idx\n",
    "    #   instance_idx (n_instance + 1): start idx of instance\n",
    "    if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "        instance_idx = [0, n_particles]\n",
    "    elif args.env == 'Hairs':\n",
    "        instance_idx = [0, count_nodes]\n",
    "    else:\n",
    "        instance_idx = phases_dict[\"instance_idx\"]\n",
    "    if verbose:\n",
    "        print(\"Instance_idx:\", instance_idx)\n",
    "\n",
    "\n",
    "    ### object attributes\n",
    "    #   dim 10: [rigid, fluid, root_0, root_1, gripper_0, gripper_1, mass_inv,\n",
    "    #            clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep]\n",
    "    attr = np.zeros((count_nodes, args.attr_dim))\n",
    "    # no need to include mass for now\n",
    "    # attr[:, 6] = positions[:, -1].data.cpu().numpy() if var else positions[:, -1] # mass_inv\n",
    "    if args.env == 'RiceGrip':\n",
    "        # clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep\n",
    "        attr[:, -3:] = scene_params[-3:]\n",
    "\n",
    "\n",
    "    ### construct relations\n",
    "    Rr_idxs = []        # relation receiver idx list\n",
    "    Rs_idxs = []        # relation sender idx list\n",
    "    Ras = []            # relation attributes list\n",
    "    values = []         # relation value list (should be 1)\n",
    "    node_r_idxs = []    # list of corresponding receiver node idx\n",
    "    node_s_idxs = []    # list of corresponding sender node idx\n",
    "    psteps = []         # propagation steps\n",
    "\n",
    "    ##### add env specific graph components\n",
    "    rels = []\n",
    "    if args.env == 'RiceGrip':\n",
    "        # nodes = np.arange(n_particles)\n",
    "        for i in range(n_shapes):\n",
    "            attr[n_particles + i, 2 + i] = 1\n",
    "\n",
    "            pos = positions.data.cpu().numpy() if var else positions\n",
    "            dis = np.linalg.norm(\n",
    "                pos[:n_particles, 3:6:2] - pos[n_particles + i, 3:6:2], axis=1)\n",
    "            nodes = np.nonzero(dis < 0.3)[0]\n",
    "\n",
    "            if verbose:\n",
    "                visualize_neighbors(positions, positions, 0, nodes)\n",
    "                print(np.sort(dis)[:10])\n",
    "\n",
    "            gripper = np.ones(nodes.shape[0], dtype=np.int) * (n_particles + i)\n",
    "            rels += [np.stack([nodes, gripper, np.ones(nodes.shape[0])], axis=1)]\n",
    "            \n",
    "    elif args.env == 'Hairs':\n",
    "        # TODO: add relations between the hairs and the stick\n",
    "        attr[:,0] = 1\n",
    "        pass\n",
    "\n",
    "    elif args.env == 'FluidShake':\n",
    "        for i in range(n_shapes):\n",
    "            attr[n_particles + i, 1 + i] = 1\n",
    "\n",
    "            pos = positions.data.cpu().numpy() if var else positions\n",
    "            if i == 0:\n",
    "                # floor\n",
    "                dis = pos[:n_particles, 1] - pos[n_particles + i, 1]\n",
    "            elif i == 1:\n",
    "                # left\n",
    "                dis = pos[:n_particles, 0] - pos[n_particles + i, 0]\n",
    "            elif i == 2:\n",
    "                # right\n",
    "                dis = pos[n_particles + i, 0] - pos[:n_particles, 0]\n",
    "            elif i == 3:\n",
    "                # back\n",
    "                dis = pos[:n_particles, 2] - pos[n_particles + i, 2]\n",
    "            elif i == 4:\n",
    "                # front\n",
    "                dis = pos[n_particles + i, 2] - pos[:n_particles, 2]\n",
    "            else:\n",
    "                raise AssertionError(\"more shapes than expected\")\n",
    "            nodes = np.nonzero(dis < 0.1)[0]\n",
    "\n",
    "            if verbose:\n",
    "                visualize_neighbors(positions, positions, 0, nodes)\n",
    "                print(np.sort(dis)[:10])\n",
    "\n",
    "            wall = np.ones(nodes.shape[0], dtype=np.int) * (n_particles + i)\n",
    "            rels += [np.stack([nodes, wall, np.ones(nodes.shape[0])], axis=1)]\n",
    "\n",
    "    if verbose and len(rels) > 0:\n",
    "        print(np.concatenate(rels, 0).shape)\n",
    "\n",
    "    ##### add relations between leaf particles\n",
    "    for i in range(len(instance_idx) - 1):\n",
    "        st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "\n",
    "        if verbose:\n",
    "            print('instance #%d' % i, st, ed)\n",
    "\n",
    "        if args.env == 'BoxBath':\n",
    "            if phases_dict['material'][i] == 'rigid':\n",
    "                attr[st:ed, 0] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.concatenate((np.arange(st), np.arange(ed, n_particles)))\n",
    "            elif phases_dict['material'][i] == 'fluid':\n",
    "                attr[st:ed, 1] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.arange(n_particles)\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported materials\")\n",
    "                \n",
    "        elif args.env == 'Hairs':\n",
    "            # TODO: add relations between the hairs and the stick\n",
    "            pass\n",
    "            \n",
    "          #  if ed not in hairs_idx_begin:\n",
    "             #   queries = np.arange(st, ed)\n",
    "              #  anchors = np.arange(n_particles)\n",
    "\n",
    "        elif args.env == 'FluidFall' or args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            if phases_dict['material'][i] == 'fluid':\n",
    "                attr[st:ed, 0] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.arange(n_particles)\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported materials\")\n",
    "\n",
    "        else:\n",
    "            raise AssertionError(\"Unsupported materials\")\n",
    "\n",
    "        # st_time = time.time()\n",
    "        pos = positions\n",
    "        pos = pos[:, -3:]\n",
    "        if args.env == 'Hairs':\n",
    "            #TODO\n",
    "            pass\n",
    "        else:\n",
    "            rels += find_relations_neighbor(pos, queries, anchors, args.neighbor_radius, 2, var)\n",
    "            # return list of [receiver, sender, relation_type]\n",
    "        # print(\"Time on neighbor search\", time.time() - st_time)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Attr shape (after add env specific graph components):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "\n",
    "    rels = np.concatenate(rels, 0)\n",
    "    if rels.shape[0] > 0:\n",
    "        if verbose:\n",
    "            print(\"Relations neighbor\", rels.shape)\n",
    "        Rr_idxs.append(torch.LongTensor([rels[:, 0], np.arange(rels.shape[0])]))\n",
    "        Rs_idxs.append(torch.LongTensor([rels[:, 1], np.arange(rels.shape[0])]))\n",
    "        Ra = np.zeros((rels.shape[0], args.relation_dim))\n",
    "        Ras.append(torch.FloatTensor(Ra))\n",
    "        values.append(torch.FloatTensor([1] * rels.shape[0]))\n",
    "        node_r_idxs.append(np.arange(n_particles))\n",
    "        node_s_idxs.append(np.arange(n_particles + n_shapes))\n",
    "        psteps.append(args.pstep)\n",
    "\n",
    "    if verbose:\n",
    "        print('clusters', clusters)\n",
    "\n",
    "    # add heirarchical relations per instance\n",
    "    cnt_clusters = 0\n",
    "    for i in range(len(instance_idx) - 1):\n",
    "        st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "        n_root_level = len(phases_dict[\"root_num\"][i])\n",
    "\n",
    "        if n_root_level > 0:\n",
    "            attr, positions, velocities, count_nodes, \\\n",
    "            rels, node_r_idx, node_s_idx, pstep = \\\n",
    "                    make_hierarchy(args.env, attr, positions, velocities, i, st, ed,\n",
    "                                   phases_dict, count_nodes, clusters[cnt_clusters], verbose, var)\n",
    "\n",
    "            for j in range(len(rels)):\n",
    "                if verbose:\n",
    "                    print(\"Relation instance\", j, rels[j].shape)\n",
    "                Rr_idxs.append(torch.LongTensor([rels[j][:, 0], np.arange(rels[j].shape[0])]))\n",
    "                Rs_idxs.append(torch.LongTensor([rels[j][:, 1], np.arange(rels[j].shape[0])]))\n",
    "                Ra = np.zeros((rels[j].shape[0], args.relation_dim)); Ra[:, 0] = 1\n",
    "                Ras.append(torch.FloatTensor(Ra))\n",
    "                values.append(torch.FloatTensor([1] * rels[j].shape[0]))\n",
    "                node_r_idxs.append(node_r_idx[j])\n",
    "                node_s_idxs.append(node_s_idx[j])\n",
    "                psteps.append(pstep[j])\n",
    "\n",
    "            cnt_clusters += 1\n",
    "\n",
    "    if verbose:\n",
    "        if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            print(\"Scene_params:\", scene_params)\n",
    "\n",
    "        print(\"Attr shape (after hierarchy building):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "        print(\"Particle attr:\", np.sum(attr[:n_particles], axis=0))\n",
    "        print(\"Shape attr:\", np.sum(attr[n_particles:n_particles+n_shapes], axis=0))\n",
    "        print(\"Roots attr:\", np.sum(attr[n_particles+n_shapes:], axis=0))\n",
    "\n",
    "    ### normalize data\n",
    "    data = [positions, velocities]\n",
    "    data = normalize(data, stat, var)\n",
    "    positions, velocities = data[0], data[1]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Particle positions stats\")\n",
    "        print(positions.shape)\n",
    "        print(np.min(positions[:n_particles], 0))\n",
    "        print(np.max(positions[:n_particles], 0))\n",
    "        print(np.mean(positions[:n_particles], 0))\n",
    "        print(np.std(positions[:n_particles], 0))\n",
    "\n",
    "        show_vel_dim = 6 if args.env == 'RiceGrip' else 3\n",
    "        print(\"Velocities stats\")\n",
    "        print(velocities.shape)\n",
    "        print(np.mean(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        print(np.std(velocities[:n_particles, :show_vel_dim], 0))\n",
    "\n",
    "    if args.env == 'RiceGrip':\n",
    "        if var:\n",
    "            quats = torch.cat(\n",
    "                [Variable(torch.zeros(n_particles, 4).cuda()), shape_quats,\n",
    "                 Variable(torch.zeros(count_nodes - n_particles - n_shapes, 4).cuda())], 0)\n",
    "            state = torch.cat([positions, velocities, quats], 1)\n",
    "        else:\n",
    "            quat_null = np.array([[0., 0., 0., 0.]])\n",
    "            quats = np.repeat(quat_null, [count_nodes], axis=0)\n",
    "            quats[n_particles:n_particles + n_shapes] = shape_quats\n",
    "            # if args.eval == 0:\n",
    "            # quats += np.random.randn(quats.shape[0], 4) * 0.05\n",
    "            state = torch.FloatTensor(np.concatenate([positions, velocities, quats], axis=1))\n",
    "    else:\n",
    "        if var:\n",
    "            state = torch.cat([positions, velocities], 1)\n",
    "        else:\n",
    "            state = torch.FloatTensor(np.concatenate([positions, velocities], axis=1))\n",
    "\n",
    "    if verbose:\n",
    "        for i in range(count_nodes - 1):\n",
    "            if np.sum(np.abs(attr[i] - attr[i + 1])) > 1e-6:\n",
    "                print(i, attr[i], attr[i + 1])\n",
    "\n",
    "        for i in range(len(Ras)):\n",
    "            print(i, np.min(node_r_idxs[i]), np.max(node_r_idxs[i]), np.min(node_s_idxs[i]), np.max(node_s_idxs[i]))\n",
    "\n",
    "    attr = torch.FloatTensor(attr)\n",
    "    relations = [Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps]\n",
    "\n",
    "    return attr, state, relations, n_particles, n_shapes, instance_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_PyFleX(info):\n",
    "\n",
    "    env, root_num = info['env'], info['root_num']\n",
    "    thread_idx, data_dir, data_names = info['thread_idx'], info['data_dir'], info['data_names']\n",
    "    n_rollout, n_instance = info['n_rollout'], info['n_instance']\n",
    "    time_step, time_step_clip = info['time_step'], info['time_step_clip']\n",
    "    shape_state_dim, dt = info['shape_state_dim'], info['dt']\n",
    "\n",
    "    env_idx = info['env_idx']\n",
    "\n",
    "    np.random.seed(round(time.time() * 1000 + thread_idx) % 2**32)\n",
    "    \n",
    "    stats = [init_stat(3), init_stat(3)]\n",
    "\n",
    "    import pyflex\n",
    "    pyflex.init()\n",
    "\n",
    "    for i in range(n_rollout):\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"%d / %d\" % (i, n_rollout))\n",
    "\n",
    "        rollout_idx = thread_idx * n_rollout + i\n",
    "        rollout_dir = os.path.join(data_dir, str(rollout_idx))\n",
    "        os.system('mkdir -p ' + rollout_dir)\n",
    "        \n",
    "        env_idx = 10\n",
    "        # scene_params: [len(box) at dim x,len(box) at dim y,len(box) at dim z, num_hair per circle, num_circle]\n",
    "        box_size = [0.1,0.1,1.5]\n",
    "        N_p_cirlcle = 50\n",
    "        N_circle = 20\n",
    "        N_hairs = N_circle*N_p_cirlcle\n",
    "\n",
    "\n",
    "        scene_params = np.array(box_size + [N_p_cirlcle] + [N_circle])\n",
    "\n",
    "        pyflex.set_scene(env_idx, scene_params, thread_idx)\n",
    "        n_particles = pyflex.get_n_particles()\n",
    "        N_particles_per_hair = int(n_particles/N_hairs)\n",
    "        idx_begins = np.arange(N_hairs)*N_particles_per_hair\n",
    "        idx_hairs = [[i,i+N_particles_per_hair-1] for i in idx_begins]\n",
    "\n",
    "        positions = np.zeros((time_step, n_particles, 3), dtype=np.float32)\n",
    "        velocities = np.zeros((time_step, n_particles, 3), dtype=np.float32)\n",
    "\n",
    "        for j in range(time_step_clip):\n",
    "            p_clip = pyflex.get_positions().reshape(-1, 4)[:, :3]\n",
    "            pyflex.step()\n",
    "\n",
    "        for j in range(time_step):\n",
    "            positions[j] = pyflex.get_positions().reshape(-1, 4)[:, :3]\n",
    "            if j == 0:\n",
    "                velocities[j] = (positions[j] - p_clip) / dt\n",
    "            else:\n",
    "                velocities[j] = (positions[j] - positions[j - 1]) / dt\n",
    "\n",
    "            pyflex.step()\n",
    "            data = [positions[j], velocities[j], idx_hairs]\n",
    "            store_data(data_names, data, os.path.join(rollout_dir, str(j) + '.h5'))\n",
    "        \n",
    "        # change dtype for more accurate stat calculation\n",
    "        # only normalize positions and velocities\n",
    "        datas = [positions.astype(np.float64), velocities.astype(np.float64)]\n",
    "\n",
    "        for j in range(len(stats)): \n",
    "            # here j = 2, refers to positions and velocities\n",
    "            stat = init_stat(stats[j].shape[0]) \n",
    "            # stat= np.zeros((3,3))\n",
    "            stat[:, 0] = np.mean(datas[j], axis=(0, 1))[:]\n",
    "            stat[:, 1] = np.std(datas[j], axis=(0, 1))[:]\n",
    "            stat[:, 2] = datas[j].shape[0] * datas[j].shape[1] # time_step*n_particles\n",
    "            stats[j] = combine_stat(stats[j], stat)\n",
    "\n",
    "    pyflex.clean()\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsFleXDataset(Dataset):\n",
    "\n",
    "    def __init__(self, args, phase, phases_dict, verbose):\n",
    "        self.args = args\n",
    "        self.phase = phase\n",
    "        self.phases_dict = phases_dict\n",
    "        self.verbose = verbose\n",
    "        self.data_dir = os.path.join(self.args.dataf, phase)\n",
    "        self.stat_path = os.path.join(self.args.dataf, 'stat.h5')\n",
    "\n",
    "        os.system('mkdir -p ' + self.data_dir)\n",
    "\n",
    "        #    self.data_names = ['positions', 'velocities', 'shape_quats', 'clusters', 'scene_params']\n",
    "        self.data_names = ['positions', 'velocities','hair_idx']\n",
    "\n",
    "        ratio = self.args.train_valid_ratio\n",
    "        if phase == 'train':\n",
    "            self.n_rollout = int(self.args.n_rollout * ratio)\n",
    "        elif phase == 'valid':\n",
    "            self.n_rollout = self.args.n_rollout - int(self.args.n_rollout * ratio)\n",
    "        else:\n",
    "            raise AssertionError(\"Unknown phase\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rollout * (self.args.time_step - 1)\n",
    "\n",
    "    def load_data(self, name):\n",
    "        self.stat = load_data(self.data_names[:2], self.stat_path)\n",
    "        for i in range(len(self.stat)):\n",
    "            self.stat[i] = self.stat[i][-self.args.position_dim:, :]\n",
    "            # print(self.data_names[i], self.stat[i].shape)\n",
    "\n",
    "    def gen_data(self, name):\n",
    "        # if the data hasn't been generated, generate the data\n",
    "        print(\"Generating data ... n_rollout=%d, time_step=%d\" % (self.n_rollout, self.args.time_step))\n",
    "\n",
    "        infos = []\n",
    "        for i in range(self.args.num_workers):\n",
    "            info = {\n",
    "                'env': self.args.env,\n",
    "                'root_num': self.phases_dict['root_num'],\n",
    "                'thread_idx': i,\n",
    "                'data_dir': self.data_dir,\n",
    "                'data_names': self.data_names,\n",
    "                'n_rollout': self.n_rollout // self.args.num_workers,\n",
    "                'n_instance': self.args.n_instance,\n",
    "                'time_step': self.args.time_step,\n",
    "                'time_step_clip': self.args.time_step_clip,\n",
    "                'dt': self.args.dt,\n",
    "                'shape_state_dim': self.args.shape_state_dim}\n",
    "\n",
    "            info['env_idx'] = 10\n",
    "            infos.append(info)\n",
    "\n",
    "        cores = self.args.num_workers\n",
    "        pool = mp.Pool(processes=cores)\n",
    "        data = pool.map(gen_PyFleX, infos)\n",
    "\n",
    "        print(\"Training data generated, warpping up stats ...\")\n",
    "\n",
    "        if self.phase == 'train' and self.args.gen_stat:\n",
    "            # positions [x, y, z], velocities[xdot, ydot, zdot]\n",
    "            if self.args.env == 'RiceGrip':\n",
    "                self.stat = [init_stat(6), init_stat(6)]\n",
    "            else:\n",
    "                self.stat = [init_stat(3), init_stat(3)]\n",
    "            for i in range(len(data)):\n",
    "                for j in range(len(self.stat)):\n",
    "                    self.stat[j] = combine_stat(self.stat[j], data[i][j])\n",
    "            store_data(self.data_names[:2], self.stat, self.stat_path)\n",
    "        else:\n",
    "            print(\"Loading stat from %s ...\" % self.stat_path)\n",
    "            self.stat = load_data(self.data_names[:2], self.stat_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_rollout = idx // (self.args.time_step - 1)\n",
    "        idx_timestep = idx % (self.args.time_step - 1)\n",
    "\n",
    "        # ignore the first frame for env RiceGrip\n",
    "        if self.args.env == 'RiceGrip' and idx_timestep == 0:\n",
    "            idx_timestep = np.random.randint(1, self.args.time_step - 1)\n",
    "\n",
    "        data_path = os.path.join(self.data_dir, str(idx_rollout), str(idx_timestep) + '.h5')\n",
    "        data_nxt_path = os.path.join(self.data_dir, str(idx_rollout), str(idx_timestep + 1) + '.h5')\n",
    "\n",
    "        data = load_data(self.data_names, data_path)\n",
    "\n",
    "        vel_his = []\n",
    "        for i in range(self.args.n_his):\n",
    "            path = os.path.join(self.data_dir, str(idx_rollout), str(max(1, idx_timestep - i - 1)) + '.h5')\n",
    "            data_his = load_data(self.data_names, path)\n",
    "            vel_his.append(data_his[1])\n",
    "\n",
    "        data[1] = np.concatenate([data[1]] + vel_his, 1)\n",
    "\n",
    "        attr, state, relations, n_particles, n_shapes, instance_idx = \\\n",
    "                prepare_input(data, self.stat, self.args, self.phases_dict, self.verbose)\n",
    "\n",
    "        ### label\n",
    "        data_nxt = normalize(load_data(self.data_names, data_nxt_path), self.stat)\n",
    "\n",
    "        label = torch.FloatTensor(data_nxt[1][:n_particles])\n",
    "\n",
    "        return attr, state, relations, n_particles, n_shapes, instance_idx, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(data, stat, args, phases_dict, verbose=0, var=False):\n",
    "\n",
    "    # Arrangement:\n",
    "    # particles, shapes, roots\n",
    "\n",
    "    if args.env == 'RiceGrip':\n",
    "        positions, velocities, shape_quats, clusters, scene_params = data\n",
    "        n_shapes = shape_quats.size(0) if var else shape_quats.shape[0]\n",
    "    elif args.env == 'FluidShake':\n",
    "        positions, velocities, shape_quats, scene_params = data\n",
    "        n_shapes = shape_quats.size(0) if var else shape_quats.shape[0]\n",
    "        clusters = None\n",
    "    elif args.env == 'BoxBath':\n",
    "        positions, velocities, clusters = data\n",
    "        n_shapes = 0\n",
    "    elif args.env == 'FluidFall':\n",
    "        positions, velocities = data\n",
    "        n_shapes = 0\n",
    "        clusters = None\n",
    "    elif args.env == 'Hairs':\n",
    "        positions, velocities, hairs_idx = data\n",
    "        n_shapes = 1\n",
    "        hairs_idx_begin = [idx[0] for idx in hairs_idx]\n",
    "        clusters = None\n",
    "\n",
    "    count_nodes = positions.size(0) if var else positions.shape[0]\n",
    "    n_particles = count_nodes - n_shapes\n",
    "\n",
    "    if verbose:\n",
    "        print(\"positions\", positions.shape)\n",
    "        print(\"velocities\", velocities.shape)\n",
    "\n",
    "        print(\"n_particles\", n_particles)\n",
    "        print(\"n_shapes\", n_shapes)\n",
    "        if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            print(\"shape_quats\", shape_quats.shape)\n",
    "\n",
    "    ### instance idx\n",
    "    #   instance_idx (n_instance + 1): start idx of instance\n",
    "    if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "        instance_idx = [0, n_particles]\n",
    "    elif args.env == 'Hairs':\n",
    "        instance_idx = [0, count_nodes]\n",
    "    else:\n",
    "        instance_idx = phases_dict[\"instance_idx\"]\n",
    "    if verbose:\n",
    "        print(\"Instance_idx:\", instance_idx)\n",
    "\n",
    "\n",
    "    ### object attributes\n",
    "    #   dim 10: [rigid, fluid, root_0, root_1, gripper_0, gripper_1, mass_inv,\n",
    "    #            clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep]\n",
    "    attr = np.zeros((count_nodes, args.attr_dim))\n",
    "    # no need to include mass for now\n",
    "    # attr[:, 6] = positions[:, -1].data.cpu().numpy() if var else positions[:, -1] # mass_inv\n",
    "    if args.env == 'RiceGrip':\n",
    "        # clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep\n",
    "        attr[:, -3:] = scene_params[-3:]\n",
    "\n",
    "\n",
    "    ### construct relations\n",
    "    Rr_idxs = []        # relation receiver idx list\n",
    "    Rs_idxs = []        # relation sender idx list\n",
    "    Ras = []            # relation attributes list\n",
    "    values = []         # relation value list (should be 1)\n",
    "    node_r_idxs = []    # list of corresponding receiver node idx\n",
    "    node_s_idxs = []    # list of corresponding sender node idx\n",
    "    psteps = []         # propagation steps\n",
    "\n",
    "    ##### add env specific graph components\n",
    "    rels = []\n",
    "    if args.env == 'RiceGrip':\n",
    "        # nodes = np.arange(n_particles)\n",
    "        for i in range(n_shapes):\n",
    "            attr[n_particles + i, 2 + i] = 1\n",
    "\n",
    "            pos = positions.data.cpu().numpy() if var else positions\n",
    "            dis = np.linalg.norm(\n",
    "                pos[:n_particles, 3:6:2] - pos[n_particles + i, 3:6:2], axis=1)\n",
    "            nodes = np.nonzero(dis < 0.3)[0]\n",
    "\n",
    "            if verbose:\n",
    "                visualize_neighbors(positions, positions, 0, nodes)\n",
    "                print(np.sort(dis)[:10])\n",
    "\n",
    "            gripper = np.ones(nodes.shape[0], dtype=np.int) * (n_particles + i)\n",
    "            rels += [np.stack([nodes, gripper, np.ones(nodes.shape[0])], axis=1)]\n",
    "            \n",
    "    elif args.env == 'Hairs':\n",
    "        # TODO: add relations between the hairs and the stick\n",
    "        attr[:,0] = 1\n",
    "        pass\n",
    "\n",
    "    elif args.env == 'FluidShake':\n",
    "        for i in range(n_shapes):\n",
    "            attr[n_particles + i, 1 + i] = 1\n",
    "\n",
    "            pos = positions.data.cpu().numpy() if var else positions\n",
    "            if i == 0:\n",
    "                # floor\n",
    "                dis = pos[:n_particles, 1] - pos[n_particles + i, 1]\n",
    "            elif i == 1:\n",
    "                # left\n",
    "                dis = pos[:n_particles, 0] - pos[n_particles + i, 0]\n",
    "            elif i == 2:\n",
    "                # right\n",
    "                dis = pos[n_particles + i, 0] - pos[:n_particles, 0]\n",
    "            elif i == 3:\n",
    "                # back\n",
    "                dis = pos[:n_particles, 2] - pos[n_particles + i, 2]\n",
    "            elif i == 4:\n",
    "                # front\n",
    "                dis = pos[n_particles + i, 2] - pos[:n_particles, 2]\n",
    "            else:\n",
    "                raise AssertionError(\"more shapes than expected\")\n",
    "            nodes = np.nonzero(dis < 0.1)[0]\n",
    "\n",
    "            if verbose:\n",
    "                visualize_neighbors(positions, positions, 0, nodes)\n",
    "                print(np.sort(dis)[:10])\n",
    "\n",
    "            wall = np.ones(nodes.shape[0], dtype=np.int) * (n_particles + i)\n",
    "            rels += [np.stack([nodes, wall, np.ones(nodes.shape[0])], axis=1)]\n",
    "\n",
    "    if verbose and len(rels) > 0:\n",
    "        print(np.concatenate(rels, 0).shape)\n",
    "\n",
    "    ##### add relations between leaf particles\n",
    "    for i in range(len(instance_idx) - 1):\n",
    "        st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "\n",
    "        if verbose:\n",
    "            print('instance #%d' % i, st, ed)\n",
    "\n",
    "        if args.env == 'BoxBath':\n",
    "            if phases_dict['material'][i] == 'rigid':\n",
    "                attr[st:ed, 0] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.concatenate((np.arange(st), np.arange(ed, n_particles)))\n",
    "            elif phases_dict['material'][i] == 'fluid':\n",
    "                attr[st:ed, 1] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.arange(n_particles)\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported materials\")\n",
    "                \n",
    "        elif args.env == 'Hairs':\n",
    "            # TODO: add relations between the hairs and the stick\n",
    "            pass\n",
    "            \n",
    "          #  if ed not in hairs_idx_begin:\n",
    "             #   queries = np.arange(st, ed)\n",
    "              #  anchors = np.arange(n_particles)\n",
    "\n",
    "        elif args.env == 'FluidFall' or args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            if phases_dict['material'][i] == 'fluid':\n",
    "                attr[st:ed, 0] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.arange(n_particles)\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported materials\")\n",
    "\n",
    "        else:\n",
    "            raise AssertionError(\"Unsupported materials\")\n",
    "\n",
    "        # st_time = time.time()\n",
    "        pos = positions\n",
    "        pos = pos[:, -3:]\n",
    "        if args.env == 'Hairs':\n",
    "            #TODO\n",
    "            pass\n",
    "        else:\n",
    "            rels += find_relations_neighbor(pos, queries, anchors, args.neighbor_radius, 2, var)\n",
    "            # return list of [receiver, sender, relation_type]\n",
    "        # print(\"Time on neighbor search\", time.time() - st_time)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Attr shape (after add env specific graph components):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "\n",
    "    rels = np.concatenate(rels, 0)\n",
    "    if rels.shape[0] > 0:\n",
    "        if verbose:\n",
    "            print(\"Relations neighbor\", rels.shape)\n",
    "        Rr_idxs.append(torch.LongTensor([rels[:, 0], np.arange(rels.shape[0])]))\n",
    "        Rs_idxs.append(torch.LongTensor([rels[:, 1], np.arange(rels.shape[0])]))\n",
    "        Ra = np.zeros((rels.shape[0], args.relation_dim))\n",
    "        Ras.append(torch.FloatTensor(Ra))\n",
    "        values.append(torch.FloatTensor([1] * rels.shape[0]))\n",
    "        node_r_idxs.append(np.arange(n_particles))\n",
    "        node_s_idxs.append(np.arange(n_particles + n_shapes))\n",
    "        psteps.append(args.pstep)\n",
    "\n",
    "    if verbose:\n",
    "        print('clusters', clusters)\n",
    "\n",
    "    # add heirarchical relations per instance\n",
    "    cnt_clusters = 0\n",
    "    for i in range(len(instance_idx) - 1):\n",
    "        st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "        n_root_level = len(phases_dict[\"root_num\"][i])\n",
    "\n",
    "        if n_root_level > 0:\n",
    "            attr, positions, velocities, count_nodes, \\\n",
    "            rels, node_r_idx, node_s_idx, pstep = \\\n",
    "                    make_hierarchy(args.env, attr, positions, velocities, i, st, ed,\n",
    "                                   phases_dict, count_nodes, clusters[cnt_clusters], verbose, var)\n",
    "\n",
    "            for j in range(len(rels)):\n",
    "                if verbose:\n",
    "                    print(\"Relation instance\", j, rels[j].shape)\n",
    "                Rr_idxs.append(torch.LongTensor([rels[j][:, 0], np.arange(rels[j].shape[0])]))\n",
    "                Rs_idxs.append(torch.LongTensor([rels[j][:, 1], np.arange(rels[j].shape[0])]))\n",
    "                Ra = np.zeros((rels[j].shape[0], args.relation_dim)); Ra[:, 0] = 1\n",
    "                Ras.append(torch.FloatTensor(Ra))\n",
    "                values.append(torch.FloatTensor([1] * rels[j].shape[0]))\n",
    "                node_r_idxs.append(node_r_idx[j])\n",
    "                node_s_idxs.append(node_s_idx[j])\n",
    "                psteps.append(pstep[j])\n",
    "\n",
    "            cnt_clusters += 1\n",
    "\n",
    "    if verbose:\n",
    "        if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            print(\"Scene_params:\", scene_params)\n",
    "\n",
    "        print(\"Attr shape (after hierarchy building):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "        print(\"Particle attr:\", np.sum(attr[:n_particles], axis=0))\n",
    "        print(\"Shape attr:\", np.sum(attr[n_particles:n_particles+n_shapes], axis=0))\n",
    "        print(\"Roots attr:\", np.sum(attr[n_particles+n_shapes:], axis=0))\n",
    "\n",
    "    ### normalize data\n",
    "    data = [positions, velocities]\n",
    "    data = normalize(data, stat, var)\n",
    "    positions, velocities = data[0], data[1]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Particle positions stats\")\n",
    "        print(positions.shape)\n",
    "        print(np.min(positions[:n_particles], 0))\n",
    "        print(np.max(positions[:n_particles], 0))\n",
    "        print(np.mean(positions[:n_particles], 0))\n",
    "        print(np.std(positions[:n_particles], 0))\n",
    "\n",
    "        show_vel_dim = 6 if args.env == 'RiceGrip' else 3\n",
    "        print(\"Velocities stats\")\n",
    "        print(velocities.shape)\n",
    "        print(np.mean(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        print(np.std(velocities[:n_particles, :show_vel_dim], 0))\n",
    "\n",
    "    if args.env == 'RiceGrip':\n",
    "        if var:\n",
    "            quats = torch.cat(\n",
    "                [Variable(torch.zeros(n_particles, 4).cuda()), shape_quats,\n",
    "                 Variable(torch.zeros(count_nodes - n_particles - n_shapes, 4).cuda())], 0)\n",
    "            state = torch.cat([positions, velocities, quats], 1)\n",
    "        else:\n",
    "            quat_null = np.array([[0., 0., 0., 0.]])\n",
    "            quats = np.repeat(quat_null, [count_nodes], axis=0)\n",
    "            quats[n_particles:n_particles + n_shapes] = shape_quats\n",
    "            # if args.eval == 0:\n",
    "            # quats += np.random.randn(quats.shape[0], 4) * 0.05\n",
    "            state = torch.FloatTensor(np.concatenate([positions, velocities, quats], axis=1))\n",
    "    else:\n",
    "        if var:\n",
    "            state = torch.cat([positions, velocities], 1)\n",
    "        else:\n",
    "            state = torch.FloatTensor(np.concatenate([positions, velocities], axis=1))\n",
    "\n",
    "    if verbose:\n",
    "        for i in range(count_nodes - 1):\n",
    "            if np.sum(np.abs(attr[i] - attr[i + 1])) > 1e-6:\n",
    "                print(i, attr[i], attr[i + 1])\n",
    "\n",
    "        for i in range(len(Ras)):\n",
    "            print(i, np.min(node_r_idxs[i]), np.max(node_r_idxs[i]), np.min(node_s_idxs[i]), np.max(node_s_idxs[i]))\n",
    "\n",
    "    attr = torch.FloatTensor(attr)\n",
    "    relations = [Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps]\n",
    "\n",
    "    return attr, state, relations, n_particles, n_shapes, instance_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "datasets = {phase: PhysicsFleXDataset(\n",
    "    args, phase, phases_dict, args.verbose_data) for phase in ['train', 'valid']}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "datasets['train'].gen_data(args.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['train'].load_data(args.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "idx_rollout = idx // (datasets['train'].args.time_step - 1)\n",
    "idx_timestep = idx % (datasets['train'].args.time_step - 1)\n",
    "\n",
    "# ignore the first frame for env RiceGrip\n",
    "if datasets['train'].args.env == 'RiceGrip' and idx_timestep == 0:\n",
    "    idx_timestep = np.random.randint(1, datasets['train'].args.time_step - 1)\n",
    "\n",
    "data_path = os.path.join(datasets['train'].data_dir, str(idx_rollout), str(idx_timestep) + '.h5')\n",
    "data_nxt_path = os.path.join(datasets['train'].data_dir, str(idx_rollout), str(idx_timestep + 1) + '.h5')\n",
    "\n",
    "data = load_data(datasets['train'].data_names, data_path)\n",
    "\n",
    "vel_his = []\n",
    "for i in range(datasets['train'].args.n_his):\n",
    "    path = os.path.join(datasets['train'].data_dir, str(idx_rollout), str(max(1, idx_timestep - i - 1)) + '.h5')\n",
    "    data_his = load_data(datasets['train'].data_names, path)\n",
    "    vel_his.append(data_his[1])\n",
    "\n",
    "data[1] = np.concatenate([data[1]] + vel_his, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['train'].stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions, velocities, hairs_idx = data\n",
    "n_shapes = 1\n",
    "hairs_idx_begin = [idx[0] for idx in hairs_idx]\n",
    "hairs_idx_end = [idx[1] for idx in hairs_idx]\n",
    "\n",
    "clusters = None\n",
    "\n",
    "n_particles = positions.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions[:3,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_idx = [0, n_particles]\n",
    "### object attributes\n",
    "count_nodes = n_particles\n",
    "attr = np.zeros((count_nodes, args.attr_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### add env specific graph components\n",
    "rels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr[:n_particles,0] = 1\n",
    "#rels += find_relations_neighbor(pos, queries, anchors, args.neighbor_radius, 2, var)\n",
    "#queries = set of hairs, anchors = stick\n",
    "for i in range(n_particles):\n",
    "    if i in hairs_idx_begin:\n",
    "        receiver = [i+1]\n",
    "        sender = [i]\n",
    "        relation_type = [1]\n",
    "    elif i in hairs_idx_end:\n",
    "        receiver = [i-1]\n",
    "        sender = [i]\n",
    "        relation_type = [1]\n",
    "    else:\n",
    "        receiver = [i-1,i+1]\n",
    "        sender = [i, i]\n",
    "        relation_type = [1,1]\n",
    "    rels.append(np.stack([receiver, sender, relation_type], axis=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels = np.concatenate(rels, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### construct relations\n",
    "Rr_idxs = []        # relation receiver idx list\n",
    "Rs_idxs = []        # relation sender idx list\n",
    "Ras = []            # relation attributes list\n",
    "values = []         # relation value list (should be 1)\n",
    "node_r_idxs = []    # list of corresponding receiver node idx\n",
    "node_s_idxs = []    # list of corresponding sender node idx\n",
    "psteps = []         # propagation steps\n",
    "\n",
    "\n",
    "Rr_idxs.append(torch.LongTensor([rels[:, 0], np.arange(rels.shape[0])]))\n",
    "Rs_idxs.append(torch.LongTensor([rels[:, 1], np.arange(rels.shape[0])]))\n",
    "Ra = np.zeros((rels.shape[0], args.relation_dim))\n",
    "Ras.append(torch.FloatTensor(Ra))\n",
    "values.append(torch.FloatTensor([1] * rels.shape[0]))\n",
    "node_r_idxs.append(np.arange(n_particles))\n",
    "node_s_idxs.append(np.arange(n_particles + n_shapes))\n",
    "psteps.append(args.pstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [positions, velocities]\n",
    "stat = datasets['train'].stat\n",
    "data = normalize(data, stat)\n",
    "positions, velocities = data[0], data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = torch.FloatTensor(attr)\n",
    "relations = [Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(data, stat, args, phases_dict, verbose=0, var=False):\n",
    "    \n",
    "    positions, velocities, hairs_idx = data\n",
    "    n_shapes = 1\n",
    "    hairs_idx_begin = [idx[0] for idx in hairs_idx]\n",
    "    clusters = None\n",
    "\n",
    "    count_nodes = positions.size(0) if var else positions.shape[0]\n",
    "    n_particles = count_nodes - n_shapes\n",
    "\n",
    "    ### instance idx\n",
    "    #   instance_idx (n_instance + 1): start idx of instance\n",
    "    if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "        instance_idx = [0, n_particles]\n",
    "    elif args.env == 'Hairs':\n",
    "        instance_idx = [0, count_nodes]\n",
    "    else:\n",
    "        instance_idx = phases_dict[\"instance_idx\"]\n",
    "    if verbose:\n",
    "        print(\"Instance_idx:\", instance_idx)\n",
    "\n",
    "\n",
    "    ### object attributes\n",
    "    #   dim 10: [rigid, fluid, root_0, root_1, gripper_0, gripper_1, mass_inv,\n",
    "    #            clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep]\n",
    "    attr = np.zeros((count_nodes, args.attr_dim))\n",
    "    # no need to include mass for now\n",
    "    # attr[:, 6] = positions[:, -1].data.cpu().numpy() if var else positions[:, -1] # mass_inv\n",
    "    if args.env == 'RiceGrip':\n",
    "        # clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep\n",
    "        attr[:, -3:] = scene_params[-3:]\n",
    "\n",
    "\n",
    "    ### construct relations\n",
    "    Rr_idxs = []        # relation receiver idx list\n",
    "    Rs_idxs = []        # relation sender idx list\n",
    "    Ras = []            # relation attributes list\n",
    "    values = []         # relation value list (should be 1)\n",
    "    node_r_idxs = []    # list of corresponding receiver node idx\n",
    "    node_s_idxs = []    # list of corresponding sender node idx\n",
    "    psteps = []         # propagation steps\n",
    "\n",
    "    ##### add env specific graph components\n",
    "    rels = []\n",
    "    if args.env == 'RiceGrip':\n",
    "        # nodes = np.arange(n_particles)\n",
    "        for i in range(n_shapes):\n",
    "            attr[n_particles + i, 2 + i] = 1\n",
    "\n",
    "            pos = positions.data.cpu().numpy() if var else positions\n",
    "            dis = np.linalg.norm(\n",
    "                pos[:n_particles, 3:6:2] - pos[n_particles + i, 3:6:2], axis=1)\n",
    "            nodes = np.nonzero(dis < 0.3)[0]\n",
    "\n",
    "            if verbose:\n",
    "                visualize_neighbors(positions, positions, 0, nodes)\n",
    "                print(np.sort(dis)[:10])\n",
    "\n",
    "            gripper = np.ones(nodes.shape[0], dtype=np.int) * (n_particles + i)\n",
    "            rels += [np.stack([nodes, gripper, np.ones(nodes.shape[0])], axis=1)]\n",
    "            \n",
    "    elif args.env = 'Hairs':\n",
    "        # TODO: add relations between the hairs and the stick\n",
    "        attr[:,0] = 1\n",
    "        pass\n",
    "\n",
    "    elif args.env == 'FluidShake':\n",
    "        for i in range(n_shapes):\n",
    "            attr[n_particles + i, 1 + i] = 1\n",
    "\n",
    "            pos = positions.data.cpu().numpy() if var else positions\n",
    "            if i == 0:\n",
    "                # floor\n",
    "                dis = pos[:n_particles, 1] - pos[n_particles + i, 1]\n",
    "            elif i == 1:\n",
    "                # left\n",
    "                dis = pos[:n_particles, 0] - pos[n_particles + i, 0]\n",
    "            elif i == 2:\n",
    "                # right\n",
    "                dis = pos[n_particles + i, 0] - pos[:n_particles, 0]\n",
    "            elif i == 3:\n",
    "                # back\n",
    "                dis = pos[:n_particles, 2] - pos[n_particles + i, 2]\n",
    "            elif i == 4:\n",
    "                # front\n",
    "                dis = pos[n_particles + i, 2] - pos[:n_particles, 2]\n",
    "            else:\n",
    "                raise AssertionError(\"more shapes than expected\")\n",
    "            nodes = np.nonzero(dis < 0.1)[0]\n",
    "\n",
    "            if verbose:\n",
    "                visualize_neighbors(positions, positions, 0, nodes)\n",
    "                print(np.sort(dis)[:10])\n",
    "\n",
    "            wall = np.ones(nodes.shape[0], dtype=np.int) * (n_particles + i)\n",
    "            rels += [np.stack([nodes, wall, np.ones(nodes.shape[0])], axis=1)]\n",
    "\n",
    "    if verbose and len(rels) > 0:\n",
    "        print(np.concatenate(rels, 0).shape)\n",
    "\n",
    "    ##### add relations between leaf particles\n",
    "    for i in range(len(instance_idx) - 1):\n",
    "        st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "\n",
    "        if verbose:\n",
    "            print('instance #%d' % i, st, ed)\n",
    "\n",
    "        if args.env == 'BoxBath':\n",
    "            if phases_dict['material'][i] == 'rigid':\n",
    "                attr[st:ed, 0] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.concatenate((np.arange(st), np.arange(ed, n_particles)))\n",
    "            elif phases_dict['material'][i] == 'fluid':\n",
    "                attr[st:ed, 1] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.arange(n_particles)\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported materials\")\n",
    "                \n",
    "        elif args.env == 'Hairs':\n",
    "            # TODO: add relations between the hairs and the stick\n",
    "            pass\n",
    "            \n",
    "          #  if ed not in hairs_idx_begin:\n",
    "             #   queries = np.arange(st, ed)\n",
    "              #  anchors = np.arange(n_particles)\n",
    "\n",
    "        elif args.env == 'FluidFall' or args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            if phases_dict['material'][i] == 'fluid':\n",
    "                attr[st:ed, 0] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.arange(n_particles)\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported materials\")\n",
    "\n",
    "        else:\n",
    "            raise AssertionError(\"Unsupported materials\")\n",
    "\n",
    "        # st_time = time.time()\n",
    "        pos = positions\n",
    "        pos = pos[:, -3:]\n",
    "        if args.env == 'Hairs':\n",
    "            #TODO\n",
    "            pass\n",
    "        else:\n",
    "            rels += find_relations_neighbor(pos, queries, anchors, args.neighbor_radius, 2, var)\n",
    "            # return list of [receiver, sender, relation_type]\n",
    "        # print(\"Time on neighbor search\", time.time() - st_time)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Attr shape (after add env specific graph components):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "\n",
    "    rels = np.concatenate(rels, 0)\n",
    "    if rels.shape[0] > 0:\n",
    "        if verbose:\n",
    "            print(\"Relations neighbor\", rels.shape)\n",
    "        Rr_idxs.append(torch.LongTensor([rels[:, 0], np.arange(rels.shape[0])]))\n",
    "        Rs_idxs.append(torch.LongTensor([rels[:, 1], np.arange(rels.shape[0])]))\n",
    "        Ra = np.zeros((rels.shape[0], args.relation_dim))\n",
    "        Ras.append(torch.FloatTensor(Ra))\n",
    "        values.append(torch.FloatTensor([1] * rels.shape[0]))\n",
    "        node_r_idxs.append(np.arange(n_particles))\n",
    "        node_s_idxs.append(np.arange(n_particles + n_shapes))\n",
    "        psteps.append(args.pstep)\n",
    "\n",
    "    if verbose:\n",
    "        print('clusters', clusters)\n",
    "\n",
    "    # add heirarchical relations per instance\n",
    "    cnt_clusters = 0\n",
    "    for i in range(len(instance_idx) - 1):\n",
    "        st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "        n_root_level = len(phases_dict[\"root_num\"][i])\n",
    "\n",
    "        if n_root_level > 0:\n",
    "            attr, positions, velocities, count_nodes, \\\n",
    "            rels, node_r_idx, node_s_idx, pstep = \\\n",
    "                    make_hierarchy(args.env, attr, positions, velocities, i, st, ed,\n",
    "                                   phases_dict, count_nodes, clusters[cnt_clusters], verbose, var)\n",
    "\n",
    "            for j in range(len(rels)):\n",
    "                if verbose:\n",
    "                    print(\"Relation instance\", j, rels[j].shape)\n",
    "                Rr_idxs.append(torch.LongTensor([rels[j][:, 0], np.arange(rels[j].shape[0])]))\n",
    "                Rs_idxs.append(torch.LongTensor([rels[j][:, 1], np.arange(rels[j].shape[0])]))\n",
    "                Ra = np.zeros((rels[j].shape[0], args.relation_dim)); Ra[:, 0] = 1\n",
    "                Ras.append(torch.FloatTensor(Ra))\n",
    "                values.append(torch.FloatTensor([1] * rels[j].shape[0]))\n",
    "                node_r_idxs.append(node_r_idx[j])\n",
    "                node_s_idxs.append(node_s_idx[j])\n",
    "                psteps.append(pstep[j])\n",
    "\n",
    "            cnt_clusters += 1\n",
    "\n",
    "    if verbose:\n",
    "        if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            print(\"Scene_params:\", scene_params)\n",
    "\n",
    "        print(\"Attr shape (after hierarchy building):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "        print(\"Particle attr:\", np.sum(attr[:n_particles], axis=0))\n",
    "        print(\"Shape attr:\", np.sum(attr[n_particles:n_particles+n_shapes], axis=0))\n",
    "        print(\"Roots attr:\", np.sum(attr[n_particles+n_shapes:], axis=0))\n",
    "\n",
    "    ### normalize data\n",
    "    data = [positions, velocities]\n",
    "    data = normalize(data, stat, var)\n",
    "    positions, velocities = data[0], data[1]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Particle positions stats\")\n",
    "        print(positions.shape)\n",
    "        print(np.min(positions[:n_particles], 0))\n",
    "        print(np.max(positions[:n_particles], 0))\n",
    "        print(np.mean(positions[:n_particles], 0))\n",
    "        print(np.std(positions[:n_particles], 0))\n",
    "\n",
    "        show_vel_dim = 6 if args.env == 'RiceGrip' else 3\n",
    "        print(\"Velocities stats\")\n",
    "        print(velocities.shape)\n",
    "        print(np.mean(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        print(np.std(velocities[:n_particles, :show_vel_dim], 0))\n",
    "\n",
    "    if args.env == 'RiceGrip':\n",
    "        if var:\n",
    "            quats = torch.cat(\n",
    "                [Variable(torch.zeros(n_particles, 4).cuda()), shape_quats,\n",
    "                 Variable(torch.zeros(count_nodes - n_particles - n_shapes, 4).cuda())], 0)\n",
    "            state = torch.cat([positions, velocities, quats], 1)\n",
    "        else:\n",
    "            quat_null = np.array([[0., 0., 0., 0.]])\n",
    "            quats = np.repeat(quat_null, [count_nodes], axis=0)\n",
    "            quats[n_particles:n_particles + n_shapes] = shape_quats\n",
    "            # if args.eval == 0:\n",
    "            # quats += np.random.randn(quats.shape[0], 4) * 0.05\n",
    "            state = torch.FloatTensor(np.concatenate([positions, velocities, quats], axis=1))\n",
    "    else:\n",
    "        if var:\n",
    "            state = torch.cat([positions, velocities], 1)\n",
    "        else:\n",
    "            state = torch.FloatTensor(np.concatenate([positions, velocities], axis=1))\n",
    "\n",
    "    if verbose:\n",
    "        for i in range(count_nodes - 1):\n",
    "            if np.sum(np.abs(attr[i] - attr[i + 1])) > 1e-6:\n",
    "                print(i, attr[i], attr[i + 1])\n",
    "\n",
    "        for i in range(len(Ras)):\n",
    "            print(i, np.min(node_r_idxs[i]), np.max(node_r_idxs[i]), np.min(node_s_idxs[i]), np.max(node_s_idxs[i]))\n",
    "\n",
    "    attr = torch.FloatTensor(attr)\n",
    "    relations = [Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps]\n",
    "\n",
    "    return attr, state, relations, n_particles, n_shapes, instance_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attr, state, relations, n_particles, n_shapes, instance_idx = \\\n",
    "        prepare_input(data, self.stat, self.args, self.phases_dict, self.verbose)\n",
    "\n",
    "### label\n",
    "data_nxt = normalize(load_data(self.data_names, data_nxt_path), self.stat)\n",
    "\n",
    "label = torch.FloatTensor(data_nxt[1][:n_particles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets['train'].__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
