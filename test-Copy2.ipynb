{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from models import DPINet\n",
    "#from data import PhysicsFleXDataset, collate_fn\n",
    "\n",
    "from utils import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--relation_dim'], dest='relation_dim', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--pstep', type=int, default=2)\n",
    "parser.add_argument('--n_rollout', type=int, default=0)\n",
    "parser.add_argument('--time_step', type=int, default=0)\n",
    "parser.add_argument('--time_step_clip', type=int, default=0)\n",
    "parser.add_argument('--dt', type=float, default=1./60.)\n",
    "parser.add_argument('--nf_relation', type=int, default=300)\n",
    "parser.add_argument('--nf_particle', type=int, default=200)\n",
    "parser.add_argument('--nf_effect', type=int, default=200)\n",
    "parser.add_argument('--env', default='')\n",
    "parser.add_argument('--train_valid_ratio', type=float, default=0.9)\n",
    "parser.add_argument('--outf', default='files')\n",
    "parser.add_argument('--dataf', default='data')\n",
    "parser.add_argument('--num_workers', type=int, default=10)\n",
    "parser.add_argument('--gen_data', type=int, default=0)\n",
    "parser.add_argument('--gen_stat', type=int, default=0)\n",
    "parser.add_argument('--log_per_iter', type=int, default=1000)\n",
    "parser.add_argument('--ckp_per_iter', type=int, default=10000)\n",
    "parser.add_argument('--eval', type=int, default=0)\n",
    "parser.add_argument('--verbose_data', type=int, default=1)\n",
    "parser.add_argument('--verbose_model', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--n_instance', type=int, default=0)\n",
    "parser.add_argument('--n_stages', type=int, default=0)\n",
    "parser.add_argument('--n_his', type=int, default=0)\n",
    "\n",
    "parser.add_argument('--n_epoch', type=int, default=1000)\n",
    "parser.add_argument('--beta1', type=float, default=0.9)\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--batch_size', type=int, default=1)\n",
    "parser.add_argument('--forward_times', type=int, default=2)\n",
    "\n",
    "parser.add_argument('--resume_epoch', type=int, default=0)\n",
    "parser.add_argument('--resume_iter', type=int, default=0)\n",
    "\n",
    "# shape state:\n",
    "# [x, y, z, x_last, y_last, z_last, quat(4), quat_last(4)]\n",
    "parser.add_argument('--shape_state_dim', type=int, default=14)\n",
    "\n",
    "# object attributes:\n",
    "parser.add_argument('--attr_dim', type=int, default=0)\n",
    "\n",
    "# object state:\n",
    "parser.add_argument('--state_dim', type=int, default=0)\n",
    "parser.add_argument('--position_dim', type=int, default=0)\n",
    "\n",
    "# relation attr:\n",
    "parser.add_argument('--relation_dim', type=int, default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"--env SingleHair --gen_data 1\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases_dict = dict()\n",
    "\n",
    "args.n_rollout = 50\n",
    "args.num_workers = 3\n",
    "args.gen_stat = 1\n",
    "\n",
    "# object states:\n",
    "# [x, y, z, xdot, ydot, zdot]\n",
    "args.state_dim = 6\n",
    "args.position_dim = 3\n",
    "\n",
    "# object attr:\n",
    "# [rigid]\n",
    "args.attr_dim = 1\n",
    "\n",
    "# relation attr:\n",
    "# [none]\n",
    "args.relation_dim = 1\n",
    "\n",
    "args.time_step = 500\n",
    "args.time_step_clip = 20\n",
    "args.n_instance = 1\n",
    "args.n_stages = 1\n",
    "\n",
    "args.neighbor_radius = 0.08\n",
    "\n",
    "phases_dict[\"instance_idx\"] = [0, 30]\n",
    "phases_dict[\"root_num\"] = [[]]\n",
    "phases_dict[\"instance\"] = ['solid']\n",
    "phases_dict[\"material\"] = ['solid']\n",
    "\n",
    "args.outf = 'dump_SingleHair/' + args.outf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/data_SingleHair\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.outf = args.outf + '_' + args.env\n",
    "args.dataf = 'data/' + args.dataf + '_' + args.env\n",
    "print (args.dataf)\n",
    "os.system('mkdir -p ' + args.outf)\n",
    "os.system('mkdir -p ' + args.dataf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_PyFleX(info):\n",
    "\n",
    "    env, root_num = info['env'], info['root_num']\n",
    "    thread_idx, data_dir, data_names = info['thread_idx'], info['data_dir'], info['data_names']\n",
    "    n_rollout, n_instance = info['n_rollout'], info['n_instance']\n",
    "    time_step, time_step_clip = info['time_step'], info['time_step_clip']\n",
    "    shape_state_dim, dt = info['shape_state_dim'], info['dt']\n",
    "\n",
    "    env_idx = info['env_idx'] # =11\n",
    "\n",
    "    np.random.seed(round(time.time() * 1000 + thread_idx) % 2**32)\n",
    "    \n",
    "    stats = [init_stat(3), init_stat(3)]\n",
    "\n",
    "    import pyflex\n",
    "    pyflex.init()\n",
    "\n",
    "    for i in range(n_rollout):\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"%d / %d\" % (i, n_rollout))\n",
    "\n",
    "        rollout_idx = thread_idx * n_rollout + i\n",
    "        rollout_dir = os.path.join(data_dir, str(rollout_idx))\n",
    "        os.system('mkdir -p ' + rollout_dir)\n",
    "        \n",
    "        # scene_params: [len(box) at dim x,len(box) at dim y,len(box) at dim z, num_hair per circle, num_circle]\n",
    "        cap_size = [0.1,1.5]\n",
    "        N_hairs = 1\n",
    "\n",
    "\n",
    "        scene_params = np.array(cap_size)\n",
    "\n",
    "        pyflex.set_scene(env_idx, scene_params, thread_idx)\n",
    "        n_particles = pyflex.get_n_particles()\n",
    "        n_shapes = 1\n",
    "        N_particles_per_hair = int(n_particles/N_hairs)\n",
    "        idx_begins = np.arange(N_hairs)*N_particles_per_hair\n",
    "        idx_hairs = [[i,i+N_particles_per_hair-1] for i in idx_begins]\n",
    "\n",
    "        positions = np.zeros((time_step, n_particles+ n_shapes, 3), dtype=np.float32)\n",
    "        velocities = np.zeros((time_step, n_particles+ n_shapes, 3), dtype=np.float32)\n",
    "   #     shape_position = np.zeros((time_step, n_shapes, 3), dtype=np.float32)\n",
    "    #    shape_velocities = np.zeros((time_step, n_shapes, 3), dtype=np.float32)\n",
    "\n",
    "        for j in range(time_step_clip):\n",
    "         #   p_clip = pyflex.get_positions().reshape(-1, 4)[:, :3]\n",
    "         #   shape_p_clip = pyflex.get_shape_states()[:3].reshape(-1,3)\n",
    "            p_clip = np.concatenate([pyflex.get_positions().reshape(-1, 4)[:, :3],pyflex.get_shape_states()[:3].reshape(-1,3)],axis = 0)\n",
    "            pyflex.step()\n",
    "\n",
    "        for j in range(time_step):\n",
    "            positions[j, :n_particles] = pyflex.get_positions().reshape(-1, 4)[:, :3]\n",
    "            for k in range(n_shapes):\n",
    "                 positions[j, n_particles + k] = pyflex.get_shape_states()[:3]\n",
    "        #    shape_position[j] = pyflex.get_shape_states()[:3].reshape(-1,3)\n",
    "        #    shape_prevposition = pyflex.get_shape_states()[3:6].reshape(-1,3)\n",
    "            if j == 0:\n",
    "                velocities[j] = (positions[j] - p_clip) / dt\n",
    "           #     shape_velocities[j] = (shape_position[j] - shape_p_clip)/dt\n",
    "            else:\n",
    "                velocities[j] = (positions[j] - positions[j - 1]) / dt\n",
    "          #      shape_velocities[j] = (shape_position[j] - shape_position[j-1])/dt\n",
    "\n",
    "            pyflex.step()\n",
    "            data = [positions[j], velocities[j], idx_hairs]\n",
    "            store_data(data_names, data, os.path.join(rollout_dir, str(j) + '.h5'))\n",
    "        \n",
    "        # change dtype for more accurate stat calculation\n",
    "        # only normalize positions and velocities\n",
    "        datas = [positions.astype(np.float64), velocities.astype(np.float64)]\n",
    "\n",
    "        for j in range(len(stats)): \n",
    "            # here j = 2, refers to positions and velocities\n",
    "            stat = init_stat(stats[j].shape[0]) \n",
    "            # stat= np.zeros((3,3))\n",
    "            stat[:, 0] = np.mean(datas[j], axis=(0, 1))[:]\n",
    "            stat[:, 1] = np.std(datas[j], axis=(0, 1))[:]\n",
    "            stat[:, 2] = datas[j].shape[0] * datas[j].shape[1] # time_step*n_particles\n",
    "            stats[j] = combine_stat(stats[j], stat)\n",
    "\n",
    "    pyflex.clean()\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = ['positions', 'velocities','hair_idx']\n",
    "verbose = 0\n",
    "phase = 'train'\n",
    "data_dir = os.path.join(args.dataf, phase)\n",
    "stat_path = os.path.join(args.dataf, 'stat.h5')\n",
    "n_rollout = 10\n",
    "\n",
    "\n",
    "\n",
    "info = {\n",
    "    'env': args.env,\n",
    "    'root_num': phases_dict['root_num'],\n",
    "    'thread_idx': 0,\n",
    "    'data_dir': data_dir,\n",
    "    'data_names': data_names,\n",
    "    'n_rollout': n_rollout // args.num_workers,\n",
    "    'n_instance': args.n_instance,\n",
    "    'time_step': args.time_step,\n",
    "    'time_step_clip': args.time_step_clip,\n",
    "    'dt': args.dt,\n",
    "    'shape_state_dim': args.shape_state_dim}\n",
    "\n",
    "info['env_idx'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsFleXDataset(Dataset):\n",
    "\n",
    "    def __init__(self, args, phase, phases_dict, verbose):\n",
    "        self.args = args\n",
    "        self.phase = phase\n",
    "        self.phases_dict = phases_dict\n",
    "        self.verbose = verbose\n",
    "        self.data_dir = os.path.join(self.args.dataf, phase)\n",
    "        self.stat_path = os.path.join(self.args.dataf, 'stat.h5')\n",
    "\n",
    "        os.system('mkdir -p ' + self.data_dir)\n",
    "\n",
    "        #    self.data_names = ['positions', 'velocities', 'shape_quats', 'clusters', 'scene_params']\n",
    "        self.data_names = ['positions', 'velocities','hair_idx']\n",
    "\n",
    "        ratio = self.args.train_valid_ratio\n",
    "        if phase == 'train':\n",
    "            self.n_rollout = int(self.args.n_rollout * ratio)\n",
    "        elif phase == 'valid':\n",
    "            self.n_rollout = self.args.n_rollout - int(self.args.n_rollout * ratio)\n",
    "        else:\n",
    "            raise AssertionError(\"Unknown phase\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rollout * (self.args.time_step - 1)\n",
    "\n",
    "    def load_data(self, name):\n",
    "        self.stat = load_data(self.data_names[:2], self.stat_path)\n",
    "        for i in range(len(self.stat)):\n",
    "            self.stat[i] = self.stat[i][-self.args.position_dim:, :]\n",
    "            # print(self.data_names[i], self.stat[i].shape)\n",
    "\n",
    "    def gen_data(self, name):\n",
    "        # if the data hasn't been generated, generate the data\n",
    "        print(\"Generating data ... n_rollout=%d, time_step=%d\" % (self.n_rollout, self.args.time_step))\n",
    "\n",
    "        infos = []\n",
    "        for i in range(self.args.num_workers):\n",
    "            info = {\n",
    "                'env': self.args.env,\n",
    "                'root_num': self.phases_dict['root_num'],\n",
    "                'thread_idx': i,\n",
    "                'data_dir': self.data_dir,\n",
    "                'data_names': self.data_names,\n",
    "                'n_rollout': self.n_rollout // self.args.num_workers,\n",
    "                'n_instance': self.args.n_instance,\n",
    "                'time_step': self.args.time_step,\n",
    "                'time_step_clip': self.args.time_step_clip,\n",
    "                'dt': self.args.dt,\n",
    "                'shape_state_dim': self.args.shape_state_dim}\n",
    "\n",
    "            info['env_idx'] = 11\n",
    "            infos.append(info)\n",
    "\n",
    "        cores = self.args.num_workers\n",
    "        pool = mp.Pool(processes=cores)\n",
    "        data = pool.map(gen_PyFleX, infos)\n",
    "\n",
    "        print(\"Training data generated, warpping up stats ...\")\n",
    "\n",
    "        if self.phase == 'train' and self.args.gen_stat:\n",
    "            # positions [x, y, z], velocities[xdot, ydot, zdot]\n",
    "            self.stat = [init_stat(3), init_stat(3)]\n",
    "            for i in range(len(data)):\n",
    "                for j in range(len(self.stat)):\n",
    "                    self.stat[j] = combine_stat(self.stat[j], data[i][j])\n",
    "            store_data(self.data_names[:2], self.stat, self.stat_path)\n",
    "        else:\n",
    "            print(\"Loading stat from %s ...\" % self.stat_path)\n",
    "            self.stat = load_data(self.data_names[:2], self.stat_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_rollout = idx // (self.args.time_step - 1)\n",
    "        idx_timestep = idx % (self.args.time_step - 1)\n",
    "\n",
    "        # ignore the first frame for env RiceGrip\n",
    "        if self.args.env == 'RiceGrip' and idx_timestep == 0:\n",
    "            idx_timestep = np.random.randint(1, self.args.time_step - 1)\n",
    "\n",
    "        data_path = os.path.join(self.data_dir, str(idx_rollout), str(idx_timestep) + '.h5')\n",
    "        data_nxt_path = os.path.join(self.data_dir, str(idx_rollout), str(idx_timestep + 1) + '.h5')\n",
    "\n",
    "        data = load_data(self.data_names, data_path)\n",
    "\n",
    "        '''\n",
    "        vel_his = []\n",
    "        for i in range(self.args.n_his):\n",
    "            path = os.path.join(self.data_dir, str(idx_rollout), str(max(1, idx_timestep - i - 1)) + '.h5')\n",
    "            data_his = load_data(self.data_names, path)\n",
    "            vel_his.append(data_his[1])\n",
    "\n",
    "        data[1] = np.concatenate([data[1]] + vel_his, 1)\n",
    "        '''\n",
    "        \n",
    "        attr, state, relations, n_particles, n_shapes = \\\n",
    "                prepare_input(data, self.stat, self.args, self.phases_dict, self.verbose)\n",
    "\n",
    "        ### label\n",
    "        data_nxt = normalize(load_data(self.data_names, data_nxt_path), self.stat)\n",
    "\n",
    "        label = torch.FloatTensor(data_nxt[1][:n_particles])\n",
    "\n",
    "        return attr, state, relations, n_particles, n_shapes, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(data, stat, args, phases_dict, verbose=0, var=False):\n",
    "    '''\n",
    "    for a single hair\n",
    "    '''\n",
    "    positions, velocities, hairs_idx = data\n",
    "    n_shapes = 1\n",
    "    hairs_idx_begin = [idx[0] for idx in hairs_idx]\n",
    "    n_particles = positions.shape[0] - n_shapes\n",
    "    R = 0.1\n",
    "    \n",
    "    ### object attributes\n",
    "    #   dim 10: [rigid, fluid, root_0, root_1, gripper_0, gripper_1, mass_inv,\n",
    "    #            clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep]\n",
    "    #   here we only consider the hairs but not the gripper, attr_dim = 1, attr = 0 for hair, attr = 1 for shapes\n",
    "    attr = np.zeros((n_particles+n_shapes, args.attr_dim))\n",
    "    \n",
    "    ### construct relations\n",
    "    Rr_idxs = []        # relation receiver idx list\n",
    "    Rs_idxs = []        # relation sender idx list\n",
    "    Ras = []            # relation attributes list\n",
    "    values = []         # relation value list (should be 1)\n",
    "    node_r_idxs = []    # list of corresponding receiver node idx\n",
    "    node_s_idxs = []    # list of corresponding sender node idx\n",
    "    psteps = []         # propagation steps\n",
    "    \n",
    "    ##### add env specific graph components\n",
    "    ### specify for shapes\n",
    "    rels = []\n",
    "    vals = []\n",
    "    \n",
    "    for i in range(n_shapes):\n",
    "        attr[n_particles+i, 0] = 1\n",
    "        dis = np.linalg.norm(positions[:n_particles,:2]-positions[n_particles+i,:2],axis = 1)\n",
    "        nodes_rel = np.nonzero(dis <= R)[0]\n",
    "        # for relation between hair nodes and a gripper, we note it as 1\n",
    "        gripper = np.ones(nodes_rel.shape[0], dtype=np.int) * (n_particles+i)\n",
    "        rels += [np.stack([nodes_rel, gripper, np.ones(nodes_rel.shape[0])], axis=1)]\n",
    "        vals += [np.ones(nodes_rel.shape[0], dtype=np.int)]\n",
    "        \n",
    "    \n",
    "    ##### add relations between leaf particles\n",
    "    ## here we only consider the relations in a hair: the relation between a node and the nodes nearby\n",
    "    ## simple case for one hair, TEMPORARY 2 rels for one link\n",
    "    nodes_p = np.arange(n_particles-1)\n",
    "    val = np.linalg.norm(positions[1:n_particles]-positions[:n_particles-1],axis = 1)\n",
    "    R1 = np.stack([nodes_p,nodes_p+1, np.zeros(n_particles-1)],axis = 1)\n",
    "    R2 = np.stack([nodes_p+1,nodes_p, np.zeros(n_particles-1)],axis = 1)\n",
    "    rels += [np.concatenate([R1,R2],axis = 0)]\n",
    "    vals += [val,val]\n",
    "    \n",
    "    rels = np.concatenate(rels, 0)\n",
    "    vals = np.concatenate(vals, 0)\n",
    "    \n",
    "  #  print (vals.shape)\n",
    " #   print (rels.shape)\n",
    "    \n",
    "    \n",
    "    if rels.shape[0] > 0:\n",
    "        if verbose:\n",
    "            print(\"Relations neighbor\", rels.shape)\n",
    "        Rr_idxs.append(torch.LongTensor([rels[:, 0], np.arange(rels.shape[0])]))\n",
    "        Rs_idxs.append(torch.LongTensor([rels[:, 1], np.arange(rels.shape[0])]))\n",
    "        # Ra: relation attributes\n",
    "    #    Ra = np.zeros((rels.shape[0], args.relation_dim))  \n",
    "        Ra = rels[:,2].reshape([-1,1])\n",
    "        Ras.append(torch.FloatTensor(Ra))\n",
    "        # values could be changed\n",
    "     #   values.append(torch.FloatTensor([1] * rels.shape[0]))\n",
    "      #  values.append(rels[:,2])\n",
    "        #### for hairs: values equals to the length of this segment\n",
    "        values.append(torch.FloatTensor(vals))\n",
    "        node_r_idxs.append(np.arange(n_particles))\n",
    "        node_s_idxs.append(np.arange(n_particles + n_shapes))\n",
    "        psteps.append(args.pstep)\n",
    "        \n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Attr shape (after hierarchy building):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "        print(\"Particle attr:\", np.sum(attr[:n_particles], axis=0))\n",
    "        print(\"Shape attr:\", np.sum(attr[n_particles:n_particles+n_shapes], axis=0))\n",
    "        print(\"Roots attr:\", np.sum(attr[n_particles+n_shapes:], axis=0))\n",
    "        \n",
    "        \n",
    "    ### normalize data\n",
    "    data = [positions, velocities]\n",
    "    data = normalize(data, stat, var)\n",
    "    positions, velocities = data[0], data[1]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Particle positions stats\")\n",
    "        print(positions.shape)\n",
    "        print(np.min(positions[:n_particles], 0))\n",
    "        print(np.max(positions[:n_particles], 0))\n",
    "        print(np.mean(positions[:n_particles], 0))\n",
    "        print(np.std(positions[:n_particles], 0))\n",
    "\n",
    "        show_vel_dim = 6 if args.env == 'RiceGrip' else 3\n",
    "        print(\"Velocities stats\")\n",
    "        print(velocities.shape)\n",
    "        print(np.mean(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        print(np.std(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        \n",
    "    state = torch.FloatTensor(np.concatenate([positions, velocities], axis=1))\n",
    "    attr = torch.FloatTensor(attr)\n",
    "    relations = [Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps]\n",
    "\n",
    "    return attr, state, relations, n_particles, n_shapes#, instance_idx\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {phase: PhysicsFleXDataset(\n",
    "    args, phase, phases_dict, args.verbose_data) for phase in ['train', 'valid']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['train'].load_data(args.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations neighbor (60, 3)\n",
      "Attr shape (after hierarchy building): (32, 1)\n",
      "Object attr: [1.]\n",
      "Particle attr: [0.]\n",
      "Shape attr: [1.]\n",
      "Roots attr: [0.]\n",
      "Particle positions stats\n",
      "(32, 3)\n",
      "[ 0.39366985 -1.84051862 -0.21294536]\n",
      "[ 0.39366985  1.72843141 -0.21294536]\n",
      "[ 0.39366985 -0.07940947 -0.21294536]\n",
      "[1.66533454e-16 1.06529481e+00 1.66533454e-16]\n",
      "Velocities stats\n",
      "(32, 3)\n",
      "[-0.08079876  3.02442785  0.01608018]\n",
      "[4.16333634e-17 1.54151227e+00 6.93889390e-18]\n"
     ]
    }
   ],
   "source": [
    "data = datasets['train'].__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr, state, rels, n_particles, n_shapes, label = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "           18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,  1,  2,  3,  4,  5,  6,\n",
       "            7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "           25, 26, 27, 28, 29, 30],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "           18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "           36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "           54, 55, 56, 57, 58, 59]])],\n",
       " [tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,  0,  1,  2,  3,  4,  5,\n",
       "            6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
       "           24, 25, 26, 27, 28, 29],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "           18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "           36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "           54, 55, 56, 57, 58, 59]])],\n",
       " [tensor([0.1146, 0.1135, 0.1172, 0.1160, 0.1160, 0.1156, 0.1153, 0.1150, 0.1147,\n",
       "          0.1143, 0.1140, 0.1136, 0.1132, 0.1129, 0.1125, 0.1121, 0.1118, 0.1114,\n",
       "          0.1111, 0.1107, 0.1104, 0.1101, 0.1097, 0.1094, 0.1091, 0.1088, 0.1085,\n",
       "          0.1082, 0.1079, 0.1077, 0.1146, 0.1135, 0.1172, 0.1160, 0.1160, 0.1156,\n",
       "          0.1153, 0.1150, 0.1147, 0.1143, 0.1140, 0.1136, 0.1132, 0.1129, 0.1125,\n",
       "          0.1121, 0.1118, 0.1114, 0.1111, 0.1107, 0.1104, 0.1101, 0.1097, 0.1094,\n",
       "          0.1091, 0.1088, 0.1085, 0.1082, 0.1079, 0.1077])],\n",
       " [tensor([[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]])],\n",
       " [array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "         17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30])],\n",
       " [array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "         17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])],\n",
       " [2]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {x: torch.utils.data.DataLoader(\n",
    "    datasets[x], batch_size=args.batch_size,\n",
    "    shuffle=True if x == 'train' else False,\n",
    "    num_workers=args.num_workers,\n",
    "    collate_fn=collate_fn)\n",
    "    for x in ['train', 'valid']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relations neighborRelations neighbor Relations neighbor (60, 3) \n",
      "(60, 3)(60, 3)Attr shape (after hierarchy building):\n",
      "\n",
      "Attr shape (after hierarchy building): Attr shape (after hierarchy building): (32, 1)(32, 1)\n",
      "\n",
      "Object attr: [1.]\n",
      "Particle attr: [0.]\n",
      "Shape attr: [1.]\n",
      "Roots attr: [0.]\n",
      "Particle positions stats Object attr:(32, 1) \n",
      "\n",
      "(32, 3)\n",
      "[1.][-1.74801819 -1.60031738 -0.29082427]\n",
      "\n",
      "Object attr: [1.]\n",
      "Particle attr: [0.]\n",
      "Shape attr:Particle attr:[0.39366985 1.72843141 2.02063993]\n",
      " [-0.92047091  0.06530966  0.76834979][1.] \n",
      "\n",
      "[0.]Roots attr:\n",
      "[0.77081759 0.9801186  0.65969195] Shape attr:[0.]\n",
      " Velocities stats\n",
      "\n",
      "(32, 3)[1.]Particle positions stats\n",
      "\n",
      "Roots attr:(32, 3) \n",
      "[ 0.39366985 -1.66066061 -0.21294536][0.]\n",
      "\n",
      "[ 0.39366985  1.72843141 -0.21294536]\n",
      "Particle positions stats[ 0.39366985  0.03547397 -0.21294536]\n",
      "\n",
      "\n",
      "[-1.16159317  0.43273923  0.00167318](32, 3)\n",
      "\n",
      "[1.66533454e-16 1.00903497e+00 1.66533454e-16]\n",
      "[ 0.39366985 -1.68000499 -0.21294536][1.87052081 0.39230342 0.63644768]Velocities stats\n",
      "\n",
      "\n",
      "(32, 3)\n",
      "[ 0.39366985  1.72843141 -0.21294536][-0.08079876  0.50673131  0.01608018]\n",
      "[4.16333634e-17 2.79565215e-01 6.93889390e-18]\n",
      "\n",
      "[ 0.39366985  0.02334509 -0.21294536]\n",
      "[1.66533454e-16 1.01513909e+00 1.66533454e-16]\n",
      "Velocities stats\n",
      "(32, 3)\n",
      "[-0.08079876  0.07640293  0.01608018]Relations neighbor\n",
      "[4.16333634e-17 5.93291955e-02 6.93889390e-18]\n",
      "Relations neighbor Relations neighbor (60, 3)\n",
      "Attr shape (after hierarchy building): (32, 1)\n",
      "Object attr: [1.]\n",
      "Particle attr: [0.]\n",
      "Shape attr: [1.]\n",
      "Roots attr: [0.]\n",
      "Particle positions stats\n",
      "(32, 3)\n",
      "[ 0.39366985 -1.67165867 -0.21294536] (60, 3)\n",
      "(60, 3)\n",
      "[ 0.39366985  1.72843141 -0.21294536]\n",
      "Attr shape (after hierarchy building):Attr shape (after hierarchy building): \n",
      "(32, 1) [ 0.39366985  0.02895185 -0.21294536](32, 1)\n",
      "\n",
      "\n",
      "Object attr:Object attr:[1.66533454e-16 1.01259422e+00 1.66533454e-16]  \n",
      "[1.][1.]\n",
      "Particle attr:\n",
      " Particle attr:[0.] \n",
      "[0.]Shape attr:\n",
      " [1.]Shape attr:Velocities stats \n",
      "\n",
      "[1.]\n",
      "Roots attr:(32, 3)\n",
      " Roots attr: [0.][-0.08079876  2.19548322  0.01608018]\n",
      "[0.]\n",
      "Particle positions stats\n",
      "\n",
      "[4.16333634e-17 1.15097365e+00 6.93889390e-18]Particle positions stats\n",
      "(32, 3)\n",
      "(32, 3)\n",
      "\n",
      "[ 0.39366985 -1.90819842 -0.21294536]\n",
      "[ 0.39366985 -1.80472323 -0.21294536][ 0.39366985  1.72843141 -0.21294536]\n",
      "\n",
      "[ 0.39366985  1.72843141 -0.21294536]\n",
      "[ 0.39366985 -0.1190164  -0.21294536][ 0.39366985 -0.0544469  -0.21294536]\n",
      "\n",
      "[1.66533454e-16 1.08743594e+00 1.66533454e-16]\n",
      "Velocities stats\n",
      "(32, 3)[1.66533454e-16 1.05463819e+00 1.66533454e-16]\n",
      "[-0.08079876  0.87675079  0.01608018]\n",
      "\n",
      "Velocities stats\n",
      "[4.16333634e-17 4.72057248e-01 6.93889390e-18]\n",
      "(32, 3)\n",
      "[-0.08079876  1.20746703  0.01608018]\n",
      "[4.16333634e-17 6.41299702e-01 6.93889390e-18]\n"
     ]
    }
   ],
   "source": [
    "phase = 'train'\n",
    "for i, data in enumerate(dataloaders[phase]):\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr, state, rels, n_particles, n_shapes, label = data\n",
    "Ra, node_r_idx, node_s_idx, pstep = rels[3], rels[4], rels[5], rels[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rr, Rs = [], []\n",
    "for j in range(len(rels[0])):\n",
    "    Rr_idx, Rs_idx, values = rels[0][j], rels[1][j], rels[2][j]\n",
    "    Rr.append(torch.sparse.FloatTensor(Rr_idx, values, torch.Size([node_r_idx[j].shape[0], Ra[j].size(0)])))\n",
    "    Rs.append(torch.sparse.FloatTensor(Rs_idx, values, torch.Size([node_s_idx[j].shape[0], Ra[j].size(0)])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [attr, state, Rr, Rs, Ra, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_idx = [0,31]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predicted = model(\n",
    "                    attr, state, Rr, Rs, Ra, n_particles,\n",
    "                    node_r_idx, node_s_idx, pstep,\n",
    "                    instance_idx, phases_dict, args.verbose_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 615210\n"
     ]
    }
   ],
   "source": [
    "args.pstep = 1\n",
    "pstep = 1\n",
    "use_gpu = False\n",
    "model = DPINet(args, datasets['train'].stat, phases_dict, residual=True, use_gpu=use_gpu)\n",
    "print(\"Number of parameters: %d\" % count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion\n",
    "criterionMSE = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, betas=(args.beta1, 0.999))\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.8, patience=3, verbose=True)\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    criterionMSE = criterionMSE.cuda()\n",
    "\n",
    "st_epoch = args.resume_epoch if args.resume_epoch > 0 else 0\n",
    "best_valid_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DPINet(\n",
       "  (particle_encoder_list): ModuleList(\n",
       "    (0): ParticleEncoder(\n",
       "      (model): Sequential(\n",
       "        (0): Linear(in_features=13, out_features=200, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (relation_encoder_list): ModuleList(\n",
       "    (0): RelationEncoder(\n",
       "      (model): Sequential(\n",
       "        (0): Linear(in_features=27, out_features=300, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=300, out_features=300, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=300, out_features=300, bias=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (relation_propagator_list): ModuleList(\n",
       "    (0): Propagator(\n",
       "      (linear): Linear(in_features=700, out_features=200, bias=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (particle_propagator_list): ModuleList(\n",
       "    (0): Propagator(\n",
       "      (linear): Linear(in_features=400, out_features=200, bias=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (rigid_particle_predictor): ParticlePredictor(\n",
       "    (linear_0): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (linear_1): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (linear_2): Linear(in_features=200, out_features=7, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (fluid_particle_predictor): ParticlePredictor(\n",
       "    (linear_0): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (linear_1): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (linear_2): Linear(in_features=200, out_features=3, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(phase=='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pstep = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stage 0\n",
      "Rrp tensor(indices=tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
      "                        14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
      "                        28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41,\n",
      "                        42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
      "                        56, 57, 58, 59],\n",
      "                       [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
      "                        14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
      "                        28, 29,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,\n",
      "                        13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
      "                        27, 28, 29, 30]]),\n",
      "       values=tensor([0.1108, 0.1102, 0.1119, 0.1113, 0.1112, 0.1110, 0.1109,\n",
      "                      0.1107, 0.1106, 0.1104, 0.1103, 0.1101, 0.1099, 0.1098,\n",
      "                      0.1096, 0.1095, 0.1094, 0.1092, 0.1090, 0.1096, 0.1095,\n",
      "                      0.1086, 0.1085, 0.1084, 0.1082, 0.1081, 0.1081, 0.1081,\n",
      "                      0.1078, 0.1077, 0.1108, 0.1102, 0.1119, 0.1113, 0.1112,\n",
      "                      0.1110, 0.1109, 0.1107, 0.1106, 0.1104, 0.1103, 0.1101,\n",
      "                      0.1099, 0.1098, 0.1096, 0.1095, 0.1094, 0.1092, 0.1090,\n",
      "                      0.1096, 0.1095, 0.1086, 0.1085, 0.1084, 0.1082, 0.1081,\n",
      "                      0.1081, 0.1081, 0.1078, 0.1077]),\n",
      "       size=(60, 31), nnz=60, layout=torch.sparse_coo)\n",
      "attr_r torch.Size([31, 7]) state_r torch.Size([31, 6])\n",
      "attr_r_rel torch.Size([60, 7]) attr_s_rel torch.Size([60, 7]) state_r_rel torch.Size([60, 6]) state_s_rel torch.Size([60, 6]) Ra[s] torch.Size([60, 1])\n",
      "relation encode: torch.Size([60, 300])\n",
      "pstep 0\n",
      "Receiver index range 0 30\n",
      "Sender index range 0 31\n",
      "relation effect: torch.Size([60, 200])\n",
      "particle effect: torch.Size([31, 200])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "There were no tensor arguments to this function (e.g., you passed an empty list of Tensors), but no fallback function is registered for schema aten::_cat.  This usually means that this function requires a non-empty list of Tensors.  Available functions are [CPU, CUDA, QuantizedCPU, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCPU: registered at /pytorch/build/aten/src/ATen/CPUType.cpp:2127 [kernel]\nCUDA: registered at /pytorch/build/aten/src/ATen/CUDAType.cpp:2983 [kernel]\nQuantizedCPU: registered at /pytorch/build/aten/src/ATen/QuantizedCPUType.cpp:297 [kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nAutogradOther: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:8078 [autograd kernel]\nAutogradCPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:8078 [autograd kernel]\nAutogradCUDA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:8078 [autograd kernel]\nAutogradXLA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:8078 [autograd kernel]\nAutogradPrivateUse1: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:8078 [autograd kernel]\nAutogradPrivateUse2: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:8078 [autograd kernel]\nAutogradPrivateUse3: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:8078 [autograd kernel]\nTracer: registered at /pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:9654 [kernel]\nAutocast: registered at /pytorch/aten/src/ATen/autocast_mode.cpp:258 [kernel]\nBatched: registered at /pytorch/aten/src/ATen/BatchingRegistrations.cpp:511 [backend fallback]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-019f2e1f5a2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_particles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mnode_r_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_s_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     instance_idx, phases_dict, args.verbose_model)\n\u001b[0m",
      "\u001b[0;32m/home/onepear/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/onepear/Documents/DPI-Net/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, attr, state, Rr, Rs, Ra, n_particles, node_r_idx, node_s_idx, pstep, instance_idx, phases_dict, verbose)\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfluid_particle_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticle_effect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0med\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: There were no tensor arguments to this function (e.g., you passed an empty list of Tensors), but no fallback function is registered for schema aten::_cat.  This usually means that this function requires a non-empty list of Tensors.  Available functions are [CPU, CUDA, QuantizedCPU, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCPU: registered at /pytorch/build/aten/src/ATen/CPUType.cpp:2127 [kernel]\nCUDA: registered at /pytorch/build/aten/src/ATen/CUDAType.cpp:2983 [kernel]\nQuantizedCPU: registered at /pytorch/build/aten/src/ATen/QuantizedCPUType.cpp:297 [kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nAutogradOther: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:8078 [autograd kernel]\nAutogradCPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:8078 [autograd kernel]\nAutogradCUDA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:8078 [autograd kernel]\nAutogradXLA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:8078 [autograd kernel]\nAutogradPrivateUse1: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:8078 [autograd kernel]\nAutogradPrivateUse2: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:8078 [autograd kernel]\nAutogradPrivateUse3: registered at /pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:8078 [autograd kernel]\nTracer: registered at /pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:9654 [kernel]\nAutocast: registered at /pytorch/aten/src/ATen/autocast_mode.cpp:258 [kernel]\nBatched: registered at /pytorch/aten/src/ATen/BatchingRegistrations.cpp:511 [backend fallback]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "predicted = model(\n",
    "                    attr, state, Rr, Rs, Ra, n_particles,\n",
    "                    node_r_idx, node_s_idx, pstep,\n",
    "                    instance_idx, phases_dict, args.verbose_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPINet(nn.Module):\n",
    "    def __init__(self, args, stat, phases_dict, residual=False, use_gpu=False):\n",
    "        super(DPINet, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        state_dim = args.state_dim\n",
    "        attr_dim = args.attr_dim\n",
    "        relation_dim = args.relation_dim\n",
    "        nf_particle = 20#args.nf_particle\n",
    "        nf_relation = 30 #args.nf_relation\n",
    "        nf_effect = 30 #args.nf_effect\n",
    "\n",
    "        self.nf_effect = nf_effect\n",
    "\n",
    "        self.stat = stat\n",
    "        self.use_gpu = use_gpu\n",
    "        self.residual = residual \n",
    "\n",
    "        # (1) particle attr (2) state\n",
    "        self.particle_encoder = ParticleEncoder(attr_dim + state_dim, nf_particle, nf_effect)\n",
    "\n",
    "        # (1) sender attr (2) receiver attr (3) state receiver (4) state_diff (5) relation attr\n",
    "        self.relation_encoder = RelationEncoder( 2 * attr_dim + 2 * state_dim + relation_dim, nf_relation, nf_relation)\n",
    "\n",
    "        # (1) relation encode (2) sender effect (3) receiver effect\n",
    "        self.relation_propagator = Propagator(nf_relation + 2 * nf_effect, nf_effect)\n",
    "\n",
    "        # (1) particle encode (2) particle effect\n",
    "        self.particle_propagator = Propagator(2 * nf_effect, nf_effect)\n",
    "\n",
    "        # (1) set particle effect\n",
    "        self.particle_predictor = ParticlePredictor(nf_effect, nf_effect, args.position_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, attr, state, Rr, Rs, Ra, n_particles, node_r_idx, node_s_idx, pstep,\n",
    "                instance_idx, phases_dict, verbose=0):\n",
    "        \n",
    "        # calculate particle encoding\n",
    "        if self.use_gpu:\n",
    "            particle_effect = Variable(torch.zeros((attr.size(0), self.nf_effect)).cuda())\n",
    "        else:\n",
    "            particle_effect = Variable(torch.zeros((attr.size(0), self.nf_effect)))\n",
    "            \n",
    "        s = 0\n",
    "        Rrp = Rr[s].t()\n",
    "        Rsp = Rs[s].t()\n",
    "\n",
    "        # receiver_attr, sender_attr\n",
    "        attr_r = attr[node_r_idx[s]]\n",
    "        attr_s = attr[node_s_idx[s]]\n",
    "        attr_r_rel = Rrp.mm(attr_r)\n",
    "        attr_s_rel = Rsp.mm(attr_s)\n",
    "\n",
    "        # receiver_state, sender_state\n",
    "        state_r = state[node_r_idx[s]]\n",
    "        state_s = state[node_s_idx[s]]\n",
    "        state_r_rel = Rrp.mm(state_r)\n",
    "        state_s_rel = Rsp.mm(state_s)\n",
    "        state_diff = state_r_rel - state_s_rel\n",
    "        \n",
    "        particle_encode = self.particle_encoder(torch.cat([attr_r, state_r], 1))\n",
    "        relation_encode = self.relation_encoder(\n",
    "                torch.cat([attr_r_rel, attr_s_rel, state_r_rel, state_s_rel, Ra[s]], 1))\n",
    "        \n",
    "        for i in range(pstep):\n",
    "            effect_p_r = particle_effect[node_r_idx[s]]\n",
    "            effect_p_s = particle_effect[node_s_idx[s]]\n",
    "\n",
    "            receiver_effect = Rrp.mm(effect_p_r)\n",
    "            sender_effect = Rsp.mm(effect_p_s)\n",
    "            \n",
    "\n",
    "            # calculate relation effect\n",
    "            effect_rel = self.relation_propagator(\n",
    "                torch.cat([relation_encode, receiver_effect, sender_effect], 1))\n",
    "\n",
    "            # calculate particle effect by aggregating relation effect\n",
    "            effect_p_r_agg = Rr[s].mm(effect_rel)\n",
    "\n",
    "            # calculate particle effect\n",
    "            effect_p = self.particle_propagator(\n",
    "                torch.cat([particle_encode, effect_p_r_agg], 1),\n",
    "                res=effect_p_r)\n",
    "            particle_effect[node_r_idx[s]] = effect_p\n",
    "            \n",
    "        pred = self.particle_predictor(particle_effect[:31])\n",
    "        return pred\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 9643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DPINet(\n",
       "  (particle_encoder): ParticleEncoder(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=7, out_features=20, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=20, out_features=30, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (relation_encoder): RelationEncoder(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=15, out_features=30, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=30, out_features=30, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=30, out_features=30, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (relation_propagator): Propagator(\n",
       "    (linear): Linear(in_features=90, out_features=30, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (particle_propagator): Propagator(\n",
       "    (linear): Linear(in_features=60, out_features=30, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (particle_predictor): ParticlePredictor(\n",
       "    (linear_0): Linear(in_features=30, out_features=30, bias=True)\n",
       "    (linear_1): Linear(in_features=30, out_features=30, bias=True)\n",
       "    (linear_2): Linear(in_features=30, out_features=3, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.pstep = 3\n",
    "pstep = 3\n",
    "use_gpu = False\n",
    "model = DPINet(args, datasets['train'].stat, phases_dict, residual=True, use_gpu=use_gpu)\n",
    "print(\"Number of parameters: %d\" % count_parameters(model))\n",
    "# criterion\n",
    "criterionMSE = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, betas=(args.beta1, 0.999))\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.8, patience=3, verbose=True)\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    criterionMSE = criterionMSE.cuda()\n",
    "\n",
    "st_epoch = args.resume_epoch if args.resume_epoch > 0 else 0\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "model.train(phase=='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model(attr, state, Rr, Rs, Ra, n_particles,\n",
    "                    node_r_idx, node_s_idx, pstep,\n",
    "                    instance_idx, phases_dict, args.verbose_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0072,  0.0273, -0.0420],\n",
       "        [ 0.0068,  0.0268, -0.0411],\n",
       "        [ 0.0065,  0.0271, -0.0418],\n",
       "        [ 0.0064,  0.0276, -0.0431],\n",
       "        [ 0.0064,  0.0278, -0.0439],\n",
       "        [ 0.0064,  0.0278, -0.0441],\n",
       "        [ 0.0066,  0.0279, -0.0442],\n",
       "        [ 0.0068,  0.0279, -0.0442],\n",
       "        [ 0.0068,  0.0278, -0.0443],\n",
       "        [ 0.0061,  0.0274, -0.0453],\n",
       "        [ 0.0059,  0.0273, -0.0457],\n",
       "        [ 0.0067,  0.0280, -0.0455],\n",
       "        [ 0.0078,  0.0304, -0.0454],\n",
       "        [ 0.0079,  0.0309, -0.0446],\n",
       "        [ 0.0075,  0.0293, -0.0451],\n",
       "        [ 0.0064,  0.0278, -0.0451],\n",
       "        [ 0.0060,  0.0273, -0.0454],\n",
       "        [ 0.0066,  0.0287, -0.0448],\n",
       "        [ 0.0076,  0.0302, -0.0436],\n",
       "        [ 0.0077,  0.0293, -0.0438],\n",
       "        [ 0.0081,  0.0286, -0.0438],\n",
       "        [ 0.0080,  0.0284, -0.0435],\n",
       "        [ 0.0081,  0.0284, -0.0427],\n",
       "        [ 0.0090,  0.0290, -0.0418],\n",
       "        [ 0.0113,  0.0303, -0.0406],\n",
       "        [ 0.0124,  0.0308, -0.0402],\n",
       "        [ 0.0123,  0.0302, -0.0391],\n",
       "        [ 0.0124,  0.0297, -0.0390],\n",
       "        [ 0.0107,  0.0286, -0.0395],\n",
       "        [ 0.0106,  0.0274, -0.0382],\n",
       "        [ 0.0125,  0.0285, -0.0375]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'train'\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train(phase=='train')\n",
    "\n",
    "    losses = 0.\n",
    "    for i, data in enumerate(dataloaders[phase]):\n",
    "#         print ('i:',i)\n",
    "\n",
    "        attr, state, rels, n_particles, n_shapes, label = data\n",
    "        Ra, node_r_idx, node_s_idx, pstep = rels[3], rels[4], rels[5], rels[6]\n",
    "\n",
    "        Rr, Rs = [], []\n",
    "        for j in range(len(rels[0])):\n",
    "            Rr_idx, Rs_idx, values = rels[0][j], rels[1][j], rels[2][j]\n",
    "            Rr.append(torch.sparse.FloatTensor(\n",
    "                Rr_idx, values, torch.Size([node_r_idx[j].shape[0], Ra[j].size(0)])))\n",
    "            Rs.append(torch.sparse.FloatTensor(\n",
    "                Rs_idx, values, torch.Size([node_s_idx[j].shape[0], Ra[j].size(0)])))\n",
    "\n",
    "        data = [attr, state, Rr, Rs, Ra, label]\n",
    "        instance_idx = [0, 31]\n",
    "\n",
    "            # st_time = time.time()\n",
    "        pstep = 3\n",
    "        predicted = model(\n",
    "            attr, state, Rr, Rs, Ra, n_particles,\n",
    "            node_r_idx, node_s_idx, pstep,\n",
    "            instance_idx, phases_dict, 0)\n",
    "            # print('Time forward', time.time() - st_time)\n",
    "\n",
    "       #     print(predicted.shape)\n",
    "       #     print(label.shape)\n",
    "\n",
    "        loss = criterionMSE(predicted, label)\n",
    "   #     print (\"loss\",loss)\n",
    "        losses += np.sqrt(loss.item())\n",
    "\n",
    "        if phase == 'train':\n",
    "            if i % 1 == 0:\n",
    "                # update parameters every args.forward_times\n",
    "          #      print ('update!')\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "          #      print ('done')\n",
    "\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            n_relations = 0\n",
    "            for j in range(len(Ra)):\n",
    "                n_relations += Ra[j].size(0)\n",
    "            print('%s [%d/%d][%d/%d] n_relations: %d, Loss: %.6f, Agg: %.6f' %\n",
    "                  (phase, epoch, args.n_epoch, i, len(dataloaders[phase]),\n",
    "                   n_relations, np.sqrt(loss.item()), losses / (i + 1)))\n",
    "\n",
    "     #   if phase == 'train' and i > 0 and i % args.ckp_per_iter == 0:\n",
    "     #       torch.save(model.state_dict(), '%s/net_epoch_%d_iter_%d.pth' % (args.outf, epoch, i))\n",
    "\n",
    "    losses /= len(dataloaders[phase])\n",
    "    print('%s [%d/%d] Loss: %.4f, Best valid: %.4f' %\n",
    "          (phase, epoch, args.n_epoch, losses, best_valid_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, attr, state, Rr, Rs, Ra, n_particles, node_r_idx, node_s_idx, pstep,\n",
    "                instance_idx, phases_dict, verbose=0):\n",
    "\n",
    "        # calculate particle encoding\n",
    "        if self.use_gpu:\n",
    "            particle_effect = Variable(torch.zeros((attr.size(0), self.nf_effect)).cuda())\n",
    "        else:\n",
    "            particle_effect = Variable(torch.zeros((attr.size(0), self.nf_effect)))\n",
    "\n",
    "        # add offset to center-of-mass for rigids to attr\n",
    "        if self.use_gpu:\n",
    "            offset = Variable(torch.zeros((attr.size(0), state.size(1))).cuda())\n",
    "        else:\n",
    "            offset = Variable(torch.zeros((attr.size(0), state.size(1))))\n",
    "\n",
    "        for i in range(len(instance_idx) - 1):\n",
    "            st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "            if phases_dict['material'][i] == 'rigid':\n",
    "                c = torch.mean(state[st:ed], dim=0)\n",
    "                offset[st:ed] = state[st:ed] - c\n",
    "        attr = torch.cat([attr, offset], 1)\n",
    "\n",
    "        n_stage = len(Rr)\n",
    "        for s in range(n_stage):\n",
    "            if verbose:\n",
    "                print(\"=== Stage\", s)\n",
    "            Rrp = Rr[s].t()\n",
    "            Rsp = Rs[s].t()\n",
    "\n",
    "            if verbose:\n",
    "                print ('Rrp',Rrp)\n",
    "\n",
    "            # receiver_attr, sender_attr\n",
    "            attr_r = attr[node_r_idx[s]]\n",
    "            attr_s = attr[node_s_idx[s]]\n",
    "            attr_r_rel = Rrp.mm(attr_r)\n",
    "            attr_s_rel = Rsp.mm(attr_s)\n",
    "\n",
    "            # receiver_state, sender_state\n",
    "            state_r = state[node_r_idx[s]]\n",
    "            state_s = state[node_s_idx[s]]\n",
    "            state_r_rel = Rrp.mm(state_r)\n",
    "            state_s_rel = Rsp.mm(state_s)\n",
    "            state_diff = state_r_rel - state_s_rel\n",
    "\n",
    "            # particle encode\n",
    "            if verbose:\n",
    "                print('attr_r', attr_r.shape, 'state_r', state_r.shape)\n",
    "            particle_encode = self.particle_encoder_list[s](torch.cat([attr_r, state_r], 1))\n",
    "\n",
    "            # calculate relation encoding\n",
    "            relation_encode = self.relation_encoder_list[s](\n",
    "                torch.cat([attr_r_rel, attr_s_rel, state_r_rel, state_s_rel, Ra[s]], 1))\n",
    "            if verbose:\n",
    "                print ('attr_r_rel', attr_r_rel.shape, 'attr_s_rel', attr_s_rel.shape, 'state_r_rel', state_r_rel.shape, 'state_s_rel', state_s_rel.shape, 'Ra[s]',  Ra[s].shape)\n",
    "                print(\"relation encode:\", relation_encode.size())\n",
    "\n",
    "            for i in range(pstep[s]):\n",
    "                if verbose:\n",
    "                    print(\"pstep\", i)\n",
    "                    print(\"Receiver index range\", np.min(node_r_idx[s]), np.max(node_r_idx[s]))\n",
    "                    print(\"Sender index range\", np.min(node_s_idx[s]), np.max(node_s_idx[s]))\n",
    "\n",
    "                effect_p_r = particle_effect[node_r_idx[s]]\n",
    "                effect_p_s = particle_effect[node_s_idx[s]]\n",
    "\n",
    "                receiver_effect = Rrp.mm(effect_p_r)\n",
    "                sender_effect = Rsp.mm(effect_p_s)\n",
    "\n",
    "                # calculate relation effect\n",
    "                effect_rel = self.relation_propagator_list[s](\n",
    "                    torch.cat([relation_encode, receiver_effect, sender_effect], 1))\n",
    "                if verbose:\n",
    "                    print(\"relation effect:\", effect_rel.size())\n",
    "\n",
    "                # calculate particle effect by aggregating relation effect\n",
    "                effect_p_r_agg = Rr[s].mm(effect_rel)\n",
    "\n",
    "                # calculate particle effect\n",
    "                effect_p = self.particle_propagator_list[s](\n",
    "                    torch.cat([particle_encode, effect_p_r_agg], 1),\n",
    "                    res=effect_p_r)\n",
    "                if verbose:\n",
    "                    print(\"particle effect:\", effect_p.size())\n",
    "\n",
    "                particle_effect[node_r_idx[s]] = effect_p\n",
    "\n",
    "        pred = []\n",
    "        for i in range(len(instance_idx) - 1):\n",
    "            st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "\n",
    "            if phases_dict['material'][i] == 'rigid':\n",
    "                t = self.rigid_particle_predictor(torch.mean(particle_effect[st:ed], 0)).view(-1)\n",
    "\n",
    "                R = self.rotation_matrix_from_quaternion(t[:4])\n",
    "                b = t[4:] * self.std_p\n",
    "\n",
    "                p_0 = state[st:ed, :3] * self.std_p + self.mean_p\n",
    "                c = torch.mean(p_0, dim=0)\n",
    "                p_1 = torch.mm(p_0 - c, R) + b + c\n",
    "                v = (p_1 - p_0) / self.dt\n",
    "                pred.append((v - self.mean_v) / self.std_v)\n",
    "\n",
    "            elif phases_dict['material'][i] == 'fluid':\n",
    "                pred.append(self.fluid_particle_predictor(particle_effect[st:ed]))\n",
    "\n",
    "        pred = torch.cat(pred, 0)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"pred:\", pred.size())\n",
    "\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "\n",
    "attr_dim = args.attr_dim\n",
    "state_dim = args.state_dim\n",
    "relation_dim = args.relation_dim\n",
    "nf_particle = args.nf_particle\n",
    "nf_relation = 30 #args.nf_relation\n",
    "nf_effect = 20 #args.nf_effect\n",
    "\n",
    "\n",
    "\n",
    "# (1) particle attr (2) state\n",
    "particle_encoder = ParticleEncoder(attr_dim + state_dim, nf_particle, nf_effect)\n",
    "\n",
    "# (1) sender attr (2) receiver attr (3) state receiver (4) state_diff (5) relation attr\n",
    "relation_encoder = RelationEncoder( 2 * attr_dim + 2 * state_dim + relation_dim, nf_relation, nf_relation)\n",
    "\n",
    "# (1) relation encode (2) sender effect (3) receiver effect\n",
    "relation_propagator = Propagator(nf_relation + 2 * nf_effect, nf_effect)\n",
    "\n",
    "# (1) particle encode (2) particle effect\n",
    "particle_propagator = Propagator(2 * nf_effect, nf_effect)\n",
    "\n",
    "# (1) set particle effect\n",
    "rigid_particle_predictor = ParticlePredictor(nf_effect, nf_effect, 7)  # predict rigid motion\n",
    "fluid_particle_predictor = ParticlePredictor(nf_effect, nf_effect, args.position_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * attr_dim + 2* state_dim + relation_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ra[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rr, Rs = [], []\n",
    "Rrv, Rsv = [], []\n",
    "j = 0\n",
    "Rr_idx, Rs_idx, values = rels[0][j], rels[1][j], rels[2][j]\n",
    "V = torch.ones(values.shape)\n",
    "Rr.append(torch.sparse.FloatTensor(Rr_idx, V, torch.Size([node_r_idx[j].shape[0], Ra[j].size(0)])))\n",
    "Rs.append(torch.sparse.FloatTensor(Rs_idx, V, torch.Size([node_s_idx[j].shape[0], Ra[j].size(0)])))\n",
    "Rrv.append(torch.sparse.FloatTensor(Rr_idx, values, torch.Size([node_r_idx[j].shape[0], Ra[j].size(0)])))\n",
    "Rsv.append(torch.sparse.FloatTensor(Rs_idx, values, torch.Size([node_s_idx[j].shape[0], Ra[j].size(0)])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 60])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rr[0].mm(values.reshape([60,1])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,  0,  1,  2,  3,  4,  5,\n",
       "          6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
       "         24, 25, 26, 27, 28, 29],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "         54, 55, 56, 57, 58, 59]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rs_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,  1,  2,  3,  4,  5,  6,\n",
       "          7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "         25, 26, 27, 28, 29, 30],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "         54, 55, 56, 57, 58, 59]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rr_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "Rrp = Rr[s].t()\n",
    "Rsp = Rs[s].t()\n",
    "\n",
    "# receiver_attr, sender_attr\n",
    "attr_r = attr[node_r_idx[s]]\n",
    "attr_s = attr[node_s_idx[s]]\n",
    "attr_r_rel = Rrp.mm(attr_r)\n",
    "attr_s_rel = Rsp.mm(attr_s)\n",
    "\n",
    "# receiver_state, sender_state\n",
    "state_r = state[node_r_idx[s]]\n",
    "state_s = state[node_s_idx[s]]\n",
    "state_r_rel = Rrp.mm(state_r)\n",
    "state_s_rel = Rsp.mm(state_s)\n",
    "state_diff = state_r_rel - state_s_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "                        14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "                        28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41,\n",
       "                        42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
       "                        56, 57, 58, 59],\n",
       "                       [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "                        14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "                        28, 29,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,\n",
       "                        13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "                        27, 28, 29, 30]]),\n",
       "       values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.]),\n",
       "       size=(60, 31), nnz=60, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rrp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 6])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_s_rel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_s_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_r_rel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_r_idx[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 60])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rs[s].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_r_rel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "                        14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "                        28, 29,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,\n",
       "                        13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "                        27, 28, 29, 30],\n",
       "                       [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "                        14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "                        28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41,\n",
       "                        42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
       "                        56, 57, 58, 59]]),\n",
       "       values=tensor([0.1079, 0.1076, 0.1078, 0.1076, 0.1073, 0.1073, 0.1072,\n",
       "                      0.1071, 0.1070, 0.1070, 0.1069, 0.1069, 0.1069, 0.1068,\n",
       "                      0.1068, 0.1068, 0.1068, 0.1068, 0.1069, 0.1069, 0.1069,\n",
       "                      0.1070, 0.1070, 0.1070, 0.1071, 0.1072, 0.1073, 0.1073,\n",
       "                      0.1074, 0.1074, 0.1079, 0.1076, 0.1078, 0.1076, 0.1073,\n",
       "                      0.1073, 0.1072, 0.1071, 0.1070, 0.1070, 0.1069, 0.1069,\n",
       "                      0.1069, 0.1068, 0.1068, 0.1068, 0.1068, 0.1068, 0.1069,\n",
       "                      0.1069, 0.1069, 0.1070, 0.1070, 0.1070, 0.1071, 0.1072,\n",
       "                      0.1073, 0.1073, 0.1074, 0.1074]),\n",
       "       size=(31, 60), nnz=60, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rr[s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(attr_r.shape)\n",
    "attr_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(attr_s.shape)\n",
    "attr_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31, 1])\n",
      "torch.Size([31, 6])\n"
     ]
    }
   ],
   "source": [
    "print(attr_r.shape)\n",
    "print(state_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_encode = particle_encoder(torch.cat([attr_r, state_r], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 20])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particle_encode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_encode = relation_encoder(\n",
    "                torch.cat([attr_r_rel, attr_s_rel, state_r_rel, state_s_rel, Ra[s]], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 30])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_encode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "pstep = 3\n",
    "particle_effect = Variable(torch.zeros((attr.size(0), nf_effect)))\n",
    "\n",
    "effect_p_r = particle_effect[node_r_idx[s]]\n",
    "effect_p_s = particle_effect[node_s_idx[s]]\n",
    "\n",
    "receiver_effect = Rrp.mm(effect_p_r)\n",
    "sender_effect = Rsp.mm(effect_p_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0717, 0.0309, 0.0000,  ..., 0.0000, 0.0547, 0.0539],\n",
       "        [0.0718, 0.0312, 0.0000,  ..., 0.0000, 0.0537, 0.0550],\n",
       "        [0.0719, 0.0307, 0.0000,  ..., 0.0000, 0.0534, 0.0544],\n",
       "        ...,\n",
       "        [0.0711, 0.0317, 0.0000,  ..., 0.0000, 0.0551, 0.0545],\n",
       "        [0.0713, 0.0312, 0.0000,  ..., 0.0000, 0.0542, 0.0543],\n",
       "        [0.0722, 0.0307, 0.0000,  ..., 0.0000, 0.0530, 0.0540]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_propagator(\n",
    "        torch.cat([relation_encode, receiver_effect, sender_effect], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "pstep = 3\n",
    "particle_effect = Variable(torch.zeros((attr.size(0), nf_effect)))\n",
    "\n",
    "for i in range(pstep):\n",
    "    effect_p_r = particle_effect[node_r_idx[s]]\n",
    "    effect_p_s = particle_effect[node_s_idx[s]]\n",
    "\n",
    "    receiver_effect = Rrp.mm(effect_p_r)\n",
    "    sender_effect = Rsp.mm(effect_p_s)\n",
    "\n",
    "    # calculate relation effect\n",
    "    effect_rel = relation_propagator(\n",
    "        torch.cat([relation_encode, receiver_effect, sender_effect], 1))\n",
    "\n",
    "    # calculate particle effect by aggregating relation effect\n",
    "    effect_p_r_agg = Rr[s].mm(effect_rel)\n",
    "\n",
    "    # calculate particle effect\n",
    "    effect_p = particle_propagator(\n",
    "        torch.cat([particle_encode, effect_p_r_agg], 1),\n",
    "        res=effect_p_r)\n",
    "    particle_effect[node_r_idx[s]] = effect_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 3])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fluid_particle_predictor(particle_effect[:31]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0437,  0.1919, -0.0236, -0.0090, -0.0035,  0.0018,\n",
       "          0.0346,  0.1793, -0.0716, -0.0090, -0.0035,  0.0018,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0344,  0.1783, -0.0712, -0.0089, -0.0034,  0.0018,\n",
       "          0.0252,  0.1659, -0.0523, -0.0087, -0.0028, -0.1887,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0256,  0.1685, -0.0532, -0.0089, -0.0028, -0.1917,\n",
       "          0.0158,  0.1557, -0.0518, -0.0087, -0.0021, -0.3785,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0157,  0.1549, -0.0515, -0.0087, -0.0021, -0.3764,\n",
       "          0.0055,  0.1423, -0.0104, -0.0028, -0.0033, -0.0802,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0055,  0.1422, -0.0104, -0.0028, -0.0033, -0.0801,\n",
       "         -0.0053,  0.1297,  0.0124,  0.0016, -0.0040, -0.0664,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0053,  0.1295,  0.0124,  0.0016, -0.0040, -0.0662,\n",
       "         -0.0167,  0.1172,  0.0378,  0.0069, -0.0051, -0.0679,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0167,  0.1170,  0.0378,  0.0069, -0.0051, -0.0678,\n",
       "         -0.0289,  0.1048,  0.0651,  0.0138, -0.0069, -0.0250,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0289,  0.1046,  0.0650,  0.0137, -0.0069, -0.0249,\n",
       "         -0.0418,  0.0926,  0.0884,  0.0224, -0.0095,  0.0408,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0418,  0.0925,  0.0882,  0.0223, -0.0095,  0.0407,\n",
       "         -0.0556,  0.0806,  0.1024,  0.0323, -0.0129,  0.0956,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0555,  0.0805,  0.1023,  0.0322, -0.0129,  0.0955,\n",
       "         -0.0701,  0.0688,  0.1077,  0.0426, -0.0169,  0.1157,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0700,  0.0688,  0.1076,  0.0425, -0.0169,  0.1155,\n",
       "         -0.0855,  0.0573,  0.1183,  0.0522, -0.0209,  0.1332,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0853,  0.0572,  0.1182,  0.0521, -0.0209,  0.1330,\n",
       "         -0.1016,  0.0460,  0.1308,  0.0600, -0.0243,  0.1032,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.1015,  0.0459,  0.1307,  0.0599, -0.0243,  0.1031,\n",
       "         -0.1186,  0.0349,  0.1604,  0.0649, -0.0265,  0.1051,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.1185,  0.0348,  0.1602,  0.0648, -0.0264,  0.1049,\n",
       "         -0.1365,  0.0241,  0.1913,  0.0662, -0.0267,  0.0882,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.1363,  0.0240,  0.1910,  0.0662, -0.0267,  0.0881,\n",
       "         -0.1552,  0.0136,  0.2349,  0.0640, -0.0248,  0.0619,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.1551,  0.0135,  0.2346,  0.0640, -0.0247,  0.0618,\n",
       "         -0.1749,  0.0034,  0.2475,  0.0594, -0.0210,  0.1226,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.1747,  0.0034,  0.2472,  0.0593, -0.0210,  0.1225,\n",
       "         -0.1955, -0.0065,  0.2679,  0.0541, -0.0166, -0.1256,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.1953, -0.0065,  0.2677,  0.0540, -0.0166, -0.1255,\n",
       "         -0.2172, -0.0160,  0.2737,  0.0496, -0.0125, -0.0594,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2165, -0.0159,  0.2728,  0.0494, -0.0124, -0.0592,\n",
       "         -0.2386, -0.0252,  0.2466,  0.0454, -0.0086,  0.0212,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2412, -0.0255,  0.2492,  0.0459, -0.0087,  0.0214,\n",
       "         -0.2667, -0.0337,  0.2939,  0.0404, -0.0027, -0.0852,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2724, -0.0344,  0.3001,  0.0413, -0.0027, -0.0870,\n",
       "         -0.2847, -0.0470,  0.3210,  0.0398, -0.0028, -0.0827,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2775, -0.0458,  0.3129,  0.0388, -0.0027, -0.0806,\n",
       "         -0.2732, -0.0586,  0.3133,  0.0449, -0.0020,  0.0366,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2702, -0.0579,  0.3098,  0.0444, -0.0020,  0.0362,\n",
       "         -0.2665, -0.0703,  0.3252,  0.0787,  0.0015,  0.0435,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2668, -0.0704,  0.3255,  0.0787,  0.0015,  0.0435,\n",
       "         -0.2628, -0.0829,  0.3235,  0.1102,  0.0046,  0.1778,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2623, -0.0827,  0.3228,  0.1100,  0.0046,  0.1774,\n",
       "         -0.2578, -0.0951,  0.2730,  0.1542,  0.0097,  0.3000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2574, -0.0949,  0.2726,  0.1539,  0.0097,  0.2996,\n",
       "         -0.2536, -0.1073,  0.2174,  0.2129,  0.0146,  0.1433,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2534, -0.1072,  0.2173,  0.2128,  0.0146,  0.1432,\n",
       "         -0.2511, -0.1196,  0.1960,  0.2513,  0.0170,  0.0554,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2511, -0.1196,  0.1960,  0.2514,  0.0170,  0.0554,\n",
       "         -0.2537, -0.1321,  0.1742,  0.3367,  0.0096, -0.0378,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2531, -0.1318,  0.1738,  0.3360,  0.0096, -0.0377,\n",
       "         -0.2618, -0.1437,  0.2088,  0.4612, -0.0274,  0.0962,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2611, -0.1433,  0.2083,  0.4599, -0.0273,  0.0959,\n",
       "         -0.2746, -0.1545,  0.3643,  0.6127, -0.1033,  0.1481,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0346,  0.1793, -0.0716, -0.0090, -0.0035,  0.0018,\n",
       "          0.0437,  0.1919, -0.0236, -0.0090, -0.0035,  0.0018,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0252,  0.1659, -0.0523, -0.0087, -0.0028, -0.1887,\n",
       "          0.0344,  0.1783, -0.0712, -0.0089, -0.0034,  0.0018,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0158,  0.1557, -0.0518, -0.0087, -0.0021, -0.3785,\n",
       "          0.0256,  0.1685, -0.0532, -0.0089, -0.0028, -0.1917,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0055,  0.1423, -0.0104, -0.0028, -0.0033, -0.0802,\n",
       "          0.0157,  0.1549, -0.0515, -0.0087, -0.0021, -0.3764,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0053,  0.1297,  0.0124,  0.0016, -0.0040, -0.0664,\n",
       "          0.0055,  0.1422, -0.0104, -0.0028, -0.0033, -0.0801,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0167,  0.1172,  0.0378,  0.0069, -0.0051, -0.0679,\n",
       "         -0.0053,  0.1295,  0.0124,  0.0016, -0.0040, -0.0662,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0289,  0.1048,  0.0651,  0.0138, -0.0069, -0.0250,\n",
       "         -0.0167,  0.1170,  0.0378,  0.0069, -0.0051, -0.0678,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0418,  0.0926,  0.0884,  0.0224, -0.0095,  0.0408,\n",
       "         -0.0289,  0.1046,  0.0650,  0.0137, -0.0069, -0.0249,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0556,  0.0806,  0.1024,  0.0323, -0.0129,  0.0956,\n",
       "         -0.0418,  0.0925,  0.0882,  0.0223, -0.0095,  0.0407,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0701,  0.0688,  0.1077,  0.0426, -0.0169,  0.1157,\n",
       "         -0.0555,  0.0805,  0.1023,  0.0322, -0.0129,  0.0955,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0855,  0.0573,  0.1183,  0.0522, -0.0209,  0.1332,\n",
       "         -0.0700,  0.0688,  0.1076,  0.0425, -0.0169,  0.1155,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.1016,  0.0460,  0.1308,  0.0600, -0.0243,  0.1032,\n",
       "         -0.0853,  0.0572,  0.1182,  0.0521, -0.0209,  0.1330,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.1186,  0.0349,  0.1604,  0.0649, -0.0265,  0.1051,\n",
       "         -0.1015,  0.0459,  0.1307,  0.0599, -0.0243,  0.1031,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.1365,  0.0241,  0.1913,  0.0662, -0.0267,  0.0882,\n",
       "         -0.1185,  0.0348,  0.1602,  0.0648, -0.0264,  0.1049,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.1552,  0.0136,  0.2349,  0.0640, -0.0248,  0.0619,\n",
       "         -0.1363,  0.0240,  0.1910,  0.0662, -0.0267,  0.0881,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.1749,  0.0034,  0.2475,  0.0594, -0.0210,  0.1226,\n",
       "         -0.1551,  0.0135,  0.2346,  0.0640, -0.0247,  0.0618,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.1955, -0.0065,  0.2679,  0.0541, -0.0166, -0.1256,\n",
       "         -0.1747,  0.0034,  0.2472,  0.0593, -0.0210,  0.1225,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2172, -0.0160,  0.2737,  0.0496, -0.0125, -0.0594,\n",
       "         -0.1953, -0.0065,  0.2677,  0.0540, -0.0166, -0.1255,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2386, -0.0252,  0.2466,  0.0454, -0.0086,  0.0212,\n",
       "         -0.2165, -0.0159,  0.2728,  0.0494, -0.0124, -0.0592,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2667, -0.0337,  0.2939,  0.0404, -0.0027, -0.0852,\n",
       "         -0.2412, -0.0255,  0.2492,  0.0459, -0.0087,  0.0214,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2847, -0.0470,  0.3210,  0.0398, -0.0028, -0.0827,\n",
       "         -0.2724, -0.0344,  0.3001,  0.0413, -0.0027, -0.0870,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2732, -0.0586,  0.3133,  0.0449, -0.0020,  0.0366,\n",
       "         -0.2775, -0.0458,  0.3129,  0.0388, -0.0027, -0.0806,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2665, -0.0703,  0.3252,  0.0787,  0.0015,  0.0435,\n",
       "         -0.2702, -0.0579,  0.3098,  0.0444, -0.0020,  0.0362,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2628, -0.0829,  0.3235,  0.1102,  0.0046,  0.1778,\n",
       "         -0.2668, -0.0704,  0.3255,  0.0787,  0.0015,  0.0435,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2578, -0.0951,  0.2730,  0.1542,  0.0097,  0.3000,\n",
       "         -0.2623, -0.0827,  0.3228,  0.1100,  0.0046,  0.1774,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2536, -0.1073,  0.2174,  0.2129,  0.0146,  0.1433,\n",
       "         -0.2574, -0.0949,  0.2726,  0.1539,  0.0097,  0.2996,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2511, -0.1196,  0.1960,  0.2513,  0.0170,  0.0554,\n",
       "         -0.2534, -0.1072,  0.2173,  0.2128,  0.0146,  0.1432,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2537, -0.1321,  0.1742,  0.3367,  0.0096, -0.0378,\n",
       "         -0.2511, -0.1196,  0.1960,  0.2514,  0.0170,  0.0554,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2618, -0.1437,  0.2088,  0.4612, -0.0274,  0.0962,\n",
       "         -0.2531, -0.1318,  0.1738,  0.3360,  0.0096, -0.0377,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.2746, -0.1545,  0.3643,  0.6127, -0.1033,  0.1481,\n",
       "         -0.2611, -0.1433,  0.2083,  0.4599, -0.0273,  0.0959,  0.0000]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([attr_r_rel, attr_s_rel, state_r_rel, state_s_rel, Ra[0].reshape([-1,1])], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_s_rel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ra[0].reshape([-1,1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = gen_PyFleX(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, datas = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions, velocities, hair_idx, shape_position = datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(data, stat, args, phases_dict, verbose=0, var=False):\n",
    "    '''\n",
    "    for a single hair\n",
    "    '''\n",
    "    positions, velocities, hairs_idx, shape_position = data\n",
    "    n_shapes = 1\n",
    "    hairs_idx_begin = [idx[0] for idx in hairs_idx]\n",
    "    n_particles = positions.shape[0]\n",
    "    R = 0.1\n",
    "    R = 2\n",
    "    \n",
    "    ### object attributes\n",
    "    #   dim 10: [rigid, fluid, root_0, root_1, gripper_0, gripper_1, mass_inv,\n",
    "    #            clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep]\n",
    "    #   here we only consider the hairs but not the gripper, attr_dim = 1, attr = 0 for hair, attr = 1 for shapes\n",
    "    attr = np.zeros((n_particles+n_shapes, args.attr_dim))\n",
    "    \n",
    "    ### construct relations\n",
    "    Rr_idxs = []        # relation receiver idx list\n",
    "    Rs_idxs = []        # relation sender idx list\n",
    "    Ras = []            # relation attributes list\n",
    "    values = []         # relation value list (should be 1)\n",
    "    node_r_idxs = []    # list of corresponding receiver node idx\n",
    "    node_s_idxs = []    # list of corresponding sender node idx\n",
    "    psteps = []         # propagation steps\n",
    "    \n",
    "    ##### add env specific graph components\n",
    "    ### specify for shapes\n",
    "    rels = []\n",
    "    vals = []\n",
    "    \n",
    "    for i in range(n_shapes):\n",
    "        attr[n_particles+i, 0] = 1\n",
    "        dis = np.linalg.norm(positions[:,:2]- shape_position[0,:2],axis = 1)\n",
    "        nodes_rel = np.nonzero(dis <= R)[0]\n",
    "        # for relation between hair nodes and a gripper, we note it as 1\n",
    "        gripper = np.ones(nodes_rel.shape[0], dtype=np.int) * (n_particles+i)\n",
    "        rels += [np.stack([nodes_rel, gripper, np.ones(nodes_rel.shape[0])], axis=1)]\n",
    "        vals += [np.ones(nodes_rel.shape[0], dtype=np.int)]\n",
    "        \n",
    "    \n",
    "    ##### add relations between leaf particles\n",
    "    ## here we only consider the relations in a hair: the relation between a node and the nodes nearby\n",
    "    ## simple case for one hair, TEMPORARY 2 rels for one link\n",
    "    nodes_p = np.arange(n_particles-1)\n",
    "    val = np.linalg.norm(positions[1:]-positions[:-1],axis = 1)\n",
    "    R1 = np.stack([nodes_p,nodes_p+1, np.zeros(n_particles-1)],axis = 1)\n",
    "    R2 = np.stack([nodes_p+1,nodes_p, np.zeros(n_particles-1)],axis = 1)\n",
    "    rels += [np.concatenate([R1,R2],axis = 0)]\n",
    "    vals += [val,val]\n",
    "    \n",
    "    rels = np.concatenate(rels, 0)\n",
    "    vals = np.concatenate(vals, 0)\n",
    "    \n",
    "  #  print (vals.shape)\n",
    " #   print (rels.shape)\n",
    "    \n",
    "    \n",
    "    if rels.shape[0] > 0:\n",
    "        if verbose:\n",
    "            print(\"Relations neighbor\", rels.shape)\n",
    "        Rr_idxs.append(torch.LongTensor([rels[:, 0], np.arange(rels.shape[0])]))\n",
    "        Rs_idxs.append(torch.LongTensor([rels[:, 1], np.arange(rels.shape[0])]))\n",
    "        # Ra: relation attributes\n",
    "    #    Ra = np.zeros((rels.shape[0], args.relation_dim))  \n",
    "        Ra = rels[:,2]\n",
    "        Ras.append(torch.FloatTensor(Ra))\n",
    "        # values could be changed\n",
    "     #   values.append(torch.FloatTensor([1] * rels.shape[0]))\n",
    "      #  values.append(rels[:,2])\n",
    "        #### for hairs: values equals to the length of this segment\n",
    "        values.append(vals)\n",
    "        node_r_idxs.append(np.arange(n_particles))\n",
    "        node_s_idxs.append(np.arange(n_particles + n_shapes))\n",
    "        psteps.append(args.pstep)\n",
    "        \n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Attr shape (after hierarchy building):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "        print(\"Particle attr:\", np.sum(attr[:n_particles], axis=0))\n",
    "        print(\"Shape attr:\", np.sum(attr[n_particles:n_particles+n_shapes], axis=0))\n",
    "        print(\"Roots attr:\", np.sum(attr[n_particles+n_shapes:], axis=0))\n",
    "        \n",
    "        \n",
    "    ### normalize data\n",
    "    data = [positions, velocities]\n",
    "    data = normalize(data, stat, var)\n",
    "    positions, velocities = data[0], data[1]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Particle positions stats\")\n",
    "        print(positions.shape)\n",
    "        print(np.min(positions[:n_particles], 0))\n",
    "        print(np.max(positions[:n_particles], 0))\n",
    "        print(np.mean(positions[:n_particles], 0))\n",
    "        print(np.std(positions[:n_particles], 0))\n",
    "\n",
    "        show_vel_dim = 6 if args.env == 'RiceGrip' else 3\n",
    "        print(\"Velocities stats\")\n",
    "        print(velocities.shape)\n",
    "        print(np.mean(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        print(np.std(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        \n",
    "    state = torch.FloatTensor(np.concatenate([positions, velocities], axis=1))\n",
    "    attr = torch.FloatTensor(attr)\n",
    "    relations = [Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps]\n",
    "\n",
    "    return attr, state, relations, n_particles, n_shapes#, instance_idx\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr, state, relations, n_particles, n_shapes = prepare_input(data, stat, args, phases_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps = relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rs_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(data, stat, args, phases_dict, verbose=0, var=False):\n",
    "\n",
    "    # Arrangement:\n",
    "    # particles, shapes, roots\n",
    "\n",
    "    if args.env == 'RiceGrip':\n",
    "        positions, velocities, shape_quats, clusters, scene_params = data\n",
    "        n_shapes = shape_quats.size(0) if var else shape_quats.shape[0]\n",
    "    elif args.env == 'FluidShake':\n",
    "        positions, velocities, shape_quats, scene_params = data\n",
    "        n_shapes = shape_quats.size(0) if var else shape_quats.shape[0]\n",
    "        clusters = None\n",
    "    elif args.env == 'BoxBath':\n",
    "        positions, velocities, clusters = data\n",
    "        n_shapes = 0\n",
    "    elif args.env == 'FluidFall':\n",
    "        positions, velocities = data\n",
    "        n_shapes = 0\n",
    "        clusters = None\n",
    "    elif args.env == 'Hairs':\n",
    "        positions, velocities, hairs_idx = data\n",
    "        n_shapes = 1\n",
    "        hairs_idx_begin = [idx[0] for idx in hairs_idx]\n",
    "        clusters = None\n",
    "\n",
    "    count_nodes = positions.size(0) if var else positions.shape[0]\n",
    "    n_particles = count_nodes - n_shapes\n",
    "\n",
    "    if verbose:\n",
    "        print(\"positions\", positions.shape)\n",
    "        print(\"velocities\", velocities.shape)\n",
    "\n",
    "        print(\"n_particles\", n_particles)\n",
    "        print(\"n_shapes\", n_shapes)\n",
    "        if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            print(\"shape_quats\", shape_quats.shape)\n",
    "\n",
    "    ### instance idx\n",
    "    #   instance_idx (n_instance + 1): start idx of instance\n",
    "    if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "        instance_idx = [0, n_particles]\n",
    "    elif args.env == 'Hairs':\n",
    "        instance_idx = [0, count_nodes]\n",
    "    else:\n",
    "        instance_idx = phases_dict[\"instance_idx\"]\n",
    "    if verbose:\n",
    "        print(\"Instance_idx:\", instance_idx)\n",
    "\n",
    "\n",
    "    ### object attributes\n",
    "    #   dim 10: [rigid, fluid, root_0, root_1, gripper_0, gripper_1, mass_inv,\n",
    "    #            clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep]\n",
    "    attr = np.zeros((count_nodes, args.attr_dim))\n",
    "    # no need to include mass for now\n",
    "    # attr[:, 6] = positions[:, -1].data.cpu().numpy() if var else positions[:, -1] # mass_inv\n",
    "    if args.env == 'RiceGrip':\n",
    "        # clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep\n",
    "        attr[:, -3:] = scene_params[-3:]\n",
    "\n",
    "\n",
    "    ### construct relations\n",
    "    Rr_idxs = []        # relation receiver idx list\n",
    "    Rs_idxs = []        # relation sender idx list\n",
    "    Ras = []            # relation attributes list\n",
    "    values = []         # relation value list (should be 1)\n",
    "    node_r_idxs = []    # list of corresponding receiver node idx\n",
    "    node_s_idxs = []    # list of corresponding sender node idx\n",
    "    psteps = []         # propagation steps\n",
    "\n",
    "    ##### add env specific graph components\n",
    "    rels = []\n",
    "    if args.env == 'RiceGrip':\n",
    "        # nodes = np.arange(n_particles)\n",
    "        for i in range(n_shapes):\n",
    "            attr[n_particles + i, 2 + i] = 1\n",
    "\n",
    "            pos = positions.data.cpu().numpy() if var else positions\n",
    "            dis = np.linalg.norm(\n",
    "                pos[:n_particles, 3:6:2] - pos[n_particles + i, 3:6:2], axis=1)\n",
    "            nodes = np.nonzero(dis < 0.3)[0]\n",
    "\n",
    "            if verbose:\n",
    "                visualize_neighbors(positions, positions, 0, nodes)\n",
    "                print(np.sort(dis)[:10])\n",
    "\n",
    "            gripper = np.ones(nodes.shape[0], dtype=np.int) * (n_particles + i)\n",
    "            rels += [np.stack([nodes, gripper, np.ones(nodes.shape[0])], axis=1)]\n",
    "            \n",
    "    elif args.env == 'Hairs':\n",
    "        # TODO: add relations between the hairs and the stick\n",
    "        attr[:,0] = 1\n",
    "        pass\n",
    "\n",
    "    elif args.env == 'FluidShake':\n",
    "        for i in range(n_shapes):\n",
    "            attr[n_particles + i, 1 + i] = 1\n",
    "\n",
    "            pos = positions.data.cpu().numpy() if var else positions\n",
    "            if i == 0:\n",
    "                # floor\n",
    "                dis = pos[:n_particles, 1] - pos[n_particles + i, 1]\n",
    "            elif i == 1:\n",
    "                # left\n",
    "                dis = pos[:n_particles, 0] - pos[n_particles + i, 0]\n",
    "            elif i == 2:\n",
    "                # right\n",
    "                dis = pos[n_particles + i, 0] - pos[:n_particles, 0]\n",
    "            elif i == 3:\n",
    "                # back\n",
    "                dis = pos[:n_particles, 2] - pos[n_particles + i, 2]\n",
    "            elif i == 4:\n",
    "                # front\n",
    "                dis = pos[n_particles + i, 2] - pos[:n_particles, 2]\n",
    "            else:\n",
    "                raise AssertionError(\"more shapes than expected\")\n",
    "            nodes = np.nonzero(dis < 0.1)[0]\n",
    "\n",
    "            if verbose:\n",
    "                visualize_neighbors(positions, positions, 0, nodes)\n",
    "                print(np.sort(dis)[:10])\n",
    "\n",
    "            wall = np.ones(nodes.shape[0], dtype=np.int) * (n_particles + i)\n",
    "            rels += [np.stack([nodes, wall, np.ones(nodes.shape[0])], axis=1)]\n",
    "\n",
    "    if verbose and len(rels) > 0:\n",
    "        print(np.concatenate(rels, 0).shape)\n",
    "\n",
    "    ##### add relations between leaf particles\n",
    "    for i in range(len(instance_idx) - 1):\n",
    "        st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "\n",
    "        if verbose:\n",
    "            print('instance #%d' % i, st, ed)\n",
    "\n",
    "        if args.env == 'BoxBath':\n",
    "            if phases_dict['material'][i] == 'rigid':\n",
    "                attr[st:ed, 0] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.concatenate((np.arange(st), np.arange(ed, n_particles)))\n",
    "            elif phases_dict['material'][i] == 'fluid':\n",
    "                attr[st:ed, 1] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.arange(n_particles)\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported materials\")\n",
    "                \n",
    "        elif args.env == 'Hairs':\n",
    "            # TODO: add relations between the hairs and the stick\n",
    "            pass\n",
    "            \n",
    "          #  if ed not in hairs_idx_begin:\n",
    "             #   queries = np.arange(st, ed)\n",
    "              #  anchors = np.arange(n_particles)\n",
    "\n",
    "        elif args.env == 'FluidFall' or args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            if phases_dict['material'][i] == 'fluid':\n",
    "                attr[st:ed, 0] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.arange(n_particles)\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported materials\")\n",
    "\n",
    "        else:\n",
    "            raise AssertionError(\"Unsupported materials\")\n",
    "\n",
    "        # st_time = time.time()\n",
    "        pos = positions\n",
    "        pos = pos[:, -3:]\n",
    "        if args.env == 'Hairs':\n",
    "            #TODO\n",
    "            pass\n",
    "        else:\n",
    "            rels += find_relations_neighbor(pos, queries, anchors, args.neighbor_radius, 2, var)\n",
    "            # return list of [receiver, sender, relation_type]\n",
    "        # print(\"Time on neighbor search\", time.time() - st_time)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Attr shape (after add env specific graph components):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "\n",
    "    rels = np.concatenate(rels, 0)\n",
    "    if rels.shape[0] > 0:\n",
    "        if verbose:\n",
    "            print(\"Relations neighbor\", rels.shape)\n",
    "        Rr_idxs.append(torch.LongTensor([rels[:, 0], np.arange(rels.shape[0])]))\n",
    "        Rs_idxs.append(torch.LongTensor([rels[:, 1], np.arange(rels.shape[0])]))\n",
    "        Ra = np.zeros((rels.shape[0], args.relation_dim))\n",
    "        Ras.append(torch.FloatTensor(Ra))\n",
    "        values.append(torch.FloatTensor([1] * rels.shape[0]))\n",
    "        node_r_idxs.append(np.arange(n_particles))\n",
    "        node_s_idxs.append(np.arange(n_particles + n_shapes))\n",
    "        psteps.append(args.pstep)\n",
    "\n",
    "    if verbose:\n",
    "        print('clusters', clusters)\n",
    "\n",
    "    # add heirarchical relations per instance\n",
    "    cnt_clusters = 0\n",
    "    for i in range(len(instance_idx) - 1):\n",
    "        st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "        n_root_level = len(phases_dict[\"root_num\"][i])\n",
    "\n",
    "        if n_root_level > 0:\n",
    "            attr, positions, velocities, count_nodes, \\\n",
    "            rels, node_r_idx, node_s_idx, pstep = \\\n",
    "                    make_hierarchy(args.env, attr, positions, velocities, i, st, ed,\n",
    "                                   phases_dict, count_nodes, clusters[cnt_clusters], verbose, var)\n",
    "\n",
    "            for j in range(len(rels)):\n",
    "                if verbose:\n",
    "                    print(\"Relation instance\", j, rels[j].shape)\n",
    "                Rr_idxs.append(torch.LongTensor([rels[j][:, 0], np.arange(rels[j].shape[0])]))\n",
    "                Rs_idxs.append(torch.LongTensor([rels[j][:, 1], np.arange(rels[j].shape[0])]))\n",
    "                Ra = np.zeros((rels[j].shape[0], args.relation_dim)); Ra[:, 0] = 1\n",
    "                Ras.append(torch.FloatTensor(Ra))\n",
    "                values.append(torch.FloatTensor([1] * rels[j].shape[0]))\n",
    "                node_r_idxs.append(node_r_idx[j])\n",
    "                node_s_idxs.append(node_s_idx[j])\n",
    "                psteps.append(pstep[j])\n",
    "\n",
    "            cnt_clusters += 1\n",
    "\n",
    "    if verbose:\n",
    "        if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            print(\"Scene_params:\", scene_params)\n",
    "\n",
    "        print(\"Attr shape (after hierarchy building):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "        print(\"Particle attr:\", np.sum(attr[:n_particles], axis=0))\n",
    "        print(\"Shape attr:\", np.sum(attr[n_particles:n_particles+n_shapes], axis=0))\n",
    "        print(\"Roots attr:\", np.sum(attr[n_particles+n_shapes:], axis=0))\n",
    "\n",
    "    ### normalize data\n",
    "    data = [positions, velocities]\n",
    "    data = normalize(data, stat, var)\n",
    "    positions, velocities = data[0], data[1]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Particle positions stats\")\n",
    "        print(positions.shape)\n",
    "        print(np.min(positions[:n_particles], 0))\n",
    "        print(np.max(positions[:n_particles], 0))\n",
    "        print(np.mean(positions[:n_particles], 0))\n",
    "        print(np.std(positions[:n_particles], 0))\n",
    "\n",
    "        show_vel_dim = 6 if args.env == 'RiceGrip' else 3\n",
    "        print(\"Velocities stats\")\n",
    "        print(velocities.shape)\n",
    "        print(np.mean(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        print(np.std(velocities[:n_particles, :show_vel_dim], 0))\n",
    "\n",
    "    if args.env == 'RiceGrip':\n",
    "        if var:\n",
    "            quats = torch.cat(\n",
    "                [Variable(torch.zeros(n_particles, 4).cuda()), shape_quats,\n",
    "                 Variable(torch.zeros(count_nodes - n_particles - n_shapes, 4).cuda())], 0)\n",
    "            state = torch.cat([positions, velocities, quats], 1)\n",
    "        else:\n",
    "            quat_null = np.array([[0., 0., 0., 0.]])\n",
    "            quats = np.repeat(quat_null, [count_nodes], axis=0)\n",
    "            quats[n_particles:n_particles + n_shapes] = shape_quats\n",
    "            # if args.eval == 0:\n",
    "            # quats += np.random.randn(quats.shape[0], 4) * 0.05\n",
    "            state = torch.FloatTensor(np.concatenate([positions, velocities, quats], axis=1))\n",
    "    else:\n",
    "        if var:\n",
    "            state = torch.cat([positions, velocities], 1)\n",
    "        else:\n",
    "            state = torch.FloatTensor(np.concatenate([positions, velocities], axis=1))\n",
    "\n",
    "    if verbose:\n",
    "        for i in range(count_nodes - 1):\n",
    "            if np.sum(np.abs(attr[i] - attr[i + 1])) > 1e-6:\n",
    "                print(i, attr[i], attr[i + 1])\n",
    "\n",
    "        for i in range(len(Ras)):\n",
    "            print(i, np.min(node_r_idxs[i]), np.max(node_r_idxs[i]), np.min(node_s_idxs[i]), np.max(node_s_idxs[i]))\n",
    "\n",
    "    attr = torch.FloatTensor(attr)\n",
    "    relations = [Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps]\n",
    "\n",
    "    return attr, state, relations, n_particles, n_shapes, instance_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_PyFleX(info):\n",
    "\n",
    "    env, root_num = info['env'], info['root_num']\n",
    "    thread_idx, data_dir, data_names = info['thread_idx'], info['data_dir'], info['data_names']\n",
    "    n_rollout, n_instance = info['n_rollout'], info['n_instance']\n",
    "    time_step, time_step_clip = info['time_step'], info['time_step_clip']\n",
    "    shape_state_dim, dt = info['shape_state_dim'], info['dt']\n",
    "\n",
    "    env_idx = info['env_idx']\n",
    "\n",
    "    np.random.seed(round(time.time() * 1000 + thread_idx) % 2**32)\n",
    "    \n",
    "    stats = [init_stat(3), init_stat(3)]\n",
    "\n",
    "    import pyflex\n",
    "    pyflex.init()\n",
    "\n",
    "    for i in range(n_rollout):\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"%d / %d\" % (i, n_rollout))\n",
    "\n",
    "        rollout_idx = thread_idx * n_rollout + i\n",
    "        rollout_dir = os.path.join(data_dir, str(rollout_idx))\n",
    "        os.system('mkdir -p ' + rollout_dir)\n",
    "        \n",
    "        env_idx = 10\n",
    "        # scene_params: [len(box) at dim x,len(box) at dim y,len(box) at dim z, num_hair per circle, num_circle]\n",
    "        box_size = [0.1,0.1,1.5]\n",
    "        N_p_cirlcle = 50\n",
    "        N_circle = 20\n",
    "        N_hairs = N_circle*N_p_cirlcle\n",
    "\n",
    "\n",
    "        scene_params = np.array(box_size + [N_p_cirlcle] + [N_circle])\n",
    "\n",
    "        pyflex.set_scene(env_idx, scene_params, thread_idx)\n",
    "        n_particles = pyflex.get_n_particles()\n",
    "        N_particles_per_hair = int(n_particles/N_hairs)\n",
    "        idx_begins = np.arange(N_hairs)*N_particles_per_hair\n",
    "        idx_hairs = [[i,i+N_particles_per_hair-1] for i in idx_begins]\n",
    "\n",
    "        positions = np.zeros((time_step, n_particles, 3), dtype=np.float32)\n",
    "        velocities = np.zeros((time_step, n_particles, 3), dtype=np.float32)\n",
    "\n",
    "        for j in range(time_step_clip):\n",
    "            p_clip = pyflex.get_positions().reshape(-1, 4)[:, :3]\n",
    "            pyflex.step()\n",
    "\n",
    "        for j in range(time_step):\n",
    "            positions[j] = pyflex.get_positions().reshape(-1, 4)[:, :3]\n",
    "            if j == 0:\n",
    "                velocities[j] = (positions[j] - p_clip) / dt\n",
    "            else:\n",
    "                velocities[j] = (positions[j] - positions[j - 1]) / dt\n",
    "\n",
    "            pyflex.step()\n",
    "            data = [positions[j], velocities[j], idx_hairs]\n",
    "            store_data(data_names, data, os.path.join(rollout_dir, str(j) + '.h5'))\n",
    "        \n",
    "        # change dtype for more accurate stat calculation\n",
    "        # only normalize positions and velocities\n",
    "        datas = [positions.astype(np.float64), velocities.astype(np.float64)]\n",
    "\n",
    "        for j in range(len(stats)): \n",
    "            # here j = 2, refers to positions and velocities\n",
    "            stat = init_stat(stats[j].shape[0]) \n",
    "            # stat= np.zeros((3,3))\n",
    "            stat[:, 0] = np.mean(datas[j], axis=(0, 1))[:]\n",
    "            stat[:, 1] = np.std(datas[j], axis=(0, 1))[:]\n",
    "            stat[:, 2] = datas[j].shape[0] * datas[j].shape[1] # time_step*n_particles\n",
    "            stats[j] = combine_stat(stats[j], stat)\n",
    "\n",
    "    pyflex.clean()\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsFleXDataset(Dataset):\n",
    "\n",
    "    def __init__(self, args, phase, phases_dict, verbose):\n",
    "        self.args = args\n",
    "        self.phase = phase\n",
    "        self.phases_dict = phases_dict\n",
    "        self.verbose = verbose\n",
    "        self.data_dir = os.path.join(self.args.dataf, phase)\n",
    "        self.stat_path = os.path.join(self.args.dataf, 'stat.h5')\n",
    "\n",
    "        os.system('mkdir -p ' + self.data_dir)\n",
    "\n",
    "        #    self.data_names = ['positions', 'velocities', 'shape_quats', 'clusters', 'scene_params']\n",
    "        self.data_names = ['positions', 'velocities','hair_idx']\n",
    "\n",
    "        ratio = self.args.train_valid_ratio\n",
    "        if phase == 'train':\n",
    "            self.n_rollout = int(self.args.n_rollout * ratio)\n",
    "        elif phase == 'valid':\n",
    "            self.n_rollout = self.args.n_rollout - int(self.args.n_rollout * ratio)\n",
    "        else:\n",
    "            raise AssertionError(\"Unknown phase\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rollout * (self.args.time_step - 1)\n",
    "\n",
    "    def load_data(self, name):\n",
    "        self.stat = load_data(self.data_names[:2], self.stat_path)\n",
    "        for i in range(len(self.stat)):\n",
    "            self.stat[i] = self.stat[i][-self.args.position_dim:, :]\n",
    "            # print(self.data_names[i], self.stat[i].shape)\n",
    "\n",
    "    def gen_data(self, name):\n",
    "        # if the data hasn't been generated, generate the data\n",
    "        print(\"Generating data ... n_rollout=%d, time_step=%d\" % (self.n_rollout, self.args.time_step))\n",
    "\n",
    "        infos = []\n",
    "        for i in range(self.args.num_workers):\n",
    "            info = {\n",
    "                'env': self.args.env,\n",
    "                'root_num': self.phases_dict['root_num'],\n",
    "                'thread_idx': i,\n",
    "                'data_dir': self.data_dir,\n",
    "                'data_names': self.data_names,\n",
    "                'n_rollout': self.n_rollout // self.args.num_workers,\n",
    "                'n_instance': self.args.n_instance,\n",
    "                'time_step': self.args.time_step,\n",
    "                'time_step_clip': self.args.time_step_clip,\n",
    "                'dt': self.args.dt,\n",
    "                'shape_state_dim': self.args.shape_state_dim}\n",
    "\n",
    "            info['env_idx'] = 10\n",
    "            infos.append(info)\n",
    "\n",
    "        cores = self.args.num_workers\n",
    "        pool = mp.Pool(processes=cores)\n",
    "        data = pool.map(gen_PyFleX, infos)\n",
    "\n",
    "        print(\"Training data generated, warpping up stats ...\")\n",
    "\n",
    "        if self.phase == 'train' and self.args.gen_stat:\n",
    "            # positions [x, y, z], velocities[xdot, ydot, zdot]\n",
    "            if self.args.env == 'RiceGrip':\n",
    "                self.stat = [init_stat(6), init_stat(6)]\n",
    "            else:\n",
    "                self.stat = [init_stat(3), init_stat(3)]\n",
    "            for i in range(len(data)):\n",
    "                for j in range(len(self.stat)):\n",
    "                    self.stat[j] = combine_stat(self.stat[j], data[i][j])\n",
    "            store_data(self.data_names[:2], self.stat, self.stat_path)\n",
    "        else:\n",
    "            print(\"Loading stat from %s ...\" % self.stat_path)\n",
    "            self.stat = load_data(self.data_names[:2], self.stat_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_rollout = idx // (self.args.time_step - 1)\n",
    "        idx_timestep = idx % (self.args.time_step - 1)\n",
    "\n",
    "        # ignore the first frame for env RiceGrip\n",
    "        if self.args.env == 'RiceGrip' and idx_timestep == 0:\n",
    "            idx_timestep = np.random.randint(1, self.args.time_step - 1)\n",
    "\n",
    "        data_path = os.path.join(self.data_dir, str(idx_rollout), str(idx_timestep) + '.h5')\n",
    "        data_nxt_path = os.path.join(self.data_dir, str(idx_rollout), str(idx_timestep + 1) + '.h5')\n",
    "\n",
    "        data = load_data(self.data_names, data_path)\n",
    "\n",
    "        vel_his = []\n",
    "        for i in range(self.args.n_his):\n",
    "            path = os.path.join(self.data_dir, str(idx_rollout), str(max(1, idx_timestep - i - 1)) + '.h5')\n",
    "            data_his = load_data(self.data_names, path)\n",
    "            vel_his.append(data_his[1])\n",
    "\n",
    "        data[1] = np.concatenate([data[1]] + vel_his, 1)\n",
    "\n",
    "        attr, state, relations, n_particles, n_shapes, instance_idx = \\\n",
    "                prepare_input(data, self.stat, self.args, self.phases_dict, self.verbose)\n",
    "\n",
    "        ### label\n",
    "        data_nxt = normalize(load_data(self.data_names, data_nxt_path), self.stat)\n",
    "\n",
    "        label = torch.FloatTensor(data_nxt[1][:n_particles])\n",
    "\n",
    "        return attr, state, relations, n_particles, n_shapes, instance_idx, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(data, stat, args, phases_dict, verbose=0, var=False):\n",
    "\n",
    "    # Arrangement:\n",
    "    # particles, shapes, roots\n",
    "\n",
    "    if args.env == 'RiceGrip':\n",
    "        positions, velocities, shape_quats, clusters, scene_params = data\n",
    "        n_shapes = shape_quats.size(0) if var else shape_quats.shape[0]\n",
    "    elif args.env == 'FluidShake':\n",
    "        positions, velocities, shape_quats, scene_params = data\n",
    "        n_shapes = shape_quats.size(0) if var else shape_quats.shape[0]\n",
    "        clusters = None\n",
    "    elif args.env == 'BoxBath':\n",
    "        positions, velocities, clusters = data\n",
    "        n_shapes = 0\n",
    "    elif args.env == 'FluidFall':\n",
    "        positions, velocities = data\n",
    "        n_shapes = 0\n",
    "        clusters = None\n",
    "    elif args.env == 'Hairs':\n",
    "        positions, velocities, hairs_idx = data\n",
    "        n_shapes = 1\n",
    "        hairs_idx_begin = [idx[0] for idx in hairs_idx]\n",
    "        clusters = None\n",
    "\n",
    "    count_nodes = positions.size(0) if var else positions.shape[0]\n",
    "    n_particles = count_nodes - n_shapes\n",
    "\n",
    "    if verbose:\n",
    "        print(\"positions\", positions.shape)\n",
    "        print(\"velocities\", velocities.shape)\n",
    "\n",
    "        print(\"n_particles\", n_particles)\n",
    "        print(\"n_shapes\", n_shapes)\n",
    "        if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            print(\"shape_quats\", shape_quats.shape)\n",
    "\n",
    "    ### instance idx\n",
    "    #   instance_idx (n_instance + 1): start idx of instance\n",
    "    if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "        instance_idx = [0, n_particles]\n",
    "    elif args.env == 'Hairs':\n",
    "        instance_idx = [0, count_nodes]\n",
    "    else:\n",
    "        instance_idx = phases_dict[\"instance_idx\"]\n",
    "    if verbose:\n",
    "        print(\"Instance_idx:\", instance_idx)\n",
    "\n",
    "\n",
    "    ### object attributes\n",
    "    #   dim 10: [rigid, fluid, root_0, root_1, gripper_0, gripper_1, mass_inv,\n",
    "    #            clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep]\n",
    "    attr = np.zeros((count_nodes, args.attr_dim))\n",
    "    # no need to include mass for now\n",
    "    # attr[:, 6] = positions[:, -1].data.cpu().numpy() if var else positions[:, -1] # mass_inv\n",
    "    if args.env == 'RiceGrip':\n",
    "        # clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep\n",
    "        attr[:, -3:] = scene_params[-3:]\n",
    "\n",
    "\n",
    "    ### construct relations\n",
    "    Rr_idxs = []        # relation receiver idx list\n",
    "    Rs_idxs = []        # relation sender idx list\n",
    "    Ras = []            # relation attributes list\n",
    "    values = []         # relation value list (should be 1)\n",
    "    node_r_idxs = []    # list of corresponding receiver node idx\n",
    "    node_s_idxs = []    # list of corresponding sender node idx\n",
    "    psteps = []         # propagation steps\n",
    "\n",
    "    ##### add env specific graph components\n",
    "    rels = []\n",
    "    if args.env == 'RiceGrip':\n",
    "        # nodes = np.arange(n_particles)\n",
    "        for i in range(n_shapes):\n",
    "            attr[n_particles + i, 2 + i] = 1\n",
    "\n",
    "            pos = positions.data.cpu().numpy() if var else positions\n",
    "            dis = np.linalg.norm(\n",
    "                pos[:n_particles, 3:6:2] - pos[n_particles + i, 3:6:2], axis=1)\n",
    "            nodes = np.nonzero(dis < 0.3)[0]\n",
    "\n",
    "            if verbose:\n",
    "                visualize_neighbors(positions, positions, 0, nodes)\n",
    "                print(np.sort(dis)[:10])\n",
    "\n",
    "            gripper = np.ones(nodes.shape[0], dtype=np.int) * (n_particles + i)\n",
    "            rels += [np.stack([nodes, gripper, np.ones(nodes.shape[0])], axis=1)]\n",
    "            \n",
    "    elif args.env == 'Hairs':\n",
    "        # TODO: add relations between the hairs and the stick\n",
    "        attr[:,0] = 1\n",
    "        pass\n",
    "\n",
    "    elif args.env == 'FluidShake':\n",
    "        for i in range(n_shapes):\n",
    "            attr[n_particles + i, 1 + i] = 1\n",
    "\n",
    "            pos = positions.data.cpu().numpy() if var else positions\n",
    "            if i == 0:\n",
    "                # floor\n",
    "                dis = pos[:n_particles, 1] - pos[n_particles + i, 1]\n",
    "            elif i == 1:\n",
    "                # left\n",
    "                dis = pos[:n_particles, 0] - pos[n_particles + i, 0]\n",
    "            elif i == 2:\n",
    "                # right\n",
    "                dis = pos[n_particles + i, 0] - pos[:n_particles, 0]\n",
    "            elif i == 3:\n",
    "                # back\n",
    "                dis = pos[:n_particles, 2] - pos[n_particles + i, 2]\n",
    "            elif i == 4:\n",
    "                # front\n",
    "                dis = pos[n_particles + i, 2] - pos[:n_particles, 2]\n",
    "            else:\n",
    "                raise AssertionError(\"more shapes than expected\")\n",
    "            nodes = np.nonzero(dis < 0.1)[0]\n",
    "\n",
    "            if verbose:\n",
    "                visualize_neighbors(positions, positions, 0, nodes)\n",
    "                print(np.sort(dis)[:10])\n",
    "\n",
    "            wall = np.ones(nodes.shape[0], dtype=np.int) * (n_particles + i)\n",
    "            rels += [np.stack([nodes, wall, np.ones(nodes.shape[0])], axis=1)]\n",
    "\n",
    "    if verbose and len(rels) > 0:\n",
    "        print(np.concatenate(rels, 0).shape)\n",
    "\n",
    "    ##### add relations between leaf particles\n",
    "    for i in range(len(instance_idx) - 1):\n",
    "        st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "\n",
    "        if verbose:\n",
    "            print('instance #%d' % i, st, ed)\n",
    "\n",
    "        if args.env == 'BoxBath':\n",
    "            if phases_dict['material'][i] == 'rigid':\n",
    "                attr[st:ed, 0] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.concatenate((np.arange(st), np.arange(ed, n_particles)))\n",
    "            elif phases_dict['material'][i] == 'fluid':\n",
    "                attr[st:ed, 1] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.arange(n_particles)\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported materials\")\n",
    "                \n",
    "        elif args.env == 'Hairs':\n",
    "            # TODO: add relations between the hairs and the stick\n",
    "            pass\n",
    "            \n",
    "          #  if ed not in hairs_idx_begin:\n",
    "             #   queries = np.arange(st, ed)\n",
    "              #  anchors = np.arange(n_particles)\n",
    "\n",
    "        elif args.env == 'FluidFall' or args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            if phases_dict['material'][i] == 'fluid':\n",
    "                attr[st:ed, 0] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.arange(n_particles)\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported materials\")\n",
    "\n",
    "        else:\n",
    "            raise AssertionError(\"Unsupported materials\")\n",
    "\n",
    "        # st_time = time.time()\n",
    "        pos = positions\n",
    "        pos = pos[:, -3:]\n",
    "        if args.env == 'Hairs':\n",
    "            #TODO\n",
    "            pass\n",
    "        else:\n",
    "            rels += find_relations_neighbor(pos, queries, anchors, args.neighbor_radius, 2, var)\n",
    "            # return list of [receiver, sender, relation_type]\n",
    "        # print(\"Time on neighbor search\", time.time() - st_time)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Attr shape (after add env specific graph components):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "\n",
    "    rels = np.concatenate(rels, 0)\n",
    "    if rels.shape[0] > 0:\n",
    "        if verbose:\n",
    "            print(\"Relations neighbor\", rels.shape)\n",
    "        Rr_idxs.append(torch.LongTensor([rels[:, 0], np.arange(rels.shape[0])]))\n",
    "        Rs_idxs.append(torch.LongTensor([rels[:, 1], np.arange(rels.shape[0])]))\n",
    "        Ra = np.zeros((rels.shape[0], args.relation_dim))\n",
    "        Ras.append(torch.FloatTensor(Ra))\n",
    "        values.append(torch.FloatTensor([1] * rels.shape[0]))\n",
    "        node_r_idxs.append(np.arange(n_particles))\n",
    "        node_s_idxs.append(np.arange(n_particles + n_shapes))\n",
    "        psteps.append(args.pstep)\n",
    "\n",
    "    if verbose:\n",
    "        print('clusters', clusters)\n",
    "\n",
    "    # add heirarchical relations per instance\n",
    "    cnt_clusters = 0\n",
    "    for i in range(len(instance_idx) - 1):\n",
    "        st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "        n_root_level = len(phases_dict[\"root_num\"][i])\n",
    "\n",
    "        if n_root_level > 0:\n",
    "            attr, positions, velocities, count_nodes, \\\n",
    "            rels, node_r_idx, node_s_idx, pstep = \\\n",
    "                    make_hierarchy(args.env, attr, positions, velocities, i, st, ed,\n",
    "                                   phases_dict, count_nodes, clusters[cnt_clusters], verbose, var)\n",
    "\n",
    "            for j in range(len(rels)):\n",
    "                if verbose:\n",
    "                    print(\"Relation instance\", j, rels[j].shape)\n",
    "                Rr_idxs.append(torch.LongTensor([rels[j][:, 0], np.arange(rels[j].shape[0])]))\n",
    "                Rs_idxs.append(torch.LongTensor([rels[j][:, 1], np.arange(rels[j].shape[0])]))\n",
    "                Ra = np.zeros((rels[j].shape[0], args.relation_dim)); Ra[:, 0] = 1\n",
    "                Ras.append(torch.FloatTensor(Ra))\n",
    "                values.append(torch.FloatTensor([1] * rels[j].shape[0]))\n",
    "                node_r_idxs.append(node_r_idx[j])\n",
    "                node_s_idxs.append(node_s_idx[j])\n",
    "                psteps.append(pstep[j])\n",
    "\n",
    "            cnt_clusters += 1\n",
    "\n",
    "    if verbose:\n",
    "        if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            print(\"Scene_params:\", scene_params)\n",
    "\n",
    "        print(\"Attr shape (after hierarchy building):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "        print(\"Particle attr:\", np.sum(attr[:n_particles], axis=0))\n",
    "        print(\"Shape attr:\", np.sum(attr[n_particles:n_particles+n_shapes], axis=0))\n",
    "        print(\"Roots attr:\", np.sum(attr[n_particles+n_shapes:], axis=0))\n",
    "\n",
    "    ### normalize data\n",
    "    data = [positions, velocities]\n",
    "    data = normalize(data, stat, var)\n",
    "    positions, velocities = data[0], data[1]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Particle positions stats\")\n",
    "        print(positions.shape)\n",
    "        print(np.min(positions[:n_particles], 0))\n",
    "        print(np.max(positions[:n_particles], 0))\n",
    "        print(np.mean(positions[:n_particles], 0))\n",
    "        print(np.std(positions[:n_particles], 0))\n",
    "\n",
    "        show_vel_dim = 6 if args.env == 'RiceGrip' else 3\n",
    "        print(\"Velocities stats\")\n",
    "        print(velocities.shape)\n",
    "        print(np.mean(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        print(np.std(velocities[:n_particles, :show_vel_dim], 0))\n",
    "\n",
    "    if args.env == 'RiceGrip':\n",
    "        if var:\n",
    "            quats = torch.cat(\n",
    "                [Variable(torch.zeros(n_particles, 4).cuda()), shape_quats,\n",
    "                 Variable(torch.zeros(count_nodes - n_particles - n_shapes, 4).cuda())], 0)\n",
    "            state = torch.cat([positions, velocities, quats], 1)\n",
    "        else:\n",
    "            quat_null = np.array([[0., 0., 0., 0.]])\n",
    "            quats = np.repeat(quat_null, [count_nodes], axis=0)\n",
    "            quats[n_particles:n_particles + n_shapes] = shape_quats\n",
    "            # if args.eval == 0:\n",
    "            # quats += np.random.randn(quats.shape[0], 4) * 0.05\n",
    "            state = torch.FloatTensor(np.concatenate([positions, velocities, quats], axis=1))\n",
    "    else:\n",
    "        if var:\n",
    "            state = torch.cat([positions, velocities], 1)\n",
    "        else:\n",
    "            state = torch.FloatTensor(np.concatenate([positions, velocities], axis=1))\n",
    "\n",
    "    if verbose:\n",
    "        for i in range(count_nodes - 1):\n",
    "            if np.sum(np.abs(attr[i] - attr[i + 1])) > 1e-6:\n",
    "                print(i, attr[i], attr[i + 1])\n",
    "\n",
    "        for i in range(len(Ras)):\n",
    "            print(i, np.min(node_r_idxs[i]), np.max(node_r_idxs[i]), np.min(node_s_idxs[i]), np.max(node_s_idxs[i]))\n",
    "\n",
    "    attr = torch.FloatTensor(attr)\n",
    "    relations = [Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps]\n",
    "\n",
    "    return attr, state, relations, n_particles, n_shapes, instance_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "datasets = {phase: PhysicsFleXDataset(\n",
    "    args, phase, phases_dict, args.verbose_data) for phase in ['train', 'valid']}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "datasets['train'].gen_data(args.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['train'].load_data(args.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "idx_rollout = idx // (datasets['train'].args.time_step - 1)\n",
    "idx_timestep = idx % (datasets['train'].args.time_step - 1)\n",
    "\n",
    "# ignore the first frame for env RiceGrip\n",
    "if datasets['train'].args.env == 'RiceGrip' and idx_timestep == 0:\n",
    "    idx_timestep = np.random.randint(1, datasets['train'].args.time_step - 1)\n",
    "\n",
    "data_path = os.path.join(datasets['train'].data_dir, str(idx_rollout), str(idx_timestep) + '.h5')\n",
    "data_nxt_path = os.path.join(datasets['train'].data_dir, str(idx_rollout), str(idx_timestep + 1) + '.h5')\n",
    "\n",
    "data = load_data(datasets['train'].data_names, data_path)\n",
    "\n",
    "vel_his = []\n",
    "for i in range(datasets['train'].args.n_his):\n",
    "    path = os.path.join(datasets['train'].data_dir, str(idx_rollout), str(max(1, idx_timestep - i - 1)) + '.h5')\n",
    "    data_his = load_data(datasets['train'].data_names, path)\n",
    "    vel_his.append(data_his[1])\n",
    "\n",
    "data[1] = np.concatenate([data[1]] + vel_his, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['train'].stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c5d84736ba45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions, velocities, hairs_idx = data\n",
    "n_shapes = 1\n",
    "hairs_idx_begin = [idx[0] for idx in hairs_idx]\n",
    "hairs_idx_end = [idx[1] for idx in hairs_idx]\n",
    "\n",
    "clusters = None\n",
    "\n",
    "n_particles = positions.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions[:3,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_idx = [0, n_particles]\n",
    "### object attributes\n",
    "count_nodes = n_particles\n",
    "attr = np.zeros((count_nodes, args.attr_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### add env specific graph components\n",
    "rels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr[:n_particles,0] = 1\n",
    "#rels += find_relations_neighbor(pos, queries, anchors, args.neighbor_radius, 2, var)\n",
    "#queries = set of hairs, anchors = stick\n",
    "for i in range(n_particles):\n",
    "    if i in hairs_idx_begin:\n",
    "        receiver = [i+1]\n",
    "        sender = [i]\n",
    "        relation_type = [1]\n",
    "    elif i in hairs_idx_end:\n",
    "        receiver = [i-1]\n",
    "        sender = [i]\n",
    "        relation_type = [1]\n",
    "    else:\n",
    "        receiver = [i-1,i+1]\n",
    "        sender = [i, i]\n",
    "        relation_type = [1,1]\n",
    "    rels.append(np.stack([receiver, sender, relation_type], axis=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels = np.concatenate(rels, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### construct relations\n",
    "Rr_idxs = []        # relation receiver idx list\n",
    "Rs_idxs = []        # relation sender idx list\n",
    "Ras = []            # relation attributes list\n",
    "values = []         # relation value list (should be 1)\n",
    "node_r_idxs = []    # list of corresponding receiver node idx\n",
    "node_s_idxs = []    # list of corresponding sender node idx\n",
    "psteps = []         # propagation steps\n",
    "\n",
    "\n",
    "Rr_idxs.append(torch.LongTensor([rels[:, 0], np.arange(rels.shape[0])]))\n",
    "Rs_idxs.append(torch.LongTensor([rels[:, 1], np.arange(rels.shape[0])]))\n",
    "Ra = np.zeros((rels.shape[0], args.relation_dim))\n",
    "Ras.append(torch.FloatTensor(Ra))\n",
    "values.append(torch.FloatTensor([1] * rels.shape[0]))\n",
    "node_r_idxs.append(np.arange(n_particles))\n",
    "node_s_idxs.append(np.arange(n_particles + n_shapes))\n",
    "psteps.append(args.pstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [positions, velocities]\n",
    "stat = datasets['train'].stat\n",
    "data = normalize(data, stat)\n",
    "positions, velocities = data[0], data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = torch.FloatTensor(attr)\n",
    "relations = [Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(data, stat, args, phases_dict, verbose=0, var=False):\n",
    "    \n",
    "    positions, velocities, hairs_idx = data\n",
    "    n_shapes = 1\n",
    "    hairs_idx_begin = [idx[0] for idx in hairs_idx]\n",
    "    clusters = None\n",
    "\n",
    "    count_nodes = positions.size(0) if var else positions.shape[0]\n",
    "    n_particles = count_nodes - n_shapes\n",
    "\n",
    "    ### instance idx\n",
    "    #   instance_idx (n_instance + 1): start idx of instance\n",
    "    if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "        instance_idx = [0, n_particles]\n",
    "    elif args.env == 'Hairs':\n",
    "        instance_idx = [0, count_nodes]\n",
    "    else:\n",
    "        instance_idx = phases_dict[\"instance_idx\"]\n",
    "    if verbose:\n",
    "        print(\"Instance_idx:\", instance_idx)\n",
    "\n",
    "\n",
    "    ### object attributes\n",
    "    #   dim 10: [rigid, fluid, root_0, root_1, gripper_0, gripper_1, mass_inv,\n",
    "    #            clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep]\n",
    "    attr = np.zeros((count_nodes, args.attr_dim))\n",
    "    # no need to include mass for now\n",
    "    # attr[:, 6] = positions[:, -1].data.cpu().numpy() if var else positions[:, -1] # mass_inv\n",
    "    if args.env == 'RiceGrip':\n",
    "        # clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep\n",
    "        attr[:, -3:] = scene_params[-3:]\n",
    "\n",
    "\n",
    "    ### construct relations\n",
    "    Rr_idxs = []        # relation receiver idx list\n",
    "    Rs_idxs = []        # relation sender idx list\n",
    "    Ras = []            # relation attributes list\n",
    "    values = []         # relation value list (should be 1)\n",
    "    node_r_idxs = []    # list of corresponding receiver node idx\n",
    "    node_s_idxs = []    # list of corresponding sender node idx\n",
    "    psteps = []         # propagation steps\n",
    "\n",
    "    ##### add env specific graph components\n",
    "    rels = []\n",
    "    if args.env == 'RiceGrip':\n",
    "        # nodes = np.arange(n_particles)\n",
    "        for i in range(n_shapes):\n",
    "            attr[n_particles + i, 2 + i] = 1\n",
    "\n",
    "            pos = positions.data.cpu().numpy() if var else positions\n",
    "            dis = np.linalg.norm(\n",
    "                pos[:n_particles, 3:6:2] - pos[n_particles + i, 3:6:2], axis=1)\n",
    "            nodes = np.nonzero(dis < 0.3)[0]\n",
    "\n",
    "            if verbose:\n",
    "                visualize_neighbors(positions, positions, 0, nodes)\n",
    "                print(np.sort(dis)[:10])\n",
    "\n",
    "            gripper = np.ones(nodes.shape[0], dtype=np.int) * (n_particles + i)\n",
    "            rels += [np.stack([nodes, gripper, np.ones(nodes.shape[0])], axis=1)]\n",
    "            \n",
    "    elif args.env = 'Hairs':\n",
    "        # TODO: add relations between the hairs and the stick\n",
    "        attr[:,0] = 1\n",
    "        pass\n",
    "\n",
    "    elif args.env == 'FluidShake':\n",
    "        for i in range(n_shapes):\n",
    "            attr[n_particles + i, 1 + i] = 1\n",
    "\n",
    "            pos = positions.data.cpu().numpy() if var else positions\n",
    "            if i == 0:\n",
    "                # floor\n",
    "                dis = pos[:n_particles, 1] - pos[n_particles + i, 1]\n",
    "            elif i == 1:\n",
    "                # left\n",
    "                dis = pos[:n_particles, 0] - pos[n_particles + i, 0]\n",
    "            elif i == 2:\n",
    "                # right\n",
    "                dis = pos[n_particles + i, 0] - pos[:n_particles, 0]\n",
    "            elif i == 3:\n",
    "                # back\n",
    "                dis = pos[:n_particles, 2] - pos[n_particles + i, 2]\n",
    "            elif i == 4:\n",
    "                # front\n",
    "                dis = pos[n_particles + i, 2] - pos[:n_particles, 2]\n",
    "            else:\n",
    "                raise AssertionError(\"more shapes than expected\")\n",
    "            nodes = np.nonzero(dis < 0.1)[0]\n",
    "\n",
    "            if verbose:\n",
    "                visualize_neighbors(positions, positions, 0, nodes)\n",
    "                print(np.sort(dis)[:10])\n",
    "\n",
    "            wall = np.ones(nodes.shape[0], dtype=np.int) * (n_particles + i)\n",
    "            rels += [np.stack([nodes, wall, np.ones(nodes.shape[0])], axis=1)]\n",
    "\n",
    "    if verbose and len(rels) > 0:\n",
    "        print(np.concatenate(rels, 0).shape)\n",
    "\n",
    "    ##### add relations between leaf particles\n",
    "    for i in range(len(instance_idx) - 1):\n",
    "        st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "\n",
    "        if verbose:\n",
    "            print('instance #%d' % i, st, ed)\n",
    "\n",
    "        if args.env == 'BoxBath':\n",
    "            if phases_dict['material'][i] == 'rigid':\n",
    "                attr[st:ed, 0] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.concatenate((np.arange(st), np.arange(ed, n_particles)))\n",
    "            elif phases_dict['material'][i] == 'fluid':\n",
    "                attr[st:ed, 1] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.arange(n_particles)\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported materials\")\n",
    "                \n",
    "        elif args.env == 'Hairs':\n",
    "            # TODO: add relations between the hairs and the stick\n",
    "            pass\n",
    "            \n",
    "          #  if ed not in hairs_idx_begin:\n",
    "             #   queries = np.arange(st, ed)\n",
    "              #  anchors = np.arange(n_particles)\n",
    "\n",
    "        elif args.env == 'FluidFall' or args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            if phases_dict['material'][i] == 'fluid':\n",
    "                attr[st:ed, 0] = 1\n",
    "                queries = np.arange(st, ed)\n",
    "                anchors = np.arange(n_particles)\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported materials\")\n",
    "\n",
    "        else:\n",
    "            raise AssertionError(\"Unsupported materials\")\n",
    "\n",
    "        # st_time = time.time()\n",
    "        pos = positions\n",
    "        pos = pos[:, -3:]\n",
    "        if args.env == 'Hairs':\n",
    "            #TODO\n",
    "            pass\n",
    "        else:\n",
    "            rels += find_relations_neighbor(pos, queries, anchors, args.neighbor_radius, 2, var)\n",
    "            # return list of [receiver, sender, relation_type]\n",
    "        # print(\"Time on neighbor search\", time.time() - st_time)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Attr shape (after add env specific graph components):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "\n",
    "    rels = np.concatenate(rels, 0)\n",
    "    if rels.shape[0] > 0:\n",
    "        if verbose:\n",
    "            print(\"Relations neighbor\", rels.shape)\n",
    "        Rr_idxs.append(torch.LongTensor([rels[:, 0], np.arange(rels.shape[0])]))\n",
    "        Rs_idxs.append(torch.LongTensor([rels[:, 1], np.arange(rels.shape[0])]))\n",
    "        Ra = np.zeros((rels.shape[0], args.relation_dim))\n",
    "        Ras.append(torch.FloatTensor(Ra))\n",
    "        values.append(torch.FloatTensor([1] * rels.shape[0]))\n",
    "        node_r_idxs.append(np.arange(n_particles))\n",
    "        node_s_idxs.append(np.arange(n_particles + n_shapes))\n",
    "        psteps.append(args.pstep)\n",
    "\n",
    "    if verbose:\n",
    "        print('clusters', clusters)\n",
    "\n",
    "    # add heirarchical relations per instance\n",
    "    cnt_clusters = 0\n",
    "    for i in range(len(instance_idx) - 1):\n",
    "        st, ed = instance_idx[i], instance_idx[i + 1]\n",
    "        n_root_level = len(phases_dict[\"root_num\"][i])\n",
    "\n",
    "        if n_root_level > 0:\n",
    "            attr, positions, velocities, count_nodes, \\\n",
    "            rels, node_r_idx, node_s_idx, pstep = \\\n",
    "                    make_hierarchy(args.env, attr, positions, velocities, i, st, ed,\n",
    "                                   phases_dict, count_nodes, clusters[cnt_clusters], verbose, var)\n",
    "\n",
    "            for j in range(len(rels)):\n",
    "                if verbose:\n",
    "                    print(\"Relation instance\", j, rels[j].shape)\n",
    "                Rr_idxs.append(torch.LongTensor([rels[j][:, 0], np.arange(rels[j].shape[0])]))\n",
    "                Rs_idxs.append(torch.LongTensor([rels[j][:, 1], np.arange(rels[j].shape[0])]))\n",
    "                Ra = np.zeros((rels[j].shape[0], args.relation_dim)); Ra[:, 0] = 1\n",
    "                Ras.append(torch.FloatTensor(Ra))\n",
    "                values.append(torch.FloatTensor([1] * rels[j].shape[0]))\n",
    "                node_r_idxs.append(node_r_idx[j])\n",
    "                node_s_idxs.append(node_s_idx[j])\n",
    "                psteps.append(pstep[j])\n",
    "\n",
    "            cnt_clusters += 1\n",
    "\n",
    "    if verbose:\n",
    "        if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            print(\"Scene_params:\", scene_params)\n",
    "\n",
    "        print(\"Attr shape (after hierarchy building):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "        print(\"Particle attr:\", np.sum(attr[:n_particles], axis=0))\n",
    "        print(\"Shape attr:\", np.sum(attr[n_particles:n_particles+n_shapes], axis=0))\n",
    "        print(\"Roots attr:\", np.sum(attr[n_particles+n_shapes:], axis=0))\n",
    "\n",
    "    ### normalize data\n",
    "    data = [positions, velocities]\n",
    "    data = normalize(data, stat, var)\n",
    "    positions, velocities = data[0], data[1]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Particle positions stats\")\n",
    "        print(positions.shape)\n",
    "        print(np.min(positions[:n_particles], 0))\n",
    "        print(np.max(positions[:n_particles], 0))\n",
    "        print(np.mean(positions[:n_particles], 0))\n",
    "        print(np.std(positions[:n_particles], 0))\n",
    "\n",
    "        show_vel_dim = 6 if args.env == 'RiceGrip' else 3\n",
    "        print(\"Velocities stats\")\n",
    "        print(velocities.shape)\n",
    "        print(np.mean(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        print(np.std(velocities[:n_particles, :show_vel_dim], 0))\n",
    "\n",
    "    if args.env == 'RiceGrip':\n",
    "        if var:\n",
    "            quats = torch.cat(\n",
    "                [Variable(torch.zeros(n_particles, 4).cuda()), shape_quats,\n",
    "                 Variable(torch.zeros(count_nodes - n_particles - n_shapes, 4).cuda())], 0)\n",
    "            state = torch.cat([positions, velocities, quats], 1)\n",
    "        else:\n",
    "            quat_null = np.array([[0., 0., 0., 0.]])\n",
    "            quats = np.repeat(quat_null, [count_nodes], axis=0)\n",
    "            quats[n_particles:n_particles + n_shapes] = shape_quats\n",
    "            # if args.eval == 0:\n",
    "            # quats += np.random.randn(quats.shape[0], 4) * 0.05\n",
    "            state = torch.FloatTensor(np.concatenate([positions, velocities, quats], axis=1))\n",
    "    else:\n",
    "        if var:\n",
    "            state = torch.cat([positions, velocities], 1)\n",
    "        else:\n",
    "            state = torch.FloatTensor(np.concatenate([positions, velocities], axis=1))\n",
    "\n",
    "    if verbose:\n",
    "        for i in range(count_nodes - 1):\n",
    "            if np.sum(np.abs(attr[i] - attr[i + 1])) > 1e-6:\n",
    "                print(i, attr[i], attr[i + 1])\n",
    "\n",
    "        for i in range(len(Ras)):\n",
    "            print(i, np.min(node_r_idxs[i]), np.max(node_r_idxs[i]), np.min(node_s_idxs[i]), np.max(node_s_idxs[i]))\n",
    "\n",
    "    attr = torch.FloatTensor(attr)\n",
    "    relations = [Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps]\n",
    "\n",
    "    return attr, state, relations, n_particles, n_shapes, instance_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attr, state, relations, n_particles, n_shapes, instance_idx = \\\n",
    "        prepare_input(data, self.stat, self.args, self.phases_dict, self.verbose)\n",
    "\n",
    "### label\n",
    "data_nxt = normalize(load_data(self.data_names, data_nxt_path), self.stat)\n",
    "\n",
    "label = torch.FloatTensor(data_nxt[1][:n_particles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets['train'].__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
