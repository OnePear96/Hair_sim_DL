{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from models import DPINet\n",
    "from data import *\n",
    "#from data import PhysicsFleXDataset, collate_fn\n",
    "\n",
    "from utils import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--relation_dim'], dest='relation_dim', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--pstep', type=int, default=2)\n",
    "parser.add_argument('--n_rollout', type=int, default=0)\n",
    "parser.add_argument('--time_step', type=int, default=0)\n",
    "parser.add_argument('--time_step_clip', type=int, default=0)\n",
    "parser.add_argument('--dt', type=float, default=1./60.)\n",
    "parser.add_argument('--nf_relation', type=int, default=300)\n",
    "parser.add_argument('--nf_particle', type=int, default=200)\n",
    "parser.add_argument('--nf_effect', type=int, default=200)\n",
    "parser.add_argument('--env', default='')\n",
    "parser.add_argument('--train_valid_ratio', type=float, default=0.9)\n",
    "parser.add_argument('--outf', default='files')\n",
    "parser.add_argument('--dataf', default='data')\n",
    "parser.add_argument('--num_workers', type=int, default=10)\n",
    "parser.add_argument('--gen_data', type=int, default=0)\n",
    "parser.add_argument('--gen_stat', type=int, default=0)\n",
    "parser.add_argument('--log_per_iter', type=int, default=1000)\n",
    "parser.add_argument('--ckp_per_iter', type=int, default=10000)\n",
    "parser.add_argument('--eval', type=int, default=0)\n",
    "parser.add_argument('--verbose_data', type=int, default=1)\n",
    "parser.add_argument('--verbose_model', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--n_instance', type=int, default=0)\n",
    "parser.add_argument('--n_stages', type=int, default=0)\n",
    "parser.add_argument('--n_his', type=int, default=0)\n",
    "\n",
    "parser.add_argument('--n_epoch', type=int, default=1000)\n",
    "parser.add_argument('--beta1', type=float, default=0.9)\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--batch_size', type=int, default=1)\n",
    "parser.add_argument('--forward_times', type=int, default=2)\n",
    "\n",
    "parser.add_argument('--resume_epoch', type=int, default=0)\n",
    "parser.add_argument('--resume_iter', type=int, default=0)\n",
    "\n",
    "# shape state:\n",
    "# [x, y, z, x_last, y_last, z_last, quat(4), quat_last(4)]\n",
    "parser.add_argument('--shape_state_dim', type=int, default=14)\n",
    "\n",
    "# object attributes:\n",
    "parser.add_argument('--attr_dim', type=int, default=0)\n",
    "\n",
    "# object state:\n",
    "parser.add_argument('--state_dim', type=int, default=0)\n",
    "parser.add_argument('--position_dim', type=int, default=0)\n",
    "\n",
    "# relation attr:\n",
    "parser.add_argument('--relation_dim', type=int, default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"--env SingleHair --gen_data 0\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/data_SingleHair\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phases_dict = dict()\n",
    "\n",
    "args.n_rollout = 50\n",
    "args.num_workers = 3\n",
    "args.gen_stat = 1\n",
    "\n",
    "args.dataf = 'data'\n",
    "\n",
    "# object states:\n",
    "# [x, y, z, xdot, ydot, zdot]\n",
    "args.state_dim = 6\n",
    "args.position_dim = 3\n",
    "\n",
    "# object attr:\n",
    "# [rigid]\n",
    "args.attr_dim = 1\n",
    "\n",
    "# relation attr:\n",
    "# [none]\n",
    "args.relation_dim = 1\n",
    "\n",
    "args.time_step = 500\n",
    "args.time_step_clip = 20\n",
    "args.n_instance = 1\n",
    "args.n_stages = 1\n",
    "\n",
    "args.neighbor_radius = 0.08\n",
    "\n",
    "phases_dict[\"instance_idx\"] = [0, 30]\n",
    "phases_dict[\"root_num\"] = [[]]\n",
    "phases_dict[\"instance\"] = ['solid']\n",
    "phases_dict[\"material\"] = ['solid']\n",
    "\n",
    "data_names = ['positions', 'velocities','hair_idx']\n",
    "verbose = 0\n",
    "phase = 'train'\n",
    "data_dir = os.path.join(args.dataf, phase)\n",
    "stat_path = os.path.join(args.dataf, 'stat.h5')\n",
    "n_rollout = 10\n",
    "\n",
    "\n",
    "\n",
    "info = {\n",
    "    'env': args.env,\n",
    "    'root_num': phases_dict['root_num'],\n",
    "    'thread_idx': 0,\n",
    "    'data_dir': data_dir,\n",
    "    'data_names': data_names,\n",
    "    'n_rollout': n_rollout // args.num_workers,\n",
    "    'n_instance': args.n_instance,\n",
    "    'time_step': args.time_step,\n",
    "    'time_step_clip': args.time_step_clip,\n",
    "    'dt': args.dt,\n",
    "    'shape_state_dim': args.shape_state_dim}\n",
    "\n",
    "info['env_idx'] = 11\n",
    "\n",
    "args.outf = 'dump_SingleHair/' + args.outf\n",
    "\n",
    "args.outf = args.outf + '_' + args.env\n",
    "args.dataf = 'data/' + args.dataf + '_' + args.env\n",
    "print (args.dataf)\n",
    "os.system('mkdir -p ' + args.outf)\n",
    "os.system('mkdir -p ' + args.dataf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "\n",
    "def gen_PyFleX(info):\n",
    "\n",
    "    env, root_num = info['env'], info['root_num']\n",
    "    thread_idx, data_dir, data_names = info['thread_idx'], info['data_dir'], info['data_names']\n",
    "    n_rollout, n_instance = info['n_rollout'], info['n_instance']\n",
    "    time_step, time_step_clip = info['time_step'], info['time_step_clip']\n",
    "    shape_state_dim, dt = info['shape_state_dim'], info['dt']\n",
    "\n",
    "    env_idx = info['env_idx'] # =11\n",
    "\n",
    "    np.random.seed(round(time.time() * 1000 + thread_idx) % 2**32)\n",
    "    \n",
    "    stats = [init_stat(3), init_stat(3)]\n",
    "\n",
    "    import pyflex\n",
    "    pyflex.init()\n",
    "\n",
    "    for i in range(n_rollout):\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"%d / %d\" % (i, n_rollout))\n",
    "\n",
    "        rollout_idx = thread_idx * n_rollout + i\n",
    "        rollout_dir = os.path.join(data_dir, str(rollout_idx))\n",
    "        os.system('mkdir -p ' + rollout_dir)\n",
    "        \n",
    "        # scene_params: [len(box) at dim x,len(box) at dim y,len(box) at dim z, num_hair per circle, num_circle]\n",
    "        cap_size = [0.1,1.5]\n",
    "        N_hairs = 1\n",
    "\n",
    "\n",
    "        scene_params = np.array(cap_size)\n",
    "\n",
    "        pyflex.set_scene(env_idx, scene_params, thread_idx)\n",
    "        n_particles = pyflex.get_n_particles()\n",
    "        n_shapes = 1\n",
    "        N_particles_per_hair = int(n_particles/N_hairs)\n",
    "        idx_begins = np.arange(N_hairs)*N_particles_per_hair\n",
    "        idx_hairs = [[i,i+N_particles_per_hair-1] for i in idx_begins]\n",
    "\n",
    "        positions = np.zeros((time_step, n_particles+ n_shapes, 3), dtype=np.float32)\n",
    "        velocities = np.zeros((time_step, n_particles+ n_shapes, 3), dtype=np.float32)\n",
    "   #     shape_position = np.zeros((time_step, n_shapes, 3), dtype=np.float32)\n",
    "    #    shape_velocities = np.zeros((time_step, n_shapes, 3), dtype=np.float32)\n",
    "\n",
    "        for j in range(time_step_clip):\n",
    "         #   p_clip = pyflex.get_positions().reshape(-1, 4)[:, :3]\n",
    "         #   shape_p_clip = pyflex.get_shape_states()[:3].reshape(-1,3)\n",
    "            p_clip = np.concatenate([pyflex.get_positions().reshape(-1, 4)[:, :3],pyflex.get_shape_states()[:3].reshape(-1,3)],axis = 0)\n",
    "            pyflex.step()\n",
    "\n",
    "        for j in range(time_step):\n",
    "            positions[j, :n_particles] = pyflex.get_positions().reshape(-1, 4)[:, :3]\n",
    "            for k in range(n_shapes):\n",
    "                 positions[j, n_particles + k] = pyflex.get_shape_states()[:3]\n",
    "        #    shape_position[j] = pyflex.get_shape_states()[:3].reshape(-1,3)\n",
    "        #    shape_prevposition = pyflex.get_shape_states()[3:6].reshape(-1,3)\n",
    "            if j == 0:\n",
    "                velocities[j] = (positions[j] - p_clip) / dt\n",
    "           #     shape_velocities[j] = (shape_position[j] - shape_p_clip)/dt\n",
    "            else:\n",
    "                velocities[j] = (positions[j] - positions[j - 1]) / dt\n",
    "          #      shape_velocities[j] = (shape_position[j] - shape_position[j-1])/dt\n",
    "\n",
    "            pyflex.step()\n",
    "            data = [positions[j], velocities[j], idx_hairs]\n",
    "            store_data(data_names, data, os.path.join(rollout_dir, str(j) + '.h5'))\n",
    "        \n",
    "        # change dtype for more accurate stat calculation\n",
    "        # only normalize positions and velocities\n",
    "        datas = [positions.astype(np.float64), velocities.astype(np.float64)]\n",
    "\n",
    "        for j in range(len(stats)): \n",
    "            # here j = 2, refers to positions and velocities\n",
    "            stat = init_stat(stats[j].shape[0]) \n",
    "            # stat= np.zeros((3,3))\n",
    "            stat[:, 0] = np.mean(datas[j], axis=(0, 1))[:]\n",
    "            stat[:, 1] = np.std(datas[j], axis=(0, 1))[:]\n",
    "            stat[:, 2] = datas[j].shape[0] * datas[j].shape[1] # time_step*n_particles\n",
    "            stats[j] = combine_stat(stats[j], stat)\n",
    "\n",
    "    pyflex.clean()\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsFleXDataset(Dataset):\n",
    "\n",
    "    def __init__(self, args, phase, phases_dict, verbose):\n",
    "        self.args = args\n",
    "        self.phase = phase\n",
    "        self.phases_dict = phases_dict\n",
    "        self.verbose = verbose\n",
    "        self.data_dir = os.path.join(self.args.dataf, phase)\n",
    "        self.stat_path = os.path.join(self.args.dataf, 'stat.h5')\n",
    "\n",
    "        os.system('mkdir -p ' + self.data_dir)\n",
    "\n",
    "        #    self.data_names = ['positions', 'velocities', 'shape_quats', 'clusters', 'scene_params']\n",
    "        self.data_names = ['positions', 'velocities','hair_idx']\n",
    "\n",
    "        ratio = self.args.train_valid_ratio\n",
    "        if phase == 'train':\n",
    "            self.n_rollout = int(self.args.n_rollout * ratio)\n",
    "        elif phase == 'valid':\n",
    "            self.n_rollout = self.args.n_rollout - int(self.args.n_rollout * ratio)\n",
    "        else:\n",
    "            raise AssertionError(\"Unknown phase\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_rollout * (self.args.time_step - 1)\n",
    "\n",
    "    def load_data(self, name):\n",
    "        self.stat = load_data(self.data_names[:2], self.stat_path)\n",
    "        for i in range(len(self.stat)):\n",
    "            self.stat[i] = self.stat[i][-self.args.position_dim:, :]\n",
    "            # print(self.data_names[i], self.stat[i].shape)\n",
    "\n",
    "    def gen_data(self, name):\n",
    "        # if the data hasn't been generated, generate the data\n",
    "        print(\"Generating data ... n_rollout=%d, time_step=%d\" % (self.n_rollout, self.args.time_step))\n",
    "\n",
    "        infos = []\n",
    "        for i in range(self.args.num_workers):\n",
    "            info = {\n",
    "                'env': self.args.env,\n",
    "                'root_num': self.phases_dict['root_num'],\n",
    "                'thread_idx': i,\n",
    "                'data_dir': self.data_dir,\n",
    "                'data_names': self.data_names,\n",
    "                'n_rollout': self.n_rollout // self.args.num_workers,\n",
    "                'n_instance': self.args.n_instance,\n",
    "                'time_step': self.args.time_step,\n",
    "                'time_step_clip': self.args.time_step_clip,\n",
    "                'dt': self.args.dt,\n",
    "                'shape_state_dim': self.args.shape_state_dim}\n",
    "\n",
    "            info['env_idx'] = 11\n",
    "            infos.append(info)\n",
    "\n",
    "        cores = self.args.num_workers\n",
    "        pool = mp.Pool(processes=cores)\n",
    "        data = pool.map(gen_PyFleX, infos)\n",
    "\n",
    "        print(\"Training data generated, warpping up stats ...\")\n",
    "\n",
    "        if self.phase == 'train' and self.args.gen_stat:\n",
    "            # positions [x, y, z], velocities[xdot, ydot, zdot]\n",
    "            self.stat = [init_stat(3), init_stat(3)]\n",
    "            for i in range(len(data)):\n",
    "                for j in range(len(self.stat)):\n",
    "                    self.stat[j] = combine_stat(self.stat[j], data[i][j])\n",
    "            store_data(self.data_names[:2], self.stat, self.stat_path)\n",
    "        else:\n",
    "            print(\"Loading stat from %s ...\" % self.stat_path)\n",
    "            self.stat = load_data(self.data_names[:2], self.stat_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_rollout = idx // (self.args.time_step - 1)\n",
    "        idx_timestep = idx % (self.args.time_step - 1)\n",
    "\n",
    "        # ignore the first frame for env RiceGrip\n",
    "        if self.args.env == 'RiceGrip' and idx_timestep == 0:\n",
    "            idx_timestep = np.random.randint(1, self.args.time_step - 1)\n",
    "\n",
    "        data_path = os.path.join(self.data_dir, str(idx_rollout), str(idx_timestep) + '.h5')\n",
    "        data_nxt_path = os.path.join(self.data_dir, str(idx_rollout), str(idx_timestep + 1) + '.h5')\n",
    "\n",
    "        data = load_data(self.data_names, data_path)\n",
    "\n",
    "        '''\n",
    "        vel_his = []\n",
    "        for i in range(self.args.n_his):\n",
    "            path = os.path.join(self.data_dir, str(idx_rollout), str(max(1, idx_timestep - i - 1)) + '.h5')\n",
    "            data_his = load_data(self.data_names, path)\n",
    "            vel_his.append(data_his[1])\n",
    "\n",
    "        data[1] = np.concatenate([data[1]] + vel_his, 1)\n",
    "        '''\n",
    "        \n",
    "        attr, state, relations, n_particles, n_shapes = \\\n",
    "                prepare_input(data, self.stat, self.args, self.phases_dict, self.verbose)\n",
    "\n",
    "        ### label\n",
    "        data_nxt = normalize(load_data(self.data_names, data_nxt_path), self.stat)\n",
    "\n",
    "        label = torch.FloatTensor(data_nxt[1][:n_particles])\n",
    "\n",
    "        return attr, state, relations, n_particles, n_shapes, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(data, stat, args, phases_dict, verbose=0, var=False):\n",
    "    '''\n",
    "    for a single hair\n",
    "    '''\n",
    "    positions, velocities, hairs_idx = data\n",
    "    n_shapes = 1\n",
    "    hairs_idx_begin = [idx[0] for idx in hairs_idx]\n",
    "    n_particles = positions.shape[0] - n_shapes\n",
    "    R = 0.1\n",
    "    \n",
    "    ### object attributes\n",
    "    #   dim 10: [rigid, fluid, root_0, root_1, gripper_0, gripper_1, mass_inv,\n",
    "    #            clusterStiffness, clusterPlasticThreshold, cluasterPlasticCreep]\n",
    "    #   here we only consider the hairs but not the gripper, attr_dim = 1, attr = 0 for hair, attr = 1 for shapes\n",
    "    attr = np.zeros((n_particles+n_shapes, args.attr_dim))\n",
    "    \n",
    "    ### construct relations\n",
    "    Rr_idxs = []        # relation receiver idx list\n",
    "    Rs_idxs = []        # relation sender idx list\n",
    "    Ras = []            # relation attributes list\n",
    "    values = []         # relation value list (should be 1)\n",
    "    node_r_idxs = []    # list of corresponding receiver node idx\n",
    "    node_s_idxs = []    # list of corresponding sender node idx\n",
    "    psteps = []         # propagation steps\n",
    "    \n",
    "    ##### add env specific graph components\n",
    "    ### specify for shapes\n",
    "    rels = []\n",
    "    vals = []\n",
    "    \n",
    "    for i in range(n_shapes):\n",
    "        attr[n_particles+i, 0] = 1\n",
    "        dis = np.linalg.norm(positions[:n_particles,:2]-positions[n_particles+i,:2],axis = 1)\n",
    "        nodes_rel = np.nonzero(dis <= R)[0]\n",
    "        # for relation between hair nodes and a gripper, we note it as 1\n",
    "        gripper = np.ones(nodes_rel.shape[0], dtype=np.int) * (n_particles+i)\n",
    "        rels += [np.stack([nodes_rel, gripper, np.ones(nodes_rel.shape[0])], axis=1)]\n",
    "        vals += [np.ones(nodes_rel.shape[0], dtype=np.int)]\n",
    "        \n",
    "    \n",
    "    ##### add relations between leaf particles\n",
    "    ## here we only consider the relations in a hair: the relation between a node and the nodes nearby\n",
    "    ## simple case for one hair, TEMPORARY 2 rels for one link\n",
    "    nodes_p = np.arange(n_particles-1)\n",
    "    val = np.linalg.norm(positions[1:n_particles]-positions[:n_particles-1],axis = 1)\n",
    "    R1 = np.stack([nodes_p,nodes_p+1, np.zeros(n_particles-1)],axis = 1)\n",
    "    R2 = np.stack([nodes_p+1,nodes_p, np.zeros(n_particles-1)],axis = 1)\n",
    "    rels += [np.concatenate([R1,R2],axis = 0)]\n",
    "    vals += [val,val]\n",
    "    \n",
    "    rels = np.concatenate(rels, 0)\n",
    "    vals = np.concatenate(vals, 0)\n",
    "    \n",
    "  #  print (vals.shape)\n",
    " #   print (rels.shape)\n",
    "    \n",
    "    \n",
    "    if rels.shape[0] > 0:\n",
    "        if verbose:\n",
    "            print(\"Relations neighbor\", rels.shape)\n",
    "        Rr_idxs.append(torch.LongTensor([rels[:, 0], np.arange(rels.shape[0])]))\n",
    "        Rs_idxs.append(torch.LongTensor([rels[:, 1], np.arange(rels.shape[0])]))\n",
    "        # Ra: relation attributes\n",
    "    #    Ra = np.zeros((rels.shape[0], args.relation_dim))  \n",
    "        Ra = rels[:,2].reshape([-1,1])\n",
    "        Ras.append(torch.FloatTensor(Ra))\n",
    "        # values could be changed\n",
    "     #   values.append(torch.FloatTensor([1] * rels.shape[0]))\n",
    "      #  values.append(rels[:,2])\n",
    "        #### for hairs: values equals to the length of this segment\n",
    "        values.append(torch.FloatTensor(vals))\n",
    "        node_r_idxs.append(np.arange(n_particles))\n",
    "        node_s_idxs.append(np.arange(n_particles + n_shapes))\n",
    "        psteps.append(args.pstep)\n",
    "        \n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Attr shape (after hierarchy building):\", attr.shape)\n",
    "        print(\"Object attr:\", np.sum(attr, axis=0))\n",
    "        print(\"Particle attr:\", np.sum(attr[:n_particles], axis=0))\n",
    "        print(\"Shape attr:\", np.sum(attr[n_particles:n_particles+n_shapes], axis=0))\n",
    "        print(\"Roots attr:\", np.sum(attr[n_particles+n_shapes:], axis=0))\n",
    "        \n",
    "        \n",
    "    ### normalize data\n",
    "    data = [positions, velocities]\n",
    "    data = normalize(data, stat, var)\n",
    "    positions, velocities = data[0], data[1]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Particle positions stats\")\n",
    "        print(positions.shape)\n",
    "        print(np.min(positions[:n_particles], 0))\n",
    "        print(np.max(positions[:n_particles], 0))\n",
    "        print(np.mean(positions[:n_particles], 0))\n",
    "        print(np.std(positions[:n_particles], 0))\n",
    "\n",
    "        show_vel_dim = 6 if args.env == 'RiceGrip' else 3\n",
    "        print(\"Velocities stats\")\n",
    "        print(velocities.shape)\n",
    "        print(np.mean(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        print(np.std(velocities[:n_particles, :show_vel_dim], 0))\n",
    "        \n",
    "    state = torch.FloatTensor(np.concatenate([positions, velocities], axis=1))\n",
    "    attr = torch.FloatTensor(attr)\n",
    "    relations = [Rr_idxs, Rs_idxs, values, Ras, node_r_idxs, node_s_idxs, psteps]\n",
    "\n",
    "    return attr, state, relations, n_particles, n_shapes#, instance_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {phase: PhysicsFleXDataset(\n",
    "    args, phase, phases_dict, args.verbose_data) for phase in ['train', 'valid']}\n",
    "datasets['train'].load_data(args.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {x: torch.utils.data.DataLoader(\n",
    "    datasets[x], batch_size=args.batch_size,\n",
    "    shuffle=True if x == 'train' else False,\n",
    "    num_workers=args.num_workers,\n",
    "    collate_fn=collate_fn)\n",
    "    for x in ['train', 'valid']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "\n",
    "class DPINet(nn.Module):\n",
    "    def __init__(self, args, stat, phases_dict, residual=False, use_gpu=False):\n",
    "        super(DPINet, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        state_dim = args.state_dim\n",
    "        attr_dim = args.attr_dim\n",
    "        relation_dim = args.relation_dim\n",
    "        nf_particle = 100#args.nf_particle\n",
    "        nf_relation = 150 #args.nf_relation\n",
    "        nf_effect = 150 #args.nf_effect\n",
    "\n",
    "        self.nf_effect = nf_effect\n",
    "\n",
    "        self.stat = stat\n",
    "        self.use_gpu = use_gpu\n",
    "        self.residual = residual \n",
    "\n",
    "        # (1) particle attr (2) state\n",
    "        self.particle_encoder = ParticleEncoder(attr_dim + state_dim, nf_particle, nf_effect)\n",
    "\n",
    "        # (1) sender attr (2) receiver attr (3) state receiver (4) state_diff (5) relation attr\n",
    "        self.relation_encoder = RelationEncoder( 2 * attr_dim + 2 * state_dim + relation_dim, nf_relation, nf_relation)\n",
    "\n",
    "        # (1) relation encode (2) sender effect (3) receiver effect\n",
    "        self.relation_propagator = Propagator(nf_relation + 2 * nf_effect, nf_effect)\n",
    "\n",
    "        # (1) particle encode (2) particle effect\n",
    "        self.particle_propagator = Propagator(2 * nf_effect, nf_effect)\n",
    "\n",
    "        # (1) set particle effect\n",
    "        self.particle_predictor = ParticlePredictor(nf_effect, nf_effect, args.position_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, attr, state, Rr, Rs, Ra, n_particles, node_r_idx, node_s_idx, pstep,\n",
    "                instance_idx, phases_dict, verbose=0):\n",
    "        \n",
    "        # calculate particle encoding\n",
    "        if self.use_gpu:\n",
    "            particle_effect = Variable(torch.zeros((attr.size(0), self.nf_effect)).cuda())\n",
    "        else:\n",
    "            particle_effect = Variable(torch.zeros((attr.size(0), self.nf_effect)))\n",
    "            \n",
    "        s = 0\n",
    "        Rrp = Rr[s].t()\n",
    "        Rsp = Rs[s].t()\n",
    "\n",
    "        # receiver_attr, sender_attr\n",
    "        attr_r = attr[node_r_idx[s]]\n",
    "        attr_s = attr[node_s_idx[s]]\n",
    "        attr_r_rel = Rrp.mm(attr_r)\n",
    "        attr_s_rel = Rsp.mm(attr_s)\n",
    "\n",
    "        # receiver_state, sender_state\n",
    "        state_r = state[node_r_idx[s]]\n",
    "        state_s = state[node_s_idx[s]]\n",
    "        state_r_rel = Rrp.mm(state_r)\n",
    "        state_s_rel = Rsp.mm(state_s)\n",
    "        state_diff = state_r_rel - state_s_rel\n",
    "        \n",
    "        particle_encode = self.particle_encoder(torch.cat([attr_r, state_r], 1))\n",
    "        relation_encode = self.relation_encoder(\n",
    "                torch.cat([attr_r_rel, attr_s_rel, state_r_rel, state_s_rel, Ra[s]], 1))\n",
    "        \n",
    "        for i in range(pstep):\n",
    "            effect_p_r = particle_effect[node_r_idx[s]]\n",
    "            effect_p_s = particle_effect[node_s_idx[s]]\n",
    "\n",
    "            receiver_effect = Rrp.mm(effect_p_r)\n",
    "            sender_effect = Rsp.mm(effect_p_s)\n",
    "            \n",
    "\n",
    "            # calculate relation effect\n",
    "            effect_rel = self.relation_propagator(\n",
    "                torch.cat([relation_encode, receiver_effect, sender_effect], 1))\n",
    "\n",
    "            # calculate particle effect by aggregating relation effect\n",
    "            effect_p_r_agg = Rr[s].mm(effect_rel)\n",
    "\n",
    "            # calculate particle effect\n",
    "            effect_p = self.particle_propagator(\n",
    "                torch.cat([particle_encode, effect_p_r_agg], 1),\n",
    "                res=effect_p_r)\n",
    "            particle_effect[node_r_idx[s]] = effect_p\n",
    "            \n",
    "        pred = self.particle_predictor(particle_effect[:31])\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 222203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DPINet(\n",
       "  (particle_encoder): ParticleEncoder(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=7, out_features=100, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=100, out_features=150, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (relation_encoder): RelationEncoder(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=15, out_features=150, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=150, out_features=150, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=150, out_features=150, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (relation_propagator): Propagator(\n",
       "    (linear): Linear(in_features=450, out_features=150, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (particle_propagator): Propagator(\n",
       "    (linear): Linear(in_features=300, out_features=150, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (particle_predictor): ParticlePredictor(\n",
       "    (linear_0): Linear(in_features=150, out_features=150, bias=True)\n",
       "    (linear_1): Linear(in_features=150, out_features=150, bias=True)\n",
       "    (linear_2): Linear(in_features=150, out_features=3, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.pstep = 3\n",
    "pstep = 3\n",
    "use_gpu = True\n",
    "model = DPINet(args, datasets['train'].stat, phases_dict, residual=True, use_gpu=use_gpu)\n",
    "print(\"Number of parameters: %d\" % count_parameters(model))\n",
    "# criterion\n",
    "criterionMSE = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, betas=(args.beta1, 0.999))\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.8, patience=3, verbose=True)\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    criterionMSE = criterionMSE.cuda()\n",
    "\n",
    "st_epoch = args.resume_epoch if args.resume_epoch > 0 else 0\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "model.train(phase=='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [0/100][0/22455] n_relations: 60, Loss: 1.701027, Agg: 1.701027\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-24f677ad12be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_particles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mnode_r_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_s_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 instance_idx, phases_dict, 0)\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0;31m# print('Time forward', time.time() - st_time)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/onepear/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-6de93df3c242>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, attr, state, Rr, Rs, Ra, n_particles, node_r_idx, node_s_idx, pstep, instance_idx, phases_dict, verbose)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m# calculate particle effect by aggregating relation effect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0meffect_p_r_agg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meffect_rel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m# calculate particle effect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "phase = 'train'\n",
    "instance_idx = [0, 31]\n",
    "args.n_epoch = 10000\n",
    "psteps = 3\n",
    "\n",
    "for epoch in range(args.n_epoch):\n",
    "    model.train(phase=='train')\n",
    "\n",
    "    losses = 0.\n",
    "    for i, data in enumerate(dataloaders[phase]):\n",
    "#         print ('i:',i)\n",
    "\n",
    "        attr, state, rels, n_particles, n_shapes, label = data\n",
    "        Ra, node_r_idx, node_s_idx, pstep = rels[3], rels[4], rels[5], rels[6]\n",
    "\n",
    "        Rr, Rs = [], []\n",
    "        for j in range(len(rels[0])):\n",
    "            Rr_idx, Rs_idx, values = rels[0][j], rels[1][j], rels[2][j]\n",
    "            Rr.append(torch.sparse.FloatTensor(\n",
    "                Rr_idx, values, torch.Size([node_r_idx[j].shape[0], Ra[j].size(0)])))\n",
    "            Rs.append(torch.sparse.FloatTensor(\n",
    "                Rs_idx, values, torch.Size([node_s_idx[j].shape[0], Ra[j].size(0)])))\n",
    "\n",
    "        data = [attr, state, Rr, Rs, Ra, label]\n",
    "        \n",
    "\n",
    "            # st_time = time.time()\n",
    "        with torch.set_grad_enabled(phase=='train'):\n",
    "            if use_gpu:\n",
    "                for d in range(len(data)):\n",
    "                    if type(data[d]) == list:\n",
    "                        for t in range(len(data[d])):\n",
    "                            data[d][t] = Variable(data[d][t].cuda())\n",
    "                    else:\n",
    "                        data[d] = Variable(data[d].cuda())\n",
    "            else:\n",
    "                for d in range(len(data)):\n",
    "                    if type(data[d]) == list:\n",
    "                        for t in range(len(data[d])):\n",
    "                            data[d][t] = Variable(data[d][t])\n",
    "                    else:\n",
    "                        data[d] = Variable(data[d])\n",
    "\n",
    "            attr, state, Rr, Rs, Ra, label = data\n",
    "            \n",
    "            pstep = 3\n",
    "\n",
    "            predicted = model(\n",
    "                attr, state, Rr, Rs, Ra, n_particles,\n",
    "                node_r_idx, node_s_idx, pstep,\n",
    "                instance_idx, phases_dict, 0)\n",
    "            # print('Time forward', time.time() - st_time)\n",
    "\n",
    "       #     print(predicted.shape)\n",
    "       #     print(label.shape)\n",
    "\n",
    "        loss = criterionMSE(predicted, label)\n",
    "        losses += np.sqrt(loss.item())\n",
    "\n",
    "        if phase == 'train':\n",
    "            if i % 5 == 0:\n",
    "                # update parameters every args.forward_times\n",
    "          #      print ('update!')\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "          #      print ('done')\n",
    "\n",
    "\n",
    "        if i % 100000 == 0:\n",
    "            n_relations = 0\n",
    "            for j in range(len(Ra)):\n",
    "                n_relations += Ra[j].size(0)\n",
    "            print('%s [%d/%d][%d/%d] n_relations: %d, Loss: %.6f, Agg: %.6f' %\n",
    "                  (phase, epoch, 100, i, len(dataloaders[phase]),\n",
    "                   n_relations, np.sqrt(loss.item()), losses / (i + 1)))\n",
    "\n",
    "      #  if phase == 'train' and i > 0 and i % args.ckp_per_iter == 0:\n",
    "    if epoch % 10 == 0 :\n",
    "        torch.save(model.state_dict(), '%s/net_epoch_%d.pth' % (args.outf, epoch))\n",
    "\n",
    "    losses /= len(dataloaders[phase])\n",
    "    print('%s [%d/%d] Loss: %.4f, Best valid: %.4f' %\n",
    "          (phase, epoch, args.n_epoch, losses, best_valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_path = stat_path = os.path.join(args.dataf, 'stat.h5')\n",
    "stat = load_data(data_names[:2], stat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "model = DPINet(args, stat, phases_dict, residual=True, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = os.path.join(args.outf, 'net_epoch_%d.pth' % (500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network from dump_SingleHair/files_SingleHair/net_epoch_500.pth\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading network from %s\" % model_file)\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "model.eval()\n",
    "\n",
    "criterionMSE = nn.MSELoss()\n",
    "\n",
    "if use_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "\n",
    "for step in range(args.time_step - 1):\n",
    "    data_path = os.path.join(args.dataf, 'valid', str(idx), str(step) + '.h5')\n",
    "    data_nxt_path = os.path.join(args.dataf, 'valid', str(idx), str(step + 1) + '.h5')\n",
    "\n",
    "    data = load_data(data_names, data_path)\n",
    "    data_nxt = load_data(data_names, data_nxt_path)\n",
    "    velocities_nxt = data_nxt[1]\n",
    "\n",
    "    if step == 0:\n",
    "        positions, velocities, hairs_idx = data\n",
    "        n_shapes = 1\n",
    "        scene_params = 11\n",
    "        count_nodes = positions.shape[0]\n",
    "        n_particles = count_nodes - n_shapes\n",
    "        p_gt = np.zeros((args.time_step - 1, n_particles + n_shapes, args.position_dim))\n",
    "        s_gt = np.zeros((args.time_step - 1, n_shapes, args.shape_state_dim))\n",
    "        v_nxt_gt = np.zeros((args.time_step - 1, n_particles + n_shapes, args.position_dim))\n",
    "\n",
    "        p_pred = np.zeros((args.time_step - 1, n_particles + n_shapes, args.position_dim))\n",
    "\n",
    "    p_gt[step] = positions[:, -args.position_dim:]\n",
    "    v_nxt_gt[step] = velocities_nxt[:, -args.position_dim:]\n",
    "    \n",
    "    s_gt[step, :, :3] = positions[n_particles:, :3]\n",
    "    s_gt[step, :, 3:6] = p_gt[max(0, step-1), n_particles:, :3]\n",
    "    s_gt[step, :, 6:] = np.array( [[0.,0.70710677,0,0.70710677,0,0.70710677, 0, 0.70710677]])\n",
    "    positions = positions + velocities_nxt * args.dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 4.        , 0.        , 0.        ],\n",
       "       [0.        , 3.88348913, 0.        , 0.        ],\n",
       "       [0.        , 3.76832294, 0.        , 0.        ],\n",
       "       [0.        , 3.64844704, 0.        , 0.        ],\n",
       "       [0.        , 3.53010917, 0.        , 0.        ],\n",
       "       [0.        , 3.4117341 , 0.        , 0.        ],\n",
       "       [0.        , 3.29377675, 0.        , 0.        ],\n",
       "       [0.        , 3.17614532, 0.        , 0.        ],\n",
       "       [0.        , 3.05889988, 0.        , 0.        ],\n",
       "       [0.        , 2.94205379, 0.        , 0.        ],\n",
       "       [0.        , 2.82562876, 0.        , 0.        ],\n",
       "       [0.        , 2.70964026, 0.        , 0.        ],\n",
       "       [0.        , 2.59410095, 0.        , 0.        ],\n",
       "       [0.        , 2.47902036, 0.        , 0.        ],\n",
       "       [0.        , 2.36440516, 0.        , 0.        ],\n",
       "       [0.        , 2.25025892, 0.        , 0.        ],\n",
       "       [0.        , 2.13658237, 0.        , 0.        ],\n",
       "       [0.        , 2.02337432, 0.        , 0.        ],\n",
       "       [0.        , 1.91063178, 0.        , 0.        ],\n",
       "       [0.        , 1.79834974, 0.        , 0.        ],\n",
       "       [0.        , 1.68652236, 0.        , 0.        ],\n",
       "       [0.        , 1.57514274, 0.        , 0.        ],\n",
       "       [0.        , 1.46420372, 0.        , 0.        ],\n",
       "       [0.        , 1.35369825, 0.        , 0.        ],\n",
       "       [0.        , 1.24361885, 0.        , 0.        ],\n",
       "       [0.        , 1.13395953, 0.        , 0.        ],\n",
       "       [0.        , 1.02471292, 0.        , 0.        ],\n",
       "       [0.        , 0.91587913, 0.        , 0.        ],\n",
       "       [0.        , 0.80743492, 0.        , 0.        ],\n",
       "       [0.        , 0.6994499 , 0.        , 0.        ],\n",
       "       [0.        , 0.5916518 , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 0\n",
    "mass = np.zeros((n_particles, 1))\n",
    "p = np.concatenate([p_gt[step, :n_particles], mass], 1)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyflex\n",
    "pyflex.init()\n",
    "\n",
    "cap_size = [0.1,1.5]\n",
    "N_hairs = 1\n",
    "env_idx = 12\n",
    "scene_params = np.array(cap_size)\n",
    "pyflex.set_scene(env_idx, scene_params, 0)\n",
    "\n",
    "for step in range(args.time_step - 1):\n",
    "    pyflex.set_shape_states(s_gt[step])\n",
    "\n",
    "    mass = np.zeros((n_particles, 1))\n",
    "    p = np.concatenate([p_gt[step, :n_particles], mass], 1)\n",
    "\n",
    "    pyflex.set_positions(p)\n",
    "    pyflex.render()\n",
    "    \n",
    "pyflex.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "data_path = os.path.join(args.dataf, 'valid', str(idx), '0.h5')\n",
    "data = load_data(data_names, data_path)\n",
    "instance_idx = [0, 31]\n",
    "p_pred = np.zeros((args.time_step - 1, n_particles, args.position_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(args.time_step - 1):\n",
    "    p_pred[step] = data[0][:n_particles]\n",
    "    attr, state, rels, n_particles, n_shapes = prepare_input(data, stat, args, phases_dict, 0)\n",
    "    Ra, node_r_idx, node_s_idx, pstep = rels[3], rels[4], rels[5], rels[6]\n",
    "\n",
    "    Rr, Rs = [], []\n",
    "    for j in range(len(rels[0])):\n",
    "        Rr_idx, Rs_idx, values = rels[0][j], rels[1][j], rels[2][j]\n",
    "        Rr.append(torch.sparse.FloatTensor(\n",
    "            Rr_idx, values, torch.Size([node_r_idx[j].shape[0], Ra[j].size(0)])))\n",
    "        Rs.append(torch.sparse.FloatTensor(\n",
    "            Rs_idx, values, torch.Size([node_s_idx[j].shape[0], Ra[j].size(0)])))\n",
    "\n",
    "    buf = [attr, state, Rr, Rs, Ra]\n",
    "\n",
    "\n",
    "        # st_time = time.time()\n",
    "    with torch.set_grad_enabled(phase=='train'):\n",
    "        if use_gpu:\n",
    "            for d in range(len(buf)):\n",
    "                if type(buf[d]) == list:\n",
    "                    for t in range(len(buf[d])):\n",
    "                        buf[d][t] = Variable(buf[d][t].cuda())\n",
    "                else:\n",
    "                    buf[d] = Variable(buf[d].cuda())\n",
    "        else:\n",
    "            for d in range(len(buf)):\n",
    "                if type(buf[d]) == list:\n",
    "                    for t in range(len(buf[d])):\n",
    "                        buf[d][t] = Variable(buf[d][t])\n",
    "                else:\n",
    "                    buf[d] = Variable(buf[d])\n",
    "\n",
    "        attr, state, Rr, Rs, Ra = buf\n",
    "\n",
    "        pstep = 3\n",
    "\n",
    "        predicted = model(\n",
    "            attr, state, Rr, Rs, Ra, n_particles,\n",
    "            node_r_idx, node_s_idx, pstep,\n",
    "            instance_idx, phases_dict, 0)\n",
    "        \n",
    "        vels = denormalize([predicted.data.cpu().numpy()], [stat[1]])[0]\n",
    "        data[0][:n_particles] += vels * args.dt\n",
    "        data[1][:n_particles] = vels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.24024999e-04,  4.00099945e+00, -6.39032578e-06],\n",
       "       [ 5.82799490e-04,  3.89558935e+00, -2.69473330e-05],\n",
       "       [ 3.54904441e-05,  3.78641129e+00, -5.59073032e-06],\n",
       "       [ 1.81983385e-04,  3.67839432e+00, -1.61776450e-06],\n",
       "       [-2.25596479e-04,  3.56450057e+00, -4.03137938e-06],\n",
       "       [-2.66419403e-04,  3.45700550e+00,  1.20745278e-06],\n",
       "       [-2.04477197e-04,  3.34188795e+00, -9.99670419e-07],\n",
       "       [-1.25723836e-05,  3.23407626e+00, -2.90207396e-07],\n",
       "       [ 2.39915225e-05,  3.12846613e+00, -1.72030730e-06],\n",
       "       [-7.33436536e-05,  3.01855302e+00, -2.50097582e-06],\n",
       "       [-1.47372106e-04,  2.90944147e+00, -2.40530676e-06],\n",
       "       [-1.99186092e-04,  2.80131030e+00, -1.47952471e-06],\n",
       "       [-2.59405613e-04,  2.69144487e+00, -2.49053573e-06],\n",
       "       [-3.06985225e-04,  2.58198762e+00, -3.90610649e-06],\n",
       "       [-3.75870964e-04,  2.47475171e+00, -3.70291468e-06],\n",
       "       [-3.58385616e-04,  2.36479306e+00, -3.70518319e-06],\n",
       "       [-4.64003184e-04,  2.24410725e+00, -3.58491593e-06],\n",
       "       [-3.43023945e-04,  2.13907266e+00, -5.57615476e-06],\n",
       "       [-4.69392253e-04,  2.04571962e+00, -3.62536139e-06],\n",
       "       [-9.75983101e-04,  1.96861696e+00, -2.92379445e-06],\n",
       "       [-2.39154720e-03,  2.00306296e+00, -2.57355823e-05],\n",
       "       [-3.21573042e-03,  1.90031278e+00, -6.58966834e-04],\n",
       "       [-2.53519486e-03,  1.73874795e+00, -2.27048833e-04],\n",
       "       [-4.23065126e-02,  2.56146979e+00, -3.58212437e-03],\n",
       "       [ 2.79089863e+04, -1.07784664e+05,  1.25248545e+04],\n",
       "       [-2.03312824e+11,  1.22934927e+11, -1.41534996e+10],\n",
       "       [-1.46377393e+13,  2.22421362e+13,  4.87192684e+12],\n",
       "       [-6.21710861e+13,  1.37724426e+14, -6.44067046e+12],\n",
       "       [-2.62106409e+12,  2.91702650e+12,  1.00005446e+12],\n",
       "       [-1.21273550e+13,  1.33239008e+13, -9.63588784e+11],\n",
       "       [-1.90794340e+07,  1.26722500e+07,  3.70852725e+06]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_pred[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyflex\n",
    "pyflex.init()\n",
    "\n",
    "recs = []\n",
    "\n",
    "for idx in range(3):\n",
    "\n",
    "    print(\"Rollout %d / %d\" % (idx, 3))\n",
    "\n",
    "    # ground truth\n",
    "    for step in range(args.time_step - 1):\n",
    "        data_path = os.path.join(args.dataf, 'valid', str(infos[idx]), str(step) + '.h5')\n",
    "        data_nxt_path = os.path.join(args.dataf, 'valid', str(infos[idx]), str(step + 1) + '.h5')\n",
    "\n",
    "        data = load_data(data_names, data_path)\n",
    "        data_nxt = load_data(data_names, data_nxt_path)\n",
    "        velocities_nxt = data_nxt[1]\n",
    "\n",
    "        if step == 0:\n",
    "            if args.env == 'BoxBath':\n",
    "                positions, velocities, clusters = data\n",
    "                n_shapes = 0\n",
    "                scene_params = np.zeros(1)\n",
    "            elif args.env == 'FluidFall':\n",
    "                positions, velocities = data\n",
    "                n_shapes = 0\n",
    "                scene_params = np.zeros(1)\n",
    "            elif args.env == 'RiceGrip':\n",
    "                positions, velocities, shape_quats, clusters, scene_params = data\n",
    "                n_shapes = shape_quats.shape[0]\n",
    "            elif args.env == 'FluidShake':\n",
    "                positions, velocities, shape_quats, scene_params = data\n",
    "                n_shapes = shape_quats.shape[0]\n",
    "            else:\n",
    "                raise AssertionError(\"Unsupported env\")\n",
    "\n",
    "            count_nodes = positions.shape[0]\n",
    "            n_particles = count_nodes - n_shapes\n",
    "            print(\"n_particles\", n_particles)\n",
    "            print(\"n_shapes\", n_shapes)\n",
    "\n",
    "            p_gt = np.zeros((args.time_step - 1, n_particles + n_shapes, args.position_dim))\n",
    "            s_gt = np.zeros((args.time_step - 1, n_shapes, args.shape_state_dim))\n",
    "            v_nxt_gt = np.zeros((args.time_step - 1, n_particles + n_shapes, args.position_dim))\n",
    "\n",
    "            p_pred = np.zeros((args.time_step - 1, n_particles + n_shapes, args.position_dim))\n",
    "\n",
    "        p_gt[step] = positions[:, -args.position_dim:]\n",
    "        v_nxt_gt[step] = velocities_nxt[:, -args.position_dim:]\n",
    "\n",
    "        # print(step, np.sum(np.abs(v_nxt_gt[step, :args.n_particles])))\n",
    "\n",
    "        if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            s_gt[step, :, :3] = positions[n_particles:, :3]\n",
    "            s_gt[step, :, 3:6] = p_gt[max(0, step-1), n_particles:, :3]\n",
    "            s_gt[step, :, 6:10] = data[2]\n",
    "            s_gt[step, :, 10:] = data[2]\n",
    "\n",
    "        positions = positions + velocities_nxt * args.dt\n",
    "\n",
    "    # model rollout\n",
    "    data_path = os.path.join(args.dataf, 'valid', str(infos[idx]), '0.h5')\n",
    "    data = load_data(data_names, data_path)\n",
    "\n",
    "    for step in range(args.time_step - 1):\n",
    "        if step % 10 == 0:\n",
    "            print(\"Step %d / %d\" % (step, args.time_step - 1))\n",
    "\n",
    "        p_pred[step] = data[0]\n",
    "\n",
    "        if args.env == 'RiceGrip' and step == 0:\n",
    "            data[0] = p_gt[step + 1].copy()\n",
    "            data[1] = np.concatenate([v_nxt_gt[step]] * (args.n_his + 1), 1)\n",
    "            continue\n",
    "\n",
    "        # st_time = time.time()\n",
    "        attr, state, rels, n_particles, n_shapes, instance_idx = \\\n",
    "                prepare_input(data, stat, args, phases_dict, args.verbose_data)\n",
    "\n",
    "        Ra, node_r_idx, node_s_idx, pstep = rels[3], rels[4], rels[5], rels[6]\n",
    "\n",
    "        Rr, Rs = [], []\n",
    "        for j in range(len(rels[0])):\n",
    "            Rr_idx, Rs_idx, values = rels[0][j], rels[1][j], rels[2][j]\n",
    "            Rr.append(torch.sparse.FloatTensor(\n",
    "                Rr_idx, values, torch.Size([node_r_idx[j].shape[0], Ra[j].size(0)])))\n",
    "            Rs.append(torch.sparse.FloatTensor(\n",
    "                Rs_idx, values, torch.Size([node_s_idx[j].shape[0], Ra[j].size(0)])))\n",
    "\n",
    "        buf = [attr, state, Rr, Rs, Ra]\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            if use_gpu:\n",
    "                for d in range(len(buf)):\n",
    "                    if type(buf[d]) == list:\n",
    "                        for t in range(len(buf[d])):\n",
    "                            buf[d][t] = Variable(buf[d][t].cuda())\n",
    "                    else:\n",
    "                        buf[d] = Variable(buf[d].cuda())\n",
    "            else:\n",
    "                for d in range(len(buf)):\n",
    "                    if type(buf[d]) == list:\n",
    "                        for t in range(len(buf[d])):\n",
    "                            buf[d][t] = Variable(buf[d][t])\n",
    "                    else:\n",
    "                        buf[d] = Variable(buf[d])\n",
    "\n",
    "            attr, state, Rr, Rs, Ra = buf\n",
    "            # print('Time prepare input', time.time() - st_time)\n",
    "\n",
    "            # st_time = time.time()\n",
    "            vels = model(\n",
    "                attr, state, Rr, Rs, Ra, n_particles,\n",
    "                node_r_idx, node_s_idx, pstep,\n",
    "                instance_idx, phases_dict, args.verbose_model)\n",
    "            # print('Time forward', time.time() - st_time)\n",
    "\n",
    "            # print(vels)\n",
    "\n",
    "            if args.debug:\n",
    "                data_nxt_path = os.path.join(args.dataf, 'valid', str(infos[idx]), str(step + 1) + '.h5')\n",
    "                data_nxt = normalize(load_data(data_names, data_nxt_path), stat)\n",
    "                label = Variable(torch.FloatTensor(data_nxt[1][:n_particles]).cuda())\n",
    "                # print(label)\n",
    "                loss = np.sqrt(criterionMSE(vels, label).item())\n",
    "                print(loss)\n",
    "\n",
    "        vels = denormalize([vels.data.cpu().numpy()], [stat[1]])[0]\n",
    "\n",
    "        if args.env == 'RiceGrip' or args.env == 'FluidShake':\n",
    "            vels = np.concatenate([vels, v_nxt_gt[step, n_particles:]], 0)\n",
    "        data[0] = data[0] + vels * args.dt\n",
    "\n",
    "        if args.env == 'RiceGrip':\n",
    "            # shifting the history\n",
    "            # positions, restPositions\n",
    "            data[1][:, args.position_dim:] = data[1][:, :-args.position_dim]\n",
    "        data[1][:, :args.position_dim] = vels\n",
    "\n",
    "        if args.debug:\n",
    "            data[0] = p_gt[step + 1].copy()\n",
    "            data[1][:, :args.position_dim] = v_nxt_gt[step]\n",
    "\n",
    "    ##### render for the ground truth\n",
    "    pyflex.set_scene(env_idx, scene_params, 0)\n",
    "\n",
    "    if args.env == 'RiceGrip':\n",
    "        halfEdge = np.array([0.15, 0.8, 0.15])\n",
    "        center = np.array([0., 0., 0.])\n",
    "        quat = np.array([1., 0., 0., 0.])\n",
    "        pyflex.add_box(halfEdge, center, quat)\n",
    "        pyflex.add_box(halfEdge, center, quat)\n",
    "    elif args.env == 'FluidShake':\n",
    "        x, y, z, dim_x, dim_y, dim_z, box_dis_x, box_dis_z = scene_params\n",
    "        boxes = calc_box_init_FluidShake(box_dis_x, box_dis_z, height, border)\n",
    "\n",
    "        x_box = x + (dim_x-1)/2.*0.055\n",
    "\n",
    "        for box_idx in range(len(boxes) - 1):\n",
    "            halfEdge = boxes[box_idx][0]\n",
    "            center = boxes[box_idx][1]\n",
    "            quat = boxes[box_idx][2]\n",
    "            pyflex.add_box(halfEdge, center, quat)\n",
    "\n",
    "\n",
    "    for step in range(args.time_step - 1):\n",
    "        if args.env == 'RiceGrip':\n",
    "            pyflex.set_shape_states(s_gt[step])\n",
    "        elif args.env == 'FluidShake':\n",
    "            pyflex.set_shape_states(s_gt[step, :-1])\n",
    "\n",
    "        mass = np.zeros((n_particles, 1))\n",
    "        if args.env == 'RiceGrip':\n",
    "            p = np.concatenate([p_gt[step, :n_particles, -3:], mass], 1)\n",
    "        else:\n",
    "            p = np.concatenate([p_gt[step, :n_particles], mass], 1)\n",
    "\n",
    "        pyflex.set_positions(p)\n",
    "        pyflex.render(capture=0)\n",
    "\n",
    "    ##### render for the predictions\n",
    "    pyflex.set_scene(env_idx, scene_params, 0)\n",
    "\n",
    "    if args.env == 'RiceGrip':\n",
    "        pyflex.add_box(halfEdge, center, quat)\n",
    "        pyflex.add_box(halfEdge, center, quat)\n",
    "    elif args.env == 'FluidShake':\n",
    "        for box_idx in range(len(boxes) - 1):\n",
    "            halfEdge = boxes[box_idx][0]\n",
    "            center = boxes[box_idx][1]\n",
    "            quat = boxes[box_idx][2]\n",
    "            pyflex.add_box(halfEdge, center, quat)\n",
    "\n",
    "    for step in range(args.time_step - 1):\n",
    "        if args.env == 'RiceGrip':\n",
    "            pyflex.set_shape_states(s_gt[step])\n",
    "        elif args.env == 'FluidShake':\n",
    "            pyflex.set_shape_states(s_gt[step, :-1])\n",
    "\n",
    "        mass = np.zeros((n_particles, 1))\n",
    "        if args.env == 'RiceGrip':\n",
    "            p = np.concatenate([p_pred[step, :n_particles, -3:], mass], 1)\n",
    "        else:\n",
    "            p = np.concatenate([p_pred[step, :n_particles], mass], 1)\n",
    "\n",
    "        pyflex.set_positions(p)\n",
    "        pyflex.render(capture=0)\n",
    "\n",
    "pyflex.clean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
